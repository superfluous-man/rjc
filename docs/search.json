[{"path":"index.html","id":"welcome","chapter":"Welcome","heading":"Welcome","text":" website work--progress 2nd edition “R Data Science”. book teach data science R: ’ll learn get data R, get useful structure, transform , visualise model . book, find practicum skills data science. Just chemist learns clean test tubes stock lab, ’ll learn clean data draw plots—many things besides. skills allow data science happen, find best practices things R. ’ll learn use grammar graphics, literate programming, reproducible research save time. ’ll also learn manage cognitive resources facilitate discoveries wrangling, visualising, exploring data.website (always ) free use, licensed Creative Commons Attribution-NonCommercial-NoDerivs 3.0 License. ’d like physical copy book, can order amazon; published O’Reilly January 2017. ’d like give back\nplease make donation Kākāpō Recovery: kākāpō (appears cover R4DS) critically endangered native NZ parrot; 213 left.Please note R4DS uses Contributor Code Conduct. contributing book, agree abide terms.","code":""},{"path":"index.html","id":"acknowledgements","chapter":"Welcome","heading":"Acknowledgements","text":"R4DS collaborative effort many people contributed fixes improvements via pull request: adi pradhan (@adidoit), Andrea Gilardi (@agila5), Ajay Deonarine (@ajay-d), @AlanFeder, pete (@alonzi), Alex (@ALShum), Andrew Landgraf (@andland), @andrewmacfarland, Michael Henry (@aviast), Mara Averick (@batpigandme), Brent Brewington (@bbrewington), Bill Behrman (@behrman), Ben Herbertson (@benherbertson), Ben Marwick (@benmarwick), Ben Steinberg (@bensteinberg), Brandon Greenwell (@bgreenwell), Brett Klamer (@bklamer), Christian Mongeau (@chrMongeau), Cooper Morris (@coopermor), Colin Gillespie (@csgillespie), Rademeyer Vermaak (@csrvermaak), Abhinav Singh (@curious-abhinav), Curtis Alexander (@curtisalexander), Christian G. Warden (@cwarden), Kenny Darrell (@darrkj), David Rubinger (@davidrubinger), David Clark (@DDClark), Derwin McGeary (@derwinmcgeary), Daniel Gromer (@dgromer), @djbirke, Devin Pastoor (@dpastoor), Julian (@duju211), Dylan Cashman (@dylancashman), Dirk Eddelbuettel (@eddelbuettel), Edwin Thoen (@EdwinTh), Ahmed El-Gabbas (@elgabbas), Eric Watt (@ericwatt), Erik Erhardt (@erikerhardt), Etienne B. Racine (@etiennebr), Everett Robinson (@evjrob), Flemming Villalona (@flemingspace), Floris Vanderhaeghe (@florisvdh), Garrick Aden-Buie (@gadenbuie), Garrett Grolemund (@garrettgman), Josh Goldberg (@GoldbergData), bahadir cankardes (@gridgrad), Gustav W Delius (@gustavdelius), Hadley Wickham (@hadley), Hao Chen (@hao-trivago), Harris McGehee (@harrismcgehee), Hengni Cai (@hengnicai), Ian Sealy (@iansealy), Ian Lyttle (@ijlyttle), Ivan Krukov (@ivan-krukov), Jacob Kaplan (@jacobkap), Jazz Weisman (@jazzlw), John D. Storey (@jdstorey), Jeff Boichuk (@jeffboichuk), Gregory Jefferis (@jefferis), 蒋雨蒙 (@JeldorPKU), Jennifer (Jenny) Bryan (@jennybc), Jen Ren (@jenren), Jeroen Janssens (@jeroenjanssens), Jim Hester (@jimhester), JJ Chen (@jjchern), Joanne Jang (@joannejang), John Sears (@johnsears), @jonathanflint, Jon Calder (@jonmcalder), Jonathan Page (@jonpage), Justinas Petuchovas (@jpetuchovas), Jose Roberto Ayala Solares (@jroberayalas), Julia Stewart Lowndes (@jules32), Sonja (@kaetschap), Kara Woo (@karawoo), Katrin Leinweber (@katrinleinweber), Karandeep Singh (@kdpsingh), Kyle Humphrey (@khumph), Kirill Sevastyanenko (@kirillseva), @koalabearski, Kirill Müller (@krlmlr), Noah Landesberg (@landesbergn), @lindbrook, Mauro Lepore (@maurolepore), Mark Beveridge (@mbeveridge), Matt Herman (@mfherman), Mine Cetinkaya-Rundel (@mine-cetinkaya-rundel), Matthew Hendrickson (@mjhendrickson), @MJMarshall, Mustafa Ascha (@mustafaascha), Nelson Areal (@nareal), Nate Olson (@nate-d-olson), Nathanael (@nateaff), Nick Clark (@nickclark1000), @nickelas, Nirmal Patel (@nirmalpatel), Nina Munkholt Jakobsen (@nmjakobsen), Jakub Nowosad (@Nowosad), Peter Hurford (@peterhurford), Patrick Kennedy (@pkq), Radu Grosu (@radugrosu), Ranae Dietzel (@Ranae), Robin Gertenbach (@rgertenbach), Richard Zijdeman (@rlzijdeman), Robin (@Robinlovelace), Emily Robinson (@robinsones), Rohan Alexander (@RohanAlexander), Romero Morais (@RomeroBarata), Albert Y. Kim (@rudeboybert), Saghir (@saghirb), Jonas (@sauercrowd), Robert Schuessler (@schuess), Seamus McKinsey (@seamus-mckinsey), @seanpwilliams, Luke Smith (@seasmith), Matthew Sedaghatfar (@sedaghatfar), Sebastian Kraus (@sekR4), Sam Firke (@sfirke), Shannon Ellis (@ShanEllis), @shoili, S’busiso Mkhondwane (@sibusiso16), @spirgel, Steven M. Mortimer (@StevenMMortimer), Stéphane Guillou (@stragu), Sergiusz Bleja (@svenski), Tal Galili (@talgalili), Tim Waterhouse (@timwaterhouse), TJ Mahr (@tjmahr), Thomas Klebel (@tklebel), Tom Prior (@tomjamesprior), Terence Teo (@tteo), Beasley (@wibeasley), @yahwes, Yihui Xie (@yihui), Yiming (Paul) Li (@yimingli), Hiroaki Yutani (@yutannihilation), @zeal626, Azza Ahmed (@zo0z)R4DS hosted https://www.netlify.com part support open source software communities.","code":""},{"path":"introduction.html","id":"introduction","chapter":"1 Introduction","heading":"1 Introduction","text":"Data science exciting discipline allows turn raw data understanding, insight, knowledge. goal “R Data Science” help learn important tools R allow data science. reading book, ’ll tools tackle wide variety data science challenges, using best parts R.","code":""},{"path":"introduction.html","id":"what-you-will-learn","chapter":"1 Introduction","heading":"1.1 What you will learn","text":"Data science huge field, ’s way can master reading single book. goal book give solid foundation important tools. model tools needed typical data science project looks something like :First must import data R. typically means take data stored file, database, web application programming interface (API), load data frame R. can’t get data R, can’t data science !’ve imported data, good idea tidy . Tidying data means storing consistent form matches semantics dataset way stored. brief, data tidy, column variable, row observation. Tidy data important consistent structure lets focus struggle questions data, fighting get data right form different functions.tidy data, common first step transform . Transformation includes narrowing observations interest (like people one city, data last year), creating new variables functions existing variables (like computing speed distance time), calculating set summary statistics (like counts means). Together, tidying transforming called wrangling, getting data form ’s natural work often feels like fight!tidy data variables need, two main engines knowledge generation: visualisation modelling. complementary strengths weaknesses real analysis iterate many times.Visualisation fundamentally human activity. good visualisation show things expect, raise new questions data. good visualisation might also hint ’re asking wrong question, need collect different data. Visualisations can surprise , don’t scale particularly well require human interpret .Models complementary tools visualisation. made questions sufficiently precise, can use model answer . Models fundamentally mathematical computational tool, generally scale well. Even don’t, ’s usually cheaper buy computers buy brains! every model makes assumptions, nature model question assumptions. means model fundamentally surprise .last step data science communication, absolutely critical part data analysis project. doesn’t matter well models visualisation led understand data unless can also communicate results others.Surrounding tools programming. Programming cross-cutting tool use every part project. don’t need expert programmer data scientist, learning programming pays becoming better programmer allows automate common tasks, solve new problems greater ease.’ll use tools every data science project, projects ’re enough. ’s rough 80-20 rule play; can tackle 80% every project using tools ’ll learn book, ’ll need tools tackle remaining 20%. Throughout book ’ll point resources can learn .","code":""},{"path":"introduction.html","id":"how-this-book-is-organised","chapter":"1 Introduction","heading":"1.2 How this book is organised","text":"previous description tools data science organised roughly according order use analysis (although course ’ll iterate multiple times). experience, however, best way learn :Starting data ingest tidying sub-optimal 80% time\n’s routine boring, 20% time ’s weird \nfrustrating. ’s bad place start learning new subject! Instead,\n’ll start visualisation transformation data ’s already \nimported tidied. way, ingest tidy data, \nmotivation stay high know pain worth .Starting data ingest tidying sub-optimal 80% time\n’s routine boring, 20% time ’s weird \nfrustrating. ’s bad place start learning new subject! Instead,\n’ll start visualisation transformation data ’s already \nimported tidied. way, ingest tidy data, \nmotivation stay high know pain worth .topics best explained tools. example, believe \n’s easier understand models work already know \nvisualisation, tidy data, programming.topics best explained tools. example, believe \n’s easier understand models work already know \nvisualisation, tidy data, programming.Programming tools necessarily interesting right,\nallow tackle considerably challenging problems. ’ll\ngive selection programming tools middle book, \n’ll see can combine data science tools tackle\ninteresting modelling problems.Programming tools necessarily interesting right,\nallow tackle considerably challenging problems. ’ll\ngive selection programming tools middle book, \n’ll see can combine data science tools tackle\ninteresting modelling problems.Within chapter, try stick similar pattern: start motivating examples can see bigger picture, dive details. section book paired exercises help practice ’ve learned. ’s tempting skip exercises, ’s better way learn practicing real problems.","code":""},{"path":"introduction.html","id":"what-you-wont-learn","chapter":"1 Introduction","heading":"1.3 What you won’t learn","text":"important topics book doesn’t cover. believe ’s important stay ruthlessly focused essentials can get running quickly possible. means book can’t cover every important topic.","code":""},{"path":"introduction.html","id":"big-data","chapter":"1 Introduction","heading":"1.3.1 Big data","text":"book proudly focuses small, -memory datasets. right place start can’t tackle big data unless experience small data. tools learn book easily handle hundreds megabytes data, little care can typically use work 1-2 Gb data. ’re routinely working larger data (10-100 Gb, say), learn data.table. book doesn’t teach data.table concise interface makes harder learn since offers fewer linguistic cues. ’re working large data, performance payoff worth extra effort required learn .data bigger , carefully consider big data problem might actually small data problem disguise. complete data might big, often data needed answer specific question small. might able find subset, subsample, summary fits memory still allows answer question ’re interested . challenge finding right small data, often requires lot iteration.Another possibility big data problem actually large number small data problems. individual problem might fit memory, millions . example, might want fit model person dataset. trivial just 10 100 people, instead million. Fortunately problem independent others (setup sometimes called embarrassingly parallel), just need system (like Hadoop Spark) allows send different datasets different computers processing. ’ve figured answer question single subset using tools described book, learn new tools like sparklyr, rhipe, ddr solve full dataset.","code":""},{"path":"introduction.html","id":"python-julia-and-friends","chapter":"1 Introduction","heading":"1.3.2 Python, Julia, and friends","text":"book, won’t learn anything Python, Julia, programming language useful data science. isn’t think tools bad. ’re ! practice, data science teams use mix languages, often least R Python.However, strongly believe ’s best master one tool time. get better faster dive deep, rather spreading thinly many topics. doesn’t mean know one thing, just ’ll generally learn faster stick one thing time. strive learn new things throughout career, make sure understanding solid move next interesting thing.think R great place start data science journey environment designed ground support data science. R just programming language, also interactive environment data science. support interaction, R much flexible language many peers. flexibility comes downsides, big upside easy evolve tailored grammars specific parts data science process. mini languages help think problems data scientist, supporting fluent interaction brain computer.","code":""},{"path":"introduction.html","id":"non-rectangular-data","chapter":"1 Introduction","heading":"1.3.3 Non-rectangular data","text":"book focuses exclusively rectangular data: collections values associated variable observation. lots datasets naturally fit paradigm, including images, sounds, trees, text. rectangular data frames extremely common science industry, believe great place start data science journey.","code":""},{"path":"introduction.html","id":"hypothesis-confirmation","chapter":"1 Introduction","heading":"1.3.4 Hypothesis confirmation","text":"’s possible divide data analysis two camps: hypothesis generation hypothesis confirmation (sometimes called confirmatory analysis). focus book unabashedly hypothesis generation, data exploration. ’ll look deeply data , combination subject knowledge, generate many interesting hypotheses help explain data behaves way . evaluate hypotheses informally, using scepticism challenge data multiple ways.complement hypothesis generation hypothesis confirmation. Hypothesis confirmation hard two reasons:need precise mathematical model order generate falsifiable\npredictions. often requires considerable statistical sophistication.need precise mathematical model order generate falsifiable\npredictions. often requires considerable statistical sophistication.can use observation confirm hypothesis. soon \nuse ’re back exploratory analysis.\nmeans hypothesis confirmation need “preregister”\n(write advance) analysis plan, deviate \neven seen data. ’ll talk little \nstrategies can use make easier modelling.can use observation confirm hypothesis. soon \nuse ’re back exploratory analysis.\nmeans hypothesis confirmation need “preregister”\n(write advance) analysis plan, deviate \neven seen data. ’ll talk little \nstrategies can use make easier modelling.’s common think modelling tool hypothesis confirmation, visualisation tool hypothesis generation. ’s false dichotomy: models often used exploration, little care can use visualisation confirmation. key difference often look observation: look , ’s confirmation; look , ’s exploration.","code":""},{"path":"introduction.html","id":"prerequisites","chapter":"1 Introduction","heading":"1.4 Prerequisites","text":"’ve made assumptions already know order get book. generally numerically literate, ’s helpful programming experience already. ’ve never programmed , might find Hands Programming R Garrett useful adjunct book.four things need run code book: R, RStudio, collection R packages called tidyverse, handful packages. Packages fundamental units reproducible R code. include reusable functions, documentation describes use , sample data.","code":""},{"path":"introduction.html","id":"r","chapter":"1 Introduction","heading":"1.4.1 R","text":"download R, go CRAN, comprehensive R archive network. CRAN composed set mirror servers distributed around world used distribute R R packages. Don’t try pick mirror ’s close : instead use cloud mirror, https://cloud.r-project.org, automatically figures .new major version R comes year, 2-3 minor releases year. ’s good idea update regularly. Upgrading can bit hassle, especially major versions, require reinstall packages, putting makes worse.","code":""},{"path":"introduction.html","id":"rstudio","chapter":"1 Introduction","heading":"1.4.2 RStudio","text":"RStudio integrated development environment, IDE, R programming. Download install http://www.rstudio.com/download. RStudio updated couple times year. new version available, RStudio let know. ’s good idea upgrade regularly can take advantage latest greatest features. book, make sure least RStudio 1.0.0.start RStudio, ’ll see two key regions interface:now, need know type R code console pane, press enter run . ’ll learn go along!","code":""},{"path":"introduction.html","id":"the-tidyverse","chapter":"1 Introduction","heading":"1.4.3 The tidyverse","text":"’ll also need install R packages. R package collection functions, data, documentation extends capabilities base R. Using packages key successful use R. majority packages learn book part -called tidyverse. packages tidyverse share common philosophy data R programming, designed work together naturally.can install complete tidyverse single line code:computer, type line code console, press enter run . R download packages CRAN install computer. problems installing, make sure connected internet, https://cloud.r-project.org/ isn’t blocked firewall proxy.able use functions, objects, help files package load library(). installed package, can load library() function:tells tidyverse loading ggplot2, tibble, tidyr, readr, purrr, dplyr packages. considered core tidyverse ’ll use almost every analysis.Packages tidyverse change fairly frequently. can see updates available, optionally install , running tidyverse_update().","code":"\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)\n#> ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──\n#> ✓ ggplot2 3.3.3     ✓ purrr   0.3.4\n#> ✓ tibble  3.0.6     ✓ dplyr   1.0.4\n#> ✓ tidyr   1.1.2     ✓ stringr 1.4.0\n#> ✓ readr   1.4.0     ✓ forcats 0.5.1\n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> x dplyr::filter() masks stats::filter()\n#> x dplyr::lag()    masks stats::lag()"},{"path":"introduction.html","id":"other-packages","chapter":"1 Introduction","heading":"1.4.4 Other packages","text":"many excellent packages part tidyverse, solve problems different domain, designed different set underlying principles. doesn’t make better worse, just different. words, complement tidyverse messyverse, many universes interrelated packages. tackle data science projects R, ’ll learn new packages new ways thinking data.book ’ll use three data packages outside tidyverse:packages provide data airline flights, world development, baseball ’ll use illustrate key data science ideas.","code":"\ninstall.packages(c(\"nycflights13\", \"gapminder\", \"Lahman\"))"},{"path":"introduction.html","id":"running-r-code","chapter":"1 Introduction","heading":"1.5 Running R code","text":"previous section showed couple examples running R code. Code book looks like :run code local console, look like :two main differences. console, type >, called prompt; don’t show prompt book. book, output commented #>; console appears directly code. two differences mean ’re working electronic version book, can easily copy code book console.Throughout book use consistent set conventions refer code:Functions code font followed parentheses, like sum(),\nmean().Functions code font followed parentheses, like sum(),\nmean().R objects (like data function arguments) code font,\nwithout parentheses, like flights x.R objects (like data function arguments) code font,\nwithout parentheses, like flights x.want make clear package object comes , ’ll use\npackage name followed two colons, like dplyr::mutate(), ornycflights13::flights. also valid R code.want make clear package object comes , ’ll use\npackage name followed two colons, like dplyr::mutate(), ornycflights13::flights. also valid R code.","code":"\n1 + 2\n#> [1] 3> 1 + 2\n[1] 3"},{"path":"introduction.html","id":"getting-help-and-learning-more","chapter":"1 Introduction","heading":"1.6 Getting help and learning more","text":"book island; single resource allow master R. start apply techniques described book data soon find questions answer. section describes tips get help, help keep learning.get stuck, start Google. Typically adding “R” query enough restrict relevant results: search isn’t useful, often means aren’t R-specific results available. Google particularly useful error messages. get error message idea means, try googling ! Chances someone else confused past, help somewhere web. (error message isn’t English, run Sys.setenv(LANGUAGE = \"en\") re-run code; ’re likely find help English error messages.)Google doesn’t help, try Stack Overflow. Start spending little time searching existing answer, including [R] restrict search questions answers use R. don’t find anything useful, prepare minimal reproducible example reprex. good reprex makes easier people help , often ’ll figure problem course making .three things need include make example reproducible: required packages, data, code.Packages loaded top script, ’s easy \nsee ones example needs. good time check ’re\nusing latest version package; ’s possible ’ve discovered\nbug ’s fixed since installed package. packages\ntidyverse, easiest way check run tidyverse_update().Packages loaded top script, ’s easy \nsee ones example needs. good time check ’re\nusing latest version package; ’s possible ’ve discovered\nbug ’s fixed since installed package. packages\ntidyverse, easiest way check run tidyverse_update().easiest way include data question use dput() \ngenerate R code recreate . example, recreate mtcars\ndataset R, ’d perform following steps:\nRun dput(mtcars) R\nCopy output\nreproducible script, type mtcars <- paste.\nTry find smallest subset data still reveals\nproblem.easiest way include data question use dput() \ngenerate R code recreate . example, recreate mtcars\ndataset R, ’d perform following steps:Run dput(mtcars) RCopy outputIn reproducible script, type mtcars <- paste.Try find smallest subset data still reveals\nproblem.Spend little bit time ensuring code easy others \nread:\nMake sure ’ve used spaces variable names concise, yet\ninformative.\nUse comments indicate problem lies.\nbest remove everything related problem.\nshorter code , easier understand, \neasier fix.\nSpend little bit time ensuring code easy others \nread:Make sure ’ve used spaces variable names concise, yet\ninformative.Make sure ’ve used spaces variable names concise, yet\ninformative.Use comments indicate problem lies.Use comments indicate problem lies.best remove everything related problem.\nshorter code , easier understand, \neasier fix.best remove everything related problem.\nshorter code , easier understand, \neasier fix.Finish checking actually made reproducible example starting fresh R session copying pasting script .also spend time preparing solve problems occur. Investing little time learning R day pay handsomely long run. One way follow Hadley, Garrett, everyone else RStudio RStudio blog. post announcements new packages, new IDE features, -person courses. might also want follow Hadley (@hadleywickham) Garrett (@statgarrett) Twitter, follow @rstudiotips keep new features IDE.keep R community broadly, recommend reading http://www.r-bloggers.com: aggregates 500 blogs R around world. ’re active Twitter user, follow (#rstats) hashtag. Twitter one key tools Hadley uses keep new developments community.","code":""},{"path":"introduction.html","id":"acknowledgements-1","chapter":"1 Introduction","heading":"1.7 Acknowledgements","text":"book isn’t just product Hadley Garrett, result many conversations (person online) ’ve many people R community. people ’d like thank particular, spent many hours answering dumb questions helping us better think data science:Jenny Bryan Lionel Henry many helpful discussions around working\nlists list-columns.Jenny Bryan Lionel Henry many helpful discussions around working\nlists list-columns.three chapters workflow adapted (permission), \nhttp://stat545.com/block002_hello-r-workspace-wd-project.html \nJenny Bryan.three chapters workflow adapted (permission), \nhttp://stat545.com/block002_hello-r-workspace-wd-project.html \nJenny Bryan.Genevera Allen discussions models, modelling, statistical\nlearning perspective, difference hypothesis generation \nhypothesis confirmation.Genevera Allen discussions models, modelling, statistical\nlearning perspective, difference hypothesis generation \nhypothesis confirmation.Yihui Xie work bookdown\npackage, tirelessly responding feature requests.Yihui Xie work bookdown\npackage, tirelessly responding feature requests.Bill Behrman thoughtful reading entire book, trying\ndata science class Stanford.Bill Behrman thoughtful reading entire book, trying\ndata science class Stanford.#rstats Twitter community reviewed draft chapters\nprovided tons useful feedback.#rstats Twitter community reviewed draft chapters\nprovided tons useful feedback.Tal Galili augmenting dendextend package support section clustering make final draft.Tal Galili augmenting dendextend package support section clustering make final draft.book written open, many people contributed pull requests fix minor problems. Special thanks goes everyone contributed via GitHub:Thanks go contributers alphabetical order: @-rosenberg, . s, Abhinav Singh, adi pradhan, Ahmed ElGabbas, Ajay Deonarine, @AlanFeder, Albert Y. Kim, @Alex, Andrea Gilardi, Andrew Landgraf, @andrewmacfarland, Angela Li, Azza Ahmed, bahadir cankardes, @batpigandme, @behrman, Ben Herbertson, Ben Marwick, Ben Steinberg, Benjamin Yeh, Bianca Peterson, Bill Behrman, @BirgerNi, @boardtc, Brandon Greenwell, Brent Brewington, Brett Klamer, Brian G. Barkley, Charlotte Wickham, Christian G. Warden, Christian Heinrich, Christian Mongeau, Colin Gillespie, Cooper Morris, Curtis Alexander, Daniel Gromer, David Clark, David Rubinger, Derwin McGeary, Devin Pastoor, Dirk Eddelbuettel, @djbirke, @DSGeoff, Dylan Cashman, Earl Brown, Edwin Thoen, Eric Watt, Erik Erhardt, Etienne B. Racine, Everett Robinson, Flemming Villalona, Floris Vanderhaeghe, Garrick Aden-Buie, George Wang, Gregory Jefferis, Gustav W Delius, Hao Chen, @harrismcgehee, Hengni Cai, Hiroaki Yutani, Hojjat Salmasian, Ian Lyttle, Ian Sealy, Ivan Krukov, Jacek Kolacz, Jacob Kaplan, Jakub Nowosad, Jazz Weisman, Jeff Boichuk, Jeffrey Arnold, Jen Ren, Jennifer (Jenny) Bryan, @jennybc, Jeroen Janssens, Jim Hester, @jjchern, Joanne Jang, Johannes Gruber, John Blischak, John D. Storey, John Sears, Jon Calder, @Jonas, Jonathan Page, @jonathanflint, Jose Roberto Ayala Solares, Josh Goldberg, @juandering, Julia Stewart Lowndes, Julian , Justinas Petuchovas, @kaetschap, Kara de la Marck, Kara Woo, Katrin Leinweber, @kdpsingh, Kenny Darrell, Kirill Müller, Kirill Sevastyanenko, @koalabearski, Kunal Marwaha, @KyleHumphrey, Lawrence Wu, @lindbrook, Luke Smith, Luke W Johnston, Mara Averick, Maria Paula Caldas, Mark Beveridge, Matt Herman, Matthew Hendrickson, Matthew Sedaghatfar, @MattWittbrodt, Mauro Lepore, Michael Henry, Mine Cetinkaya-Rundel, @MJMarshall, Mustafa Ascha, @nate-d-olson, @nattalides, Nelson Areal, Nicholas Tierney, Nick Clark, @nickelas, Nina Munkholt Jakobsen, Nirmal Patel, Nischal Shrestha, Noah Landesberg, @nwaff, @OaCantona, Pablo E, Patrick Kennedy, @Paul, @pete, Peter Hurford, Rademeyer Vermaak, Radu Grosu, Ranae Dietzel, Riva Quiroga, @rlzijdeman, Rob Tenorio, Robert Schuessler, @robertchu03, Robin Gertenbach, @robinlovelace, @robinsones, Rohan Alexander, @RomeroBarata, S’busiso Mkhondwane, @Saghir, Sam Firke, Seamus McKinsey, @seamus-mckinsey, @seanpwilliams, Sebastian Kraus, Shannon Ellis, @shoili, @sibusiso16, @Sophiazj, @spirgel, Stéphane Guillou, Steve Mortimer, @svenski, Tal Galili, Terence Teo, Thomas Klebel, Tim Waterhouse, TJ Mahr, Tom Prior, @twgardner2, Ulrik Lyngs, Beasley, @yahwes, Yihui Xie, Yiming (Paul) Li, Yu Yu Aung, Zach Bogart, @zeal626, Zhuoer Dong, @蒋雨蒙.","code":""},{"path":"introduction.html","id":"colophon","chapter":"1 Introduction","heading":"1.8 Colophon","text":"online version book available http://r4ds..co.nz. continue evolve reprints physical book. source book available https://github.com/hadley/r4ds. book powered https://bookdown.org makes easy turn R markdown files HTML, PDF, EPUB.book built :","code":"\nsessioninfo::session_info(c(\"tidyverse\"))\n#> ─ Session info ───────────────────────────────────────────────────────────────\n#>  setting  value                       \n#>  version  R version 4.0.3 (2020-10-10)\n#>  os       Manjaro Linux               \n#>  system   x86_64, linux-gnu           \n#>  ui       X11                         \n#>  language (EN)                        \n#>  collate  en_US.UTF-8                 \n#>  ctype    en_US.UTF-8                 \n#>  tz       Asia/Hong_Kong              \n#>  date     2021-02-14                  \n#> \n#> ─ Packages ───────────────────────────────────────────────────────────────────\n#>  package      * version  date       lib source        \n#>  askpass        1.1      2019-01-13 [1] CRAN (R 4.0.3)\n#>  assertthat     0.2.1    2019-03-21 [1] CRAN (R 4.0.3)\n#>  backports      1.2.1    2020-12-09 [1] CRAN (R 4.0.3)\n#>  base64enc      0.1-3    2015-07-28 [1] CRAN (R 4.0.3)\n#>  BH             1.75.0-0 2021-01-11 [1] CRAN (R 4.0.3)\n#>  blob           1.2.1    2020-01-20 [1] CRAN (R 4.0.3)\n#>  brio           1.1.1    2021-01-20 [1] CRAN (R 4.0.3)\n#>  broom          0.7.4    2021-01-29 [1] CRAN (R 4.0.3)\n#>  callr          3.5.1    2020-10-13 [1] CRAN (R 4.0.3)\n#>  cellranger     1.1.0    2016-07-27 [1] CRAN (R 4.0.3)\n#>  cli            2.3.0    2021-01-31 [1] CRAN (R 4.0.3)\n#>  clipr          0.7.1    2020-10-08 [1] CRAN (R 4.0.3)\n#>  colorspace     2.0-0    2020-11-11 [1] CRAN (R 4.0.3)\n#>  cpp11          0.2.6    2021-01-29 [1] CRAN (R 4.0.3)\n#>  crayon         1.4.1    2021-02-08 [1] CRAN (R 4.0.3)\n#>  curl           4.3      2019-12-02 [1] CRAN (R 4.0.3)\n#>  DBI            1.1.1    2021-01-15 [1] CRAN (R 4.0.3)\n#>  dbplyr         2.1.0    2021-02-03 [1] CRAN (R 4.0.3)\n#>  desc           1.2.0    2018-05-01 [1] CRAN (R 4.0.3)\n#>  diffobj        0.3.3    2021-01-07 [1] CRAN (R 4.0.3)\n#>  digest         0.6.27   2020-10-24 [1] CRAN (R 4.0.3)\n#>  dplyr        * 1.0.4    2021-02-02 [1] CRAN (R 4.0.3)\n#>  ellipsis       0.3.1    2020-05-15 [1] CRAN (R 4.0.3)\n#>  evaluate       0.14     2019-05-28 [1] CRAN (R 4.0.3)\n#>  fansi          0.4.2    2021-01-15 [1] CRAN (R 4.0.3)\n#>  farver         2.0.3    2020-01-16 [1] CRAN (R 4.0.3)\n#>  forcats      * 0.5.1    2021-01-27 [1] CRAN (R 4.0.3)\n#>  fs             1.5.0    2020-07-31 [1] CRAN (R 4.0.3)\n#>  generics       0.1.0    2020-10-31 [1] CRAN (R 4.0.3)\n#>  ggplot2      * 3.3.3    2020-12-30 [1] CRAN (R 4.0.3)\n#>  glue           1.4.2    2020-08-27 [1] CRAN (R 4.0.3)\n#>  gtable         0.3.0    2019-03-25 [1] CRAN (R 4.0.3)\n#>  haven          2.3.1    2020-06-01 [1] CRAN (R 4.0.3)\n#>  highr          0.8      2019-03-20 [1] CRAN (R 4.0.3)\n#>  hms            1.0.0    2021-01-13 [1] CRAN (R 4.0.3)\n#>  htmltools      0.5.1.1  2021-01-22 [1] CRAN (R 4.0.3)\n#>  httr           1.4.2    2020-07-20 [1] CRAN (R 4.0.3)\n#>  isoband        0.2.3    2020-12-01 [1] CRAN (R 4.0.3)\n#>  jsonlite       1.7.2    2020-12-09 [1] CRAN (R 4.0.3)\n#>  knitr          1.31     2021-01-27 [1] CRAN (R 4.0.3)\n#>  labeling       0.4.2    2020-10-20 [1] CRAN (R 4.0.3)\n#>  lattice        0.20-41  2020-04-02 [2] CRAN (R 4.0.3)\n#>  lifecycle      0.2.0    2020-03-06 [1] CRAN (R 4.0.3)\n#>  lubridate      1.7.9.2  2020-11-13 [1] CRAN (R 4.0.3)\n#>  magrittr       2.0.1    2020-11-17 [1] CRAN (R 4.0.3)\n#>  markdown       1.1      2019-08-07 [1] CRAN (R 4.0.3)\n#>  MASS           7.3-53   2020-09-09 [2] CRAN (R 4.0.3)\n#>  Matrix         1.2-18   2019-11-27 [2] CRAN (R 4.0.3)\n#>  mgcv           1.8-33   2020-08-27 [2] CRAN (R 4.0.3)\n#>  mime           0.9      2020-02-04 [1] CRAN (R 4.0.3)\n#>  modelr         0.1.8    2020-05-19 [1] CRAN (R 4.0.3)\n#>  munsell        0.5.0    2018-06-12 [1] CRAN (R 4.0.3)\n#>  nlme           3.1-149  2020-08-23 [2] CRAN (R 4.0.3)\n#>  openssl        1.4.3    2020-09-18 [1] CRAN (R 4.0.3)\n#>  pillar         1.4.7    2020-11-20 [1] CRAN (R 4.0.3)\n#>  pkgbuild       1.2.0    2020-12-15 [1] CRAN (R 4.0.3)\n#>  pkgconfig      2.0.3    2019-09-22 [1] CRAN (R 4.0.3)\n#>  pkgload        1.1.0    2020-05-29 [1] CRAN (R 4.0.3)\n#>  praise         1.0.0    2015-08-11 [1] CRAN (R 4.0.3)\n#>  prettyunits    1.1.1    2020-01-24 [1] CRAN (R 4.0.3)\n#>  processx       3.4.5    2020-11-30 [1] CRAN (R 4.0.3)\n#>  progress       1.2.2    2019-05-16 [1] CRAN (R 4.0.3)\n#>  ps             1.5.0    2020-12-05 [1] CRAN (R 4.0.3)\n#>  purrr        * 0.3.4    2020-04-17 [1] CRAN (R 4.0.3)\n#>  R6             2.5.0    2020-10-28 [1] CRAN (R 4.0.3)\n#>  RColorBrewer   1.1-2    2014-12-07 [1] CRAN (R 4.0.3)\n#>  Rcpp           1.0.6    2021-01-15 [1] CRAN (R 4.0.3)\n#>  readr        * 1.4.0    2020-10-05 [1] CRAN (R 4.0.3)\n#>  readxl         1.3.1    2019-03-13 [1] CRAN (R 4.0.3)\n#>  rematch        1.0.1    2016-04-21 [1] CRAN (R 4.0.3)\n#>  rematch2       2.1.2    2020-05-01 [1] CRAN (R 4.0.3)\n#>  reprex         1.0.0    2021-01-27 [1] CRAN (R 4.0.3)\n#>  rlang          0.4.10   2020-12-30 [1] CRAN (R 4.0.3)\n#>  rmarkdown      2.6      2020-12-14 [1] CRAN (R 4.0.3)\n#>  rprojroot      2.0.2    2020-11-15 [1] CRAN (R 4.0.3)\n#>  rstudioapi     0.13     2020-11-12 [1] CRAN (R 4.0.3)\n#>  rvest          0.3.6    2020-07-25 [1] CRAN (R 4.0.3)\n#>  scales         1.1.1    2020-05-11 [1] CRAN (R 4.0.3)\n#>  selectr        0.4-2    2019-11-20 [1] CRAN (R 4.0.3)\n#>  stringi        1.5.3    2020-09-09 [1] CRAN (R 4.0.3)\n#>  stringr      * 1.4.0    2019-02-10 [1] CRAN (R 4.0.3)\n#>  sys            3.4      2020-07-23 [1] CRAN (R 4.0.3)\n#>  testthat       3.0.1    2020-12-17 [1] CRAN (R 4.0.3)\n#>  tibble       * 3.0.6    2021-01-29 [1] CRAN (R 4.0.3)\n#>  tidyr        * 1.1.2    2020-08-27 [1] CRAN (R 4.0.3)\n#>  tidyselect     1.1.0    2020-05-11 [1] CRAN (R 4.0.3)\n#>  tidyverse    * 1.3.0    2019-11-21 [1] CRAN (R 4.0.3)\n#>  tinytex        0.29     2021-01-21 [1] CRAN (R 4.0.3)\n#>  utf8           1.1.4    2018-05-24 [1] CRAN (R 4.0.3)\n#>  vctrs          0.3.6    2020-12-17 [1] CRAN (R 4.0.3)\n#>  viridisLite    0.3.0    2018-02-01 [1] CRAN (R 4.0.3)\n#>  waldo          0.2.4    2021-02-11 [1] CRAN (R 4.0.3)\n#>  withr          2.4.1    2021-01-26 [1] CRAN (R 4.0.3)\n#>  xfun           0.21     2021-02-10 [1] CRAN (R 4.0.3)\n#>  xml2           1.3.2    2020-04-23 [1] CRAN (R 4.0.3)\n#>  yaml           2.2.1    2020-02-01 [1] CRAN (R 4.0.3)\n#> \n#> [1] /home/mxj/R/x86_64-pc-linux-gnu-library/4.0\n#> [2] /usr/lib/R/library"},{"path":"explore-intro.html","id":"explore-intro","chapter":"2 Introduction","heading":"2 Introduction","text":"goal first part book get speed basic tools data exploration quickly possible. Data exploration art looking data, rapidly generating hypotheses, quickly testing , repeating . goal data exploration generate many promising leads can later explore depth.part book learn useful tools immediate payoff:Visualisation great place start R programming, \npayoff clear: get make elegant informative plots help\nunderstand data. data visualisation ’ll dive visualisation,\nlearning basic structure ggplot2 plot, powerful techniques \nturning data plots.Visualisation great place start R programming, \npayoff clear: get make elegant informative plots help\nunderstand data. data visualisation ’ll dive visualisation,\nlearning basic structure ggplot2 plot, powerful techniques \nturning data plots.Visualisation alone typically enough, data transformation\n’ll learn key verbs allow select important variables,\nfilter key observations, create new variables, compute summaries.Visualisation alone typically enough, data transformation\n’ll learn key verbs allow select important variables,\nfilter key observations, create new variables, compute summaries.Finally, exploratory data analysis, ’ll combine visualisation \ntransformation curiosity scepticism ask answer\ninteresting questions data.Finally, exploratory data analysis, ’ll combine visualisation \ntransformation curiosity scepticism ask answer\ninteresting questions data.Modelling important part exploratory process, don’t skills effectively learn apply yet. ’ll come back modelling, ’re better equipped data wrangling programming tools.Nestled among three chapters teach tools exploration three chapters focus R workflow. workflow: basics, workflow: scripts, workflow: projects ’ll learn good practices writing organising R code. set success long run, ’ll give tools stay organised tackle real projects.","code":""},{"path":"data-visualisation.html","id":"data-visualisation","chapter":"3 Data visualisation","heading":"3 Data visualisation","text":"","code":""},{"path":"data-visualisation.html","id":"introduction-1","chapter":"3 Data visualisation","heading":"3.1 Introduction","text":"“simple graph brought information data analyst’s mind\ndevice.” — John TukeyThis chapter teach visualise data using ggplot2. R several systems making graphs, ggplot2 one elegant versatile. ggplot2 implements grammar graphics, coherent system describing building graphs. ggplot2, can faster learning one system applying many places.’d like learn theoretical underpinnings ggplot2 start, ’d recommend reading “Layered Grammar Graphics”, http://vita..co.nz/papers/layered-grammar.pdf.","code":""},{"path":"data-visualisation.html","id":"prerequisites-1","chapter":"3 Data visualisation","heading":"3.1.1 Prerequisites","text":"chapter focusses ggplot2, one core members tidyverse. access datasets, help pages, functions use chapter, load tidyverse running code:one line code loads core tidyverse; packages use almost every data analysis. also tells functions tidyverse conflict functions base R (packages might loaded).run code get error message “package called ‘tidyverse’”, ’ll need first install , run library() .need install package , need reload every time start new session.need explicit function (dataset) comes , ’ll use special form package::function(). example, ggplot2::ggplot() tells explicitly ’re using ggplot() function ggplot2 package.","code":"\nlibrary(tidyverse)\n#> ── Attaching packages ─────────────────────────────────────── tidyverse 1.3.0 ──\n#> ✓ ggplot2 3.3.3     ✓ purrr   0.3.4\n#> ✓ tibble  3.0.6     ✓ dplyr   1.0.4\n#> ✓ tidyr   1.1.2     ✓ stringr 1.4.0\n#> ✓ readr   1.4.0     ✓ forcats 0.5.1\n#> ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n#> x dplyr::filter() masks stats::filter()\n#> x dplyr::lag()    masks stats::lag()\ninstall.packages(\"tidyverse\")\nlibrary(tidyverse)"},{"path":"data-visualisation.html","id":"first-steps","chapter":"3 Data visualisation","heading":"3.2 First steps","text":"Let’s use first graph answer question: cars big engines use fuel cars small engines? probably already answer, try make answer precise. relationship engine size fuel efficiency look like? positive? Negative? Linear? Nonlinear?","code":""},{"path":"data-visualisation.html","id":"the-mpg-data-frame","chapter":"3 Data visualisation","heading":"3.2.1 The mpg data frame","text":"can test answer mpg data frame found ggplot2 (aka ggplot2::mpg). data frame rectangular collection variables (columns) observations (rows). mpg contains observations collected US Environmental Protection Agency 38 models car.Among variables mpg :displ, car’s engine size, litres.displ, car’s engine size, litres.hwy, car’s fuel efficiency highway, miles per gallon (mpg).\ncar low fuel efficiency consumes fuel car high\nfuel efficiency travel distance.hwy, car’s fuel efficiency highway, miles per gallon (mpg).\ncar low fuel efficiency consumes fuel car high\nfuel efficiency travel distance.learn mpg, open help page running ?mpg.","code":"\nmpg\n#> # A tibble: 234 x 11\n#>   manufacturer model displ  year   cyl trans      drv     cty   hwy fl    class \n#>   <chr>        <chr> <dbl> <int> <int> <chr>      <chr> <int> <int> <chr> <chr> \n#> 1 audi         a4      1.8  1999     4 auto(l5)   f        18    29 p     compa…\n#> 2 audi         a4      1.8  1999     4 manual(m5) f        21    29 p     compa…\n#> 3 audi         a4      2    2008     4 manual(m6) f        20    31 p     compa…\n#> 4 audi         a4      2    2008     4 auto(av)   f        21    30 p     compa…\n#> 5 audi         a4      2.8  1999     6 auto(l5)   f        16    26 p     compa…\n#> 6 audi         a4      2.8  1999     6 manual(m5) f        18    26 p     compa…\n#> # … with 228 more rows"},{"path":"data-visualisation.html","id":"creating-a-ggplot","chapter":"3 Data visualisation","heading":"3.2.2 Creating a ggplot","text":"plot mpg, run code put displ x-axis hwy y-axis:plot shows negative relationship engine size (displ) fuel efficiency (hwy). words, cars big engines use fuel. confirm refute hypothesis fuel efficiency engine size?ggplot2, begin plot function ggplot(). ggplot() creates coordinate system can add layers . first argument ggplot() dataset use graph. ggplot(data = mpg) creates empty graph, ’s interesting ’m going show .complete graph adding one layers ggplot(). function geom_point() adds layer points plot, creates scatterplot. ggplot2 comes many geom functions add different type layer plot. ’ll learn whole bunch throughout chapter.geom function ggplot2 takes mapping argument. defines variables dataset mapped visual properties. mapping argument always paired aes(), x y arguments aes() specify variables map x y axes. ggplot2 looks mapped variables data argument, case, mpg.","code":"\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))"},{"path":"data-visualisation.html","id":"a-graphing-template","chapter":"3 Data visualisation","heading":"3.2.3 A graphing template","text":"Let’s turn code reusable template making graphs ggplot2. make graph, replace bracketed sections code dataset, geom function, collection mappings.rest chapter show complete extend template make different types graphs. begin <MAPPINGS> component.","code":"ggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(mapping = aes(<MAPPINGS>))"},{"path":"data-visualisation.html","id":"exercises","chapter":"3 Data visualisation","heading":"3.2.4 Exercises","text":"Run ggplot(data = mpg). see?Run ggplot(data = mpg). see?many rows mpg? many columns?many rows mpg? many columns?drv variable describe? Read help ?mpg find\n.drv variable describe? Read help ?mpg find\n.Make scatterplot hwy vs cyl.Make scatterplot hwy vs cyl.happens make scatterplot class vs drv? \nplot useful?happens make scatterplot class vs drv? \nplot useful?","code":""},{"path":"data-visualisation.html","id":"aesthetic-mappings","chapter":"3 Data visualisation","heading":"3.3 Aesthetic mappings","text":"“greatest value picture forces us notice \nnever expected see.” — John TukeyIn plot , one group points (highlighted red) seems fall outside linear trend. cars higher mileage might expect. can explain cars?Let’s hypothesize cars hybrids. One way test hypothesis look class value car. class variable mpg dataset classifies cars groups compact, midsize, SUV. outlying points hybrids, classified compact cars , perhaps, subcompact cars (keep mind data collected hybrid trucks SUVs became popular).can add third variable, like class, two dimensional scatterplot mapping aesthetic. aesthetic visual property objects plot. Aesthetics include things like size, shape, color points. can display point (like one ) different ways changing values aesthetic properties. Since already use word “value” describe data, let’s use word “level” describe aesthetic properties. change levels point’s size, shape, color make point small, triangular, blue:can convey information data mapping aesthetics plot variables dataset. example, can map colors points class variable reveal class car.(prefer British English, like Hadley, can use colour instead color.)map aesthetic variable, associate name aesthetic name variable inside aes(). ggplot2 automatically assign unique level aesthetic (unique color) unique value variable, process known scaling. ggplot2 also add legend explains levels correspond values.colors reveal many unusual points two-seater cars. cars don’t seem like hybrids, , fact, sports cars! Sports cars large engines like SUVs pickup trucks, small bodies like midsize compact cars, improves gas mileage. hindsight, cars unlikely hybrids since large engines.example, mapped class color aesthetic, mapped class size aesthetic way. case, exact size point reveal class affiliation. get warning , mapping unordered variable (class) ordered aesthetic (size) good idea.mapped class alpha aesthetic, controls transparency points, shape aesthetic, controls shape points.happened SUVs? ggplot2 use six shapes time. default, additional groups go unplotted use shape aesthetic.aesthetic, use aes() associate name aesthetic variable display. aes() function gathers together aesthetic mappings used layer passes layer’s mapping argument. syntax highlights useful insight x y: x y locations point aesthetics, visual properties can map variables display information data.map aesthetic, ggplot2 takes care rest. selects reasonable scale use aesthetic, constructs legend explains mapping levels values. x y aesthetics, ggplot2 create legend, creates axis line tick marks label. axis line acts legend; explains mapping locations values.can also set aesthetic properties geom manually. example, can make points plot blue:, color doesn’t convey information variable, changes appearance plot. set aesthetic manually, set aesthetic name argument geom function; .e. goes outside aes(). ’ll need pick level makes sense aesthetic:name color character string.name color character string.size point mm.size point mm.shape point number, shown Figure 3.1.shape point number, shown Figure 3.1.\nFigure 3.1: R 25 built shapes identified numbers. seeming duplicates: example, 0, 15, 22 squares. difference comes interaction colour fill aesthetics. hollow shapes (0–14) border determined colour; solid shapes (15–20) filled colour; filled shapes (21–24) border colour filled fill.\n","code":"\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = class))\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, size = class))\n#> Warning: Using size for a discrete variable is not advised.\n# Left\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, alpha = class))\n\n# Right\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, shape = class))\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy), color = \"blue\")"},{"path":"data-visualisation.html","id":"exercises-1","chapter":"3 Data visualisation","heading":"3.3.1 Exercises","text":"’s gone wrong code? points blue?\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = \"blue\"))\n’s gone wrong code? points blue?variables mpg categorical? variables continuous?\n(Hint: type ?mpg read documentation dataset). \ncan see information run mpg?variables mpg categorical? variables continuous?\n(Hint: type ?mpg read documentation dataset). \ncan see information run mpg?Map continuous variable color, size, shape. \naesthetics behave differently categorical vs. continuous\nvariables?Map continuous variable color, size, shape. \naesthetics behave differently categorical vs. continuous\nvariables?happens map variable multiple aesthetics?happens map variable multiple aesthetics?stroke aesthetic ? shapes work ?\n(Hint: use ?geom_point)stroke aesthetic ? shapes work ?\n(Hint: use ?geom_point)happens map aesthetic something variable\nname, like aes(colour = displ < 5)? Note, ’ll also need specify x y.happens map aesthetic something variable\nname, like aes(colour = displ < 5)? Note, ’ll also need specify x y.","code":"\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy, color = \"blue\"))"},{"path":"data-visualisation.html","id":"common-problems","chapter":"3 Data visualisation","heading":"3.4 Common problems","text":"start run R code, ’re likely run problems. Don’t worry — happens everyone. writing R code years, every day still write code doesn’t work!Start carefully comparing code ’re running code book. R extremely picky, misplaced character can make difference. Make sure every ( matched ) every \" paired another \". Sometimes ’ll run code nothing happens. Check left-hand console: ’s +, means R doesn’t think ’ve typed complete expression ’s waiting finish . case, ’s usually easy start scratch pressing ESCAPE abort processing current command.One common problem creating ggplot2 graphics put + wrong place: come end line, start. words, make sure haven’t accidentally written code like :’re still stuck, try help. can get help R function running ?function_name console, selecting function name pressing F1 RStudio. Don’t worry help doesn’t seem helpful - instead skip examples look code matches ’re trying .doesn’t help, carefully read error message. Sometimes answer buried ! ’re new R, answer might error message don’t yet know understand . Another great tool Google: try googling error message, ’s likely someone else problem, gotten help online.","code":"\nggplot(data = mpg) \n+ geom_point(mapping = aes(x = displ, y = hwy))"},{"path":"data-visualisation.html","id":"facets","chapter":"3 Data visualisation","heading":"3.5 Facets","text":"One way add additional variables aesthetics. Another way, particularly useful categorical variables, split plot facets, subplots display one subset data.facet plot single variable, use facet_wrap(). first argument facet_wrap() formula, create ~ followed variable name (“formula” name data structure R, synonym “equation”). variable pass facet_wrap() discrete.facet plot combination two variables, add facet_grid() plot call. first argument facet_grid() also formula. time formula contain two variable names separated ~.prefer facet rows columns dimension, use . instead variable name, e.g. + facet_grid(. ~ cyl).","code":"\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ class, nrow = 2)\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_grid(drv ~ cyl)"},{"path":"data-visualisation.html","id":"exercises-2","chapter":"3 Data visualisation","heading":"3.5.1 Exercises","text":"happens facet continuous variable?happens facet continuous variable?empty cells plot facet_grid(drv ~ cyl) mean?\nrelate plot?\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = drv, y = cyl))empty cells plot facet_grid(drv ~ cyl) mean?\nrelate plot?plots following code make? . ?\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  facet_grid(drv ~ .)\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  facet_grid(. ~ cyl)plots following code make? . ?Take first faceted plot section:\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ class, nrow = 2)\nadvantages using faceting instead colour aesthetic?\ndisadvantages? might balance change \nlarger dataset?Take first faceted plot section:advantages using faceting instead colour aesthetic?\ndisadvantages? might balance change \nlarger dataset?Read ?facet_wrap. nrow ? ncol ? \noptions control layout individual panels? doesn’t\nfacet_grid() nrow ncol arguments?Read ?facet_wrap. nrow ? ncol ? \noptions control layout individual panels? doesn’t\nfacet_grid() nrow ncol arguments?using facet_grid() usually put variable \nunique levels columns. ?using facet_grid() usually put variable \nunique levels columns. ?","code":"\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = drv, y = cyl))\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  facet_grid(drv ~ .)\n\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  facet_grid(. ~ cyl)\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) + \n  facet_wrap(~ class, nrow = 2)"},{"path":"data-visualisation.html","id":"geometric-objects","chapter":"3 Data visualisation","heading":"3.6 Geometric objects","text":"two plots similar?plots contain x variable, y variable, describe data. plots identical. plot uses different visual object represent data. ggplot2 syntax, say use different geoms.geom geometrical object plot uses represent data. People often describe plots type geom plot uses. example, bar charts use bar geoms, line charts use line geoms, boxplots use boxplot geoms, . Scatterplots break trend; use point geom. see , can use different geoms plot data. plot left uses point geom, plot right uses smooth geom, smooth line fitted data.change geom plot, change geom function add ggplot(). instance, make plots , can use code:Every geom function ggplot2 takes mapping argument. However, every aesthetic works every geom. set shape point, couldn’t set “shape” line. hand, set linetype line. geom_smooth() draw different line, different linetype, unique value variable map linetype.geom_smooth() separates cars three lines based drv value, describes car’s drivetrain. One line describes points 4 value, one line describes points f value, one line describes points r value. , 4 stands four-wheel drive, f front-wheel drive, r rear-wheel drive.sounds strange, can make clear overlaying lines top raw data coloring everything according drv.Notice plot contains two geoms graph! makes excited, buckle . learn place multiple geoms plot soon.ggplot2 provides 40 geoms, extension packages provide even (see https://exts.ggplot2.tidyverse.org/gallery/ sampling). best way get comprehensive overview ggplot2 cheatsheet, can find http://rstudio.com/resources/cheatsheets. learn single geom, use help: ?geom_smooth.Many geoms, like geom_smooth(), use single geometric object display multiple rows data. geoms, can set group aesthetic categorical variable draw multiple objects. ggplot2 draw separate object unique value grouping variable. practice, ggplot2 automatically group data geoms whenever map aesthetic discrete variable (linetype example). convenient rely feature group aesthetic add legend distinguishing features geoms.display multiple geoms plot, add multiple geom functions ggplot():, however, introduces duplication code. Imagine wanted change y-axis display cty instead hwy. ’d need change variable two places, might forget update one. can avoid type repetition passing set mappings ggplot(). ggplot2 treat mappings global mappings apply geom graph. words, code produce plot previous code:place mappings geom function, ggplot2 treat local mappings layer. use mappings extend overwrite global mappings layer . makes possible display different aesthetics different layers.can use idea specify different data layer. , smooth line displays just subset mpg dataset, subcompact cars. local data argument geom_smooth() overrides global data argument ggplot() layer .(’ll learn filter() works chapter data transformations: now, just know command selects subcompact cars.)","code":"\n# left\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))\n\n# right\nggplot(data = mpg) + \n  geom_smooth(mapping = aes(x = displ, y = hwy))\nggplot(data = mpg) + \n  geom_smooth(mapping = aes(x = displ, y = hwy, linetype = drv))\nggplot(data = mpg) +\n  geom_smooth(mapping = aes(x = displ, y = hwy))\n              \nggplot(data = mpg) +\n  geom_smooth(mapping = aes(x = displ, y = hwy, group = drv))\n    \nggplot(data = mpg) +\n  geom_smooth(\n    mapping = aes(x = displ, y = hwy, color = drv),\n    show.legend = FALSE\n  )\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy)) +\n  geom_smooth(mapping = aes(x = displ, y = hwy))\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_smooth()\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(color = class)) + \n  geom_smooth()\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point(mapping = aes(color = class)) + \n  geom_smooth(data = filter(mpg, class == \"subcompact\"), se = FALSE)"},{"path":"data-visualisation.html","id":"exercises-3","chapter":"3 Data visualisation","heading":"3.6.1 Exercises","text":"geom use draw line chart? boxplot?\nhistogram? area chart?geom use draw line chart? boxplot?\nhistogram? area chart?Run code head predict output look like.\n, run code R check predictions.\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + \n  geom_point() + \n  geom_smooth(se = FALSE)Run code head predict output look like.\n, run code R check predictions.show.legend = FALSE ? happens remove ?\nthink used earlier chapter?show.legend = FALSE ? happens remove ?\nthink used earlier chapter?se argument geom_smooth() ?se argument geom_smooth() ?two graphs look different? /?\n\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_smooth()\n\nggplot() + \n  geom_point(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_smooth(data = mpg, mapping = aes(x = displ, y = hwy))two graphs look different? /?Recreate R code necessary generate following graphs.\nRecreate R code necessary generate following graphs.","code":"\nggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + \n  geom_point() + \n  geom_smooth(se = FALSE)\nggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_point() + \n  geom_smooth()\n\nggplot() + \n  geom_point(data = mpg, mapping = aes(x = displ, y = hwy)) + \n  geom_smooth(data = mpg, mapping = aes(x = displ, y = hwy))"},{"path":"data-visualisation.html","id":"statistical-transformations","chapter":"3 Data visualisation","heading":"3.7 Statistical transformations","text":"Next, let’s take look bar chart. Bar charts seem simple, interesting reveal something subtle plots. Consider basic bar chart, drawn geom_bar(). following chart displays total number diamonds diamonds dataset, grouped cut. diamonds dataset comes ggplot2 contains information ~54,000 diamonds, including price, carat, color, clarity, cut diamond. chart shows diamonds available high quality cuts low quality cuts.x-axis, chart displays cut, variable diamonds. y-axis, displays count, count variable diamonds! count come ? Many graphs, like scatterplots, plot raw values dataset. graphs, like bar charts, calculate new values plot:bar charts, histograms, frequency polygons bin data\nplot bin counts, number points fall bin.bar charts, histograms, frequency polygons bin data\nplot bin counts, number points fall bin.smoothers fit model data plot predictions \nmodel.smoothers fit model data plot predictions \nmodel.boxplots compute robust summary distribution display \nspecially formatted box.boxplots compute robust summary distribution display \nspecially formatted box.algorithm used calculate new values graph called stat, short statistical transformation. figure describes process works geom_bar().can learn stat geom uses inspecting default value stat argument. example, ?geom_bar shows default value stat “count”, means geom_bar() uses stat_count(). stat_count() documented page geom_bar(), scroll can find section called “Computed variables”. describes computes two new variables: count prop.can generally use geoms stats interchangeably. example, can recreate previous plot using stat_count() instead geom_bar():works every geom default stat; every stat default geom. means can typically use geoms without worrying underlying statistical transformation. three reasons might need use stat explicitly:might want override default stat. code , change\nstat geom_bar() count (default) identity. lets\nmap height bars raw values \\(y\\) variable.\nUnfortunately people talk bar charts casually, might \nreferring type bar chart, height bar already\npresent data, previous bar chart height bar\ngenerated counting rows.\n\ndemo <- tribble(\n  ~cut,         ~freq,\n  \"Fair\",       1610,\n  \"Good\",       4906,\n  \"Good\",  12082,\n  \"Premium\",    13791,\n  \"Ideal\",      21551\n)\n\nggplot(data = demo) +\n  geom_bar(mapping = aes(x = cut, y = freq), stat = \"identity\")\n\n(Don’t worry haven’t seen <- tribble() . might \nable guess meaning context, ’ll learn exactly\nsoon!)might want override default stat. code , change\nstat geom_bar() count (default) identity. lets\nmap height bars raw values \\(y\\) variable.\nUnfortunately people talk bar charts casually, might \nreferring type bar chart, height bar already\npresent data, previous bar chart height bar\ngenerated counting rows.(Don’t worry haven’t seen <- tribble() . might \nable guess meaning context, ’ll learn exactly\nsoon!)might want override default mapping transformed variables\naesthetics. example, might want display bar chart \nproportion, rather count:\n\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, y = after_stat(prop), group = 1))\n\nfind variables computed stat, look help section\ntitled “computed variables”.might want override default mapping transformed variables\naesthetics. example, might want display bar chart \nproportion, rather count:find variables computed stat, look help section\ntitled “computed variables”.might want draw greater attention statistical transformation\ncode. example, might use stat_summary(), \nsummarises y values unique x value, draw\nattention summary ’re computing:\n\nggplot(data = diamonds) + \n  stat_summary(\n    mapping = aes(x = cut, y = depth),\n    fun.min = min,\n    fun.max = max,\n    fun = median\n  )\nmight want draw greater attention statistical transformation\ncode. example, might use stat_summary(), \nsummarises y values unique x value, draw\nattention summary ’re computing:ggplot2 provides 20 stats use. stat function, can get help usual way, e.g. ?stat_bin. see complete list stats, try ggplot2 cheatsheet.","code":"\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut))\nggplot(data = diamonds) + \n  stat_count(mapping = aes(x = cut))\ndemo <- tribble(\n  ~cut,         ~freq,\n  \"Fair\",       1610,\n  \"Good\",       4906,\n  \"Very Good\",  12082,\n  \"Premium\",    13791,\n  \"Ideal\",      21551\n)\n\nggplot(data = demo) +\n  geom_bar(mapping = aes(x = cut, y = freq), stat = \"identity\")\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, y = after_stat(prop), group = 1))\nggplot(data = diamonds) + \n  stat_summary(\n    mapping = aes(x = cut, y = depth),\n    fun.min = min,\n    fun.max = max,\n    fun = median\n  )"},{"path":"data-visualisation.html","id":"exercises-4","chapter":"3 Data visualisation","heading":"3.7.1 Exercises","text":"default geom associated stat_summary()? \nrewrite previous plot use geom function instead \nstat function?default geom associated stat_summary()? \nrewrite previous plot use geom function instead \nstat function?geom_col() ? different geom_bar()?geom_col() ? different geom_bar()?geoms stats come pairs almost always used \nconcert. Read documentation make list \npairs. common?geoms stats come pairs almost always used \nconcert. Read documentation make list \npairs. common?variables stat_smooth() compute? parameters control\nbehaviour?variables stat_smooth() compute? parameters control\nbehaviour?proportion bar chart, need set group = 1. ? \nwords problem two graphs?\n\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, y = after_stat(prop)))\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, fill = color, y = after_stat(prop)))proportion bar chart, need set group = 1. ? \nwords problem two graphs?","code":"\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, y = after_stat(prop)))\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, fill = color, y = after_stat(prop)))"},{"path":"data-visualisation.html","id":"position-adjustments","chapter":"3 Data visualisation","heading":"3.8 Position adjustments","text":"’s one piece magic associated bar charts. can colour bar chart using either colour aesthetic, , usefully, fill:Note happens map fill aesthetic another variable, like clarity: bars automatically stacked. colored rectangle represents combination cut clarity.stacking performed automatically position adjustment specified position argument. don’t want stacked bar chart, can use one three options: \"identity\", \"dodge\" \"fill\".position = \"identity\" place object exactly falls \ncontext graph. useful bars, \noverlaps . see overlapping either need make bars\nslightly transparent setting alpha small value, completely\ntransparent setting fill = NA.\n\nggplot(data = diamonds, mapping = aes(x = cut, fill = clarity)) + \n  geom_bar(alpha = 1/5, position = \"identity\")\nggplot(data = diamonds, mapping = aes(x = cut, colour = clarity)) + \n  geom_bar(fill = NA, position = \"identity\")\n\nidentity position adjustment useful 2d geoms, like points,\ndefault.position = \"identity\" place object exactly falls \ncontext graph. useful bars, \noverlaps . see overlapping either need make bars\nslightly transparent setting alpha small value, completely\ntransparent setting fill = NA.identity position adjustment useful 2d geoms, like points,\ndefault.position = \"fill\" works like stacking, makes set stacked bars\nheight. makes easier compare proportions across\ngroups.\n\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, fill = clarity), position = \"fill\")\nposition = \"fill\" works like stacking, makes set stacked bars\nheight. makes easier compare proportions across\ngroups.position = \"dodge\" places overlapping objects directly beside one\nanother. makes easier compare individual values.\n\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, fill = clarity), position = \"dodge\")\nposition = \"dodge\" places overlapping objects directly beside one\nanother. makes easier compare individual values.’s one type adjustment ’s useful bar charts, can useful scatterplots. Recall first scatterplot. notice plot displays 126 points, even though 234 observations dataset?values hwy displ rounded points appear grid many points overlap . problem known overplotting. arrangement makes hard see mass data . data points spread equally throughout graph, one special combination hwy displ contains 109 values?can avoid gridding setting position adjustment “jitter”. position = \"jitter\" adds small amount random noise point. spreads points two points likely receive amount random noise.Adding randomness seems like strange way improve plot, makes graph less accurate small scales, makes graph revealing large scales. useful operation, ggplot2 comes shorthand geom_point(position = \"jitter\"): geom_jitter().learn position adjustment, look help page associated adjustment: ?position_dodge, ?position_fill, ?position_identity, ?position_jitter, ?position_stack.","code":"\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, colour = cut))\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, fill = cut))\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, fill = clarity))\nggplot(data = diamonds, mapping = aes(x = cut, fill = clarity)) + \n  geom_bar(alpha = 1/5, position = \"identity\")\nggplot(data = diamonds, mapping = aes(x = cut, colour = clarity)) + \n  geom_bar(fill = NA, position = \"identity\")\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, fill = clarity), position = \"fill\")\nggplot(data = diamonds) + \n  geom_bar(mapping = aes(x = cut, fill = clarity), position = \"dodge\")\nggplot(data = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy), position = \"jitter\")"},{"path":"data-visualisation.html","id":"exercises-5","chapter":"3 Data visualisation","heading":"3.8.1 Exercises","text":"problem plot? improve ?\n\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + \n  geom_point()\nproblem plot? improve ?parameters geom_jitter() control amount jittering?parameters geom_jitter() control amount jittering?Compare contrast geom_jitter() geom_count().Compare contrast geom_jitter() geom_count().’s default position adjustment geom_boxplot()? Create\nvisualisation mpg dataset demonstrates .’s default position adjustment geom_boxplot()? Create\nvisualisation mpg dataset demonstrates .","code":"\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) + \n  geom_point()"},{"path":"data-visualisation.html","id":"coordinate-systems","chapter":"3 Data visualisation","heading":"3.9 Coordinate systems","text":"Coordinate systems probably complicated part ggplot2. default coordinate system Cartesian coordinate system x y positions act independently determine location point. number coordinate systems occasionally helpful.coord_flip() switches x y axes. useful (example),\nwant horizontal boxplots. ’s also useful long labels: ’s\nhard get fit without overlapping x-axis.\n\nggplot(data = mpg, mapping = aes(x = class, y = hwy)) + \n  geom_boxplot()\nggplot(data = mpg, mapping = aes(x = class, y = hwy)) + \n  geom_boxplot() +\n  coord_flip()\ncoord_flip() switches x y axes. useful (example),\nwant horizontal boxplots. ’s also useful long labels: ’s\nhard get fit without overlapping x-axis.coord_quickmap() sets aspect ratio correctly maps. \nimportant ’re plotting spatial data ggplot2 (unfortunately\ndon’t space cover book).\n\nnz <- map_data(\"nz\")\n\nggplot(nz, aes(long, lat, group = group)) +\n  geom_polygon(fill = \"white\", colour = \"black\")\n\nggplot(nz, aes(long, lat, group = group)) +\n  geom_polygon(fill = \"white\", colour = \"black\") +\n  coord_quickmap()\ncoord_quickmap() sets aspect ratio correctly maps. \nimportant ’re plotting spatial data ggplot2 (unfortunately\ndon’t space cover book).coord_polar() uses polar coordinates. Polar coordinates reveal \ninteresting connection bar chart Coxcomb chart.\n\nbar <- ggplot(data = diamonds) + \n  geom_bar(\n    mapping = aes(x = cut, fill = cut), \n    show.legend = FALSE,\n    width = 1\n  ) + \n  theme(aspect.ratio = 1) +\n  labs(x = NULL, y = NULL)\n\nbar + coord_flip()\nbar + coord_polar()\ncoord_polar() uses polar coordinates. Polar coordinates reveal \ninteresting connection bar chart Coxcomb chart.","code":"\nggplot(data = mpg, mapping = aes(x = class, y = hwy)) + \n  geom_boxplot()\nggplot(data = mpg, mapping = aes(x = class, y = hwy)) + \n  geom_boxplot() +\n  coord_flip()\nnz <- map_data(\"nz\")\n\nggplot(nz, aes(long, lat, group = group)) +\n  geom_polygon(fill = \"white\", colour = \"black\")\n\nggplot(nz, aes(long, lat, group = group)) +\n  geom_polygon(fill = \"white\", colour = \"black\") +\n  coord_quickmap()\nbar <- ggplot(data = diamonds) + \n  geom_bar(\n    mapping = aes(x = cut, fill = cut), \n    show.legend = FALSE,\n    width = 1\n  ) + \n  theme(aspect.ratio = 1) +\n  labs(x = NULL, y = NULL)\n\nbar + coord_flip()\nbar + coord_polar()"},{"path":"data-visualisation.html","id":"exercises-6","chapter":"3 Data visualisation","heading":"3.9.1 Exercises","text":"Turn stacked bar chart pie chart using coord_polar().Turn stacked bar chart pie chart using coord_polar().labs() ? Read documentation.labs() ? Read documentation.’s difference coord_quickmap() coord_map()?’s difference coord_quickmap() coord_map()?plot tell relationship city\nhighway mpg? coord_fixed() important? \ngeom_abline() ?\n\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\n  geom_point() + \n  geom_abline() +\n  coord_fixed()\nplot tell relationship city\nhighway mpg? coord_fixed() important? \ngeom_abline() ?","code":"\nggplot(data = mpg, mapping = aes(x = cty, y = hwy)) +\n  geom_point() + \n  geom_abline() +\n  coord_fixed()"},{"path":"data-visualisation.html","id":"the-layered-grammar-of-graphics","chapter":"3 Data visualisation","heading":"3.10 The layered grammar of graphics","text":"previous sections, learned much make scatterplots, bar charts, boxplots. learned foundation can use make type plot ggplot2. see , let’s add position adjustments, stats, coordinate systems, faceting code template:new template takes seven parameters, bracketed words appear template. practice, rarely need supply seven parameters make graph ggplot2 provide useful defaults everything except data, mappings, geom function.seven parameters template compose grammar graphics, formal system building plots. grammar graphics based insight can uniquely describe plot combination dataset, geom, set mappings, stat, position adjustment, coordinate system, faceting scheme.see works, consider build basic plot scratch: start dataset transform information want display (stat).Next, choose geometric object represent observation transformed data. use aesthetic properties geoms represent variables data. map values variable levels aesthetic.’d select coordinate system place geoms . ’d use location objects (aesthetic property) display values x y variables. point, complete graph, adjust positions geoms within coordinate system (position adjustment) split graph subplots (faceting). also extend plot adding one additional layers, additional layer uses dataset, geom, set mappings, stat, position adjustment.use method build plot imagine. words, can use code template ’ve learned chapter build hundreds thousands unique plots.","code":"ggplot(data = <DATA>) + \n  <GEOM_FUNCTION>(\n     mapping = aes(<MAPPINGS>),\n     stat = <STAT>, \n     position = <POSITION>\n  ) +\n  <COORDINATE_FUNCTION> +\n  <FACET_FUNCTION>"},{"path":"workflow-basics.html","id":"workflow-basics","chapter":"4 Workflow: basics","heading":"4 Workflow: basics","text":"now experience running R code. didn’t give many details, ’ve obviously figured basics, ’ve thrown book away frustration! Frustration natural start programming R, stickler punctuation, even one character place cause complain. expect little frustrated, take comfort ’s typical temporary: happens everyone, way get keep trying.go , let’s make sure ’ve got solid foundation running R code, know helpful RStudio features.","code":""},{"path":"workflow-basics.html","id":"coding-basics","chapter":"4 Workflow: basics","heading":"4.1 Coding basics","text":"Let’s review basics ’ve far omitted interests getting plotting quickly possible. can use R calculator:can create new objects <-:R statements create objects, assignment statements, form:reading code say “object name gets value” head.make lots assignments <- pain type. Don’t lazy use =: work, cause confusion later. Instead, use RStudio’s keyboard shortcut: Alt + - (minus sign). Notice RStudio automagically surrounds <- spaces, good code formatting practice. Code miserable read good day, giveyoureyesabreak use spaces.","code":"\n1 / 200 * 30\n#> [1] 0.15\n(59 + 73 + 2) / 3\n#> [1] 44.66667\nsin(pi / 2)\n#> [1] 1\nx <- 3 * 4\nobject_name <- value"},{"path":"workflow-basics.html","id":"whats-in-a-name","chapter":"4 Workflow: basics","heading":"4.2 What’s in a name?","text":"Object names must start letter, can contain letters, numbers, _ .. want object names descriptive, ’ll need convention multiple words. recommend snake_case separate lowercase words _.’ll come back code style later, functions.can inspect object typing name:Make another assignment:inspect object, try RStudio’s completion facility: type “”, press TAB, add characters unique prefix, press return.Ooops, made mistake! this_is_a_really_long_name value 3.5 2.5. Use another keyboard shortcut help fix . Type “” press Cmd/Ctrl + ↑. list commands ’ve typed start letters. Use arrow keys navigate, press enter retype command. Change 2.5 3.5 rerun.Make yet another assignment:Let’s try inspect :’s implied contract R: tedious computation , return, must completely precise instructions. Typos matter. Case matters.","code":"\ni_use_snake_case\notherPeopleUseCamelCase\nsome.people.use.periods\nAnd_aFew.People_RENOUNCEconvention\nx\n#> [1] 12\nthis_is_a_really_long_name <- 2.5\nr_rocks <- 2 ^ 3\nr_rock\n#> Error: object 'r_rock' not found\nR_rocks\n#> Error: object 'R_rocks' not found"},{"path":"workflow-basics.html","id":"calling-functions","chapter":"4 Workflow: basics","heading":"4.3 Calling functions","text":"R large collection built-functions called like :Let’s try using seq() makes regular sequences numbers , ’re , learn helpful features RStudio. Type se hit TAB. popup shows possible completions. Specify seq() typing (“q”) disambiguate, using ↑/↓ arrows select. Notice floating tooltip pops , reminding function’s arguments purpose. want help, press F1 get details help tab lower right pane.Press TAB ’ve selected function want. RStudio add matching opening (() closing ()) parentheses . Type arguments 1, 10 hit return.Type code notice get similar assistance paired quotation marks:Quotation marks parentheses must always come pair. RStudio best help , ’s still possible mess end mismatch. happens, R show continuation character “+”:+ tells R waiting input; doesn’t think ’re done yet. Usually means ’ve forgotten either \" ). Either add missing pair, press ESCAPE abort expression try .make assignment, don’t get see value. ’re tempted immediately double-check result:common action can shortened surrounding assignment parentheses, causes assignment “print screen” happen.Now look environment upper right pane:can see objects ’ve created.","code":"\nfunction_name(arg1 = val1, arg2 = val2, ...)\nseq(1, 10)\n#>  [1]  1  2  3  4  5  6  7  8  9 10\nx <- \"hello world\"> x <- \"hello\n+\ny <- seq(1, 10, length.out = 5)\ny\n#> [1]  1.00  3.25  5.50  7.75 10.00\n(y <- seq(1, 10, length.out = 5))\n#> [1]  1.00  3.25  5.50  7.75 10.00"},{"path":"workflow-basics.html","id":"exercises-7","chapter":"4 Workflow: basics","heading":"4.4 Exercises","text":"code work?\n\nmy_variable <- 10\nmy_varıable\n#> Error eval(expr, envir, enclos): object 'my_varıable' found\nLook carefully! (may seem like exercise pointlessness, \ntraining brain notice even tiniest difference pay \nprogramming.)code work?Look carefully! (may seem like exercise pointlessness, \ntraining brain notice even tiniest difference pay \nprogramming.)Tweak following R commands run correctly:\n\nlibrary(tidyverse)\n\nggplot(dota = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))\n\nfliter(mpg, cyl = 8)\nfilter(diamond, carat > 3)Tweak following R commands run correctly:Press Alt + Shift + K. happens? can get place\nusing menus?Press Alt + Shift + K. happens? can get place\nusing menus?","code":"\nmy_variable <- 10\nmy_varıable\n#> Error in eval(expr, envir, enclos): object 'my_varıable' not found\nlibrary(tidyverse)\n\nggplot(dota = mpg) + \n  geom_point(mapping = aes(x = displ, y = hwy))\n\nfliter(mpg, cyl = 8)\nfilter(diamond, carat > 3)"},{"path":"transform.html","id":"transform","chapter":"5 Data transformation","heading":"5 Data transformation","text":"","code":""},{"path":"transform.html","id":"introduction-2","chapter":"5 Data transformation","heading":"5.1 Introduction","text":"Visualisation important tool insight generation, rare get data exactly right form need. Often ’ll need create new variables summaries, maybe just want rename variables reorder observations order make data little easier work . ’ll learn (!) chapter, teach transform data using dplyr package new dataset flights departing New York City 2013.","code":""},{"path":"transform.html","id":"prerequisites-2","chapter":"5 Data transformation","heading":"5.1.1 Prerequisites","text":"chapter ’re going focus use dplyr package, another core member tidyverse. ’ll illustrate key ideas using data nycflights13 package, use ggplot2 help us understand data.Take careful note conflicts message ’s printed load tidyverse. tells dplyr overwrites functions base R. want use base version functions loading dplyr, ’ll need use full names: stats::filter() stats::lag().","code":"\nlibrary(nycflights13)\nlibrary(tidyverse)"},{"path":"transform.html","id":"nycflights13","chapter":"5 Data transformation","heading":"5.1.2 nycflights13","text":"explore basic data manipulation verbs dplyr, ’ll use nycflights13::flights. data frame contains 336,776 flights departed New York City 2013. data comes US Bureau Transportation Statistics, documented ?flights.might notice data frame prints little differently data frames might used past: shows first rows columns fit one screen. (see whole dataset, can run View(flights) open dataset RStudio viewer). prints differently ’s tibble. Tibbles data frames, slightly tweaked work better tidyverse. now, don’t need worry differences; ’ll come back tibbles detail wrangle.might also noticed row three (four) letter abbreviations column names. describe type variable:int stands integers.int stands integers.dbl stands doubles, real numbers.dbl stands doubles, real numbers.chr stands character vectors, strings.chr stands character vectors, strings.dttm stands date-times (date + time).dttm stands date-times (date + time).three common types variables aren’t used dataset ’ll encounter later book:lgl stands logical, vectors contain TRUE FALSE.lgl stands logical, vectors contain TRUE FALSE.fctr stands factors, R uses represent categorical variables\nfixed possible values.fctr stands factors, R uses represent categorical variables\nfixed possible values.date stands dates.date stands dates.","code":"\nflights\n#> # A tibble: 336,776 x 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # … with 336,770 more rows, and 11 more variables: arr_delay <dbl>,\n#> #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#> #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>"},{"path":"transform.html","id":"dplyr-basics","chapter":"5 Data transformation","heading":"5.1.3 dplyr basics","text":"chapter going learn five key dplyr functions allow solve vast majority data manipulation challenges:Pick observations values (filter()).Reorder rows (arrange()).Pick variables names (select()).Create new variables functions existing variables (mutate()).Collapse many values single summary (summarise()).can used conjunction group_by() changes scope function operating entire dataset operating group--group. six functions provide verbs language data manipulation.verbs work similarly:first argument data frame.first argument data frame.subsequent arguments describe data frame,\nusing variable names (without quotes).subsequent arguments describe data frame,\nusing variable names (without quotes).result new data frame.result new data frame.Together properties make easy chain together multiple simple steps achieve complex result. Let’s dive see verbs work.","code":""},{"path":"transform.html","id":"filter-rows-with-filter","chapter":"5 Data transformation","heading":"5.2 Filter rows with filter()","text":"filter() allows subset observations based values. first argument name data frame. second subsequent arguments expressions filter data frame. example, can select flights January 1st :run line code, dplyr executes filtering operation returns new data frame. dplyr functions never modify inputs, want save result, ’ll need use assignment operator, <-:R either prints results, saves variable. want , can wrap assignment parentheses:","code":"\nfilter(flights, month == 1, day == 1)\n#> # A tibble: 842 x 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # … with 836 more rows, and 11 more variables: arr_delay <dbl>, carrier <chr>,\n#> #   flight <int>, tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,\n#> #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>\njan1 <- filter(flights, month == 1, day == 1)\n(dec25 <- filter(flights, month == 12, day == 25))\n#> # A tibble: 719 x 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013    12    25      456            500        -4      649            651\n#> 2  2013    12    25      524            515         9      805            814\n#> 3  2013    12    25      542            540         2      832            850\n#> 4  2013    12    25      546            550        -4     1022           1027\n#> 5  2013    12    25      556            600        -4      730            745\n#> 6  2013    12    25      557            600        -3      743            752\n#> # … with 713 more rows, and 11 more variables: arr_delay <dbl>, carrier <chr>,\n#> #   flight <int>, tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,\n#> #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>"},{"path":"transform.html","id":"comparisons","chapter":"5 Data transformation","heading":"5.2.1 Comparisons","text":"use filtering effectively, know select observations want using comparison operators. R provides standard suite: >, >=, <, <=, != (equal), == (equal).’re starting R, easiest mistake make use = instead == testing equality. happens ’ll get informative error:’s another common problem might encounter using ==: floating point numbers. results might surprise !Computers use finite precision arithmetic (obviously can’t store infinite number digits!) remember every number see approximation. Instead relying ==, use near():","code":"\nfilter(flights, month = 1)\n#> Error: Problem with `filter()` input `..1`.\n#> x Input `..1` is named.\n#> ℹ This usually means that you've used `=` instead of `==`.\n#> ℹ Did you mean `month == 1`?\nsqrt(2) ^ 2 == 2\n#> [1] FALSE\n1 / 49 * 49 == 1\n#> [1] FALSE\nnear(sqrt(2) ^ 2,  2)\n#> [1] TRUE\nnear(1 / 49 * 49, 1)\n#> [1] TRUE"},{"path":"transform.html","id":"logical-operators","chapter":"5 Data transformation","heading":"5.2.2 Logical operators","text":"Multiple arguments filter() combined “”: every expression must true order row included output. types combinations, ’ll need use Boolean operators : & “”, | “”, ! “”. Figure 5.1 shows complete set Boolean operations.\nFigure 5.1: Complete set boolean operations. x left-hand circle, y right-hand circle, shaded region show parts operator selects.\nfollowing code finds flights departed November December:order operations doesn’t work like English. can’t write filter(flights, month == (11 | 12)), might literally translate “finds flights departed November December”. Instead finds months equal 11 | 12, expression evaluates TRUE. numeric context (like ), TRUE becomes one, finds flights January, November December. quite confusing!useful short-hand problem x %% y. select every row x one values y. use rewrite code :Sometimes can simplify complicated subsetting remembering De Morgan’s law: !(x & y) !x | !y, !(x | y) !x & !y. example, wanted find flights weren’t delayed (arrival departure) two hours, use either following two filters:well & |, R also && ||. Don’t use ! ’ll learn use conditional execution.Whenever start using complicated, multipart expressions filter(), consider making explicit variables instead. makes much easier check work. ’ll learn create new variables shortly.","code":"\nfilter(flights, month == 11 | month == 12)\nnov_dec <- filter(flights, month %in% c(11, 12))\nfilter(flights, !(arr_delay > 120 | dep_delay > 120))\nfilter(flights, arr_delay <= 120, dep_delay <= 120)"},{"path":"transform.html","id":"missing-values","chapter":"5 Data transformation","heading":"5.2.3 Missing values","text":"One important feature R can make comparison tricky missing values, NAs (“availables”). NA represents unknown value missing values “contagious”: almost operation involving unknown value also unknown.confusing result one:’s easiest understand true bit context:want determine value missing, use .na():filter() includes rows condition TRUE; excludes FALSE NA values. want preserve missing values, ask explicitly:","code":"\nNA > 5\n#> [1] NA\n10 == NA\n#> [1] NA\nNA + 10\n#> [1] NA\nNA / 2\n#> [1] NA\nNA == NA\n#> [1] NA\n# Let x be Mary's age. We don't know how old she is.\nx <- NA\n\n# Let y be John's age. We don't know how old he is.\ny <- NA\n\n# Are John and Mary the same age?\nx == y\n#> [1] NA\n# We don't know!\nis.na(x)\n#> [1] TRUE\ndf <- tibble(x = c(1, NA, 3))\nfilter(df, x > 1)\n#> # A tibble: 1 x 1\n#>       x\n#>   <dbl>\n#> 1     3\nfilter(df, is.na(x) | x > 1)\n#> # A tibble: 2 x 1\n#>       x\n#>   <dbl>\n#> 1    NA\n#> 2     3"},{"path":"transform.html","id":"exercises-8","chapter":"5 Data transformation","heading":"5.2.4 Exercises","text":"Find flights \narrival delay two hours\nFlew Houston (IAH HOU)\noperated United, American, Delta\nDeparted summer (July, August, September)\nArrived two hours late, didn’t leave late\ndelayed least hour, made 30 minutes flight\nDeparted midnight 6am (inclusive)\nFind flights thatHad arrival delay two hoursFlew Houston (IAH HOU)operated United, American, DeltaDeparted summer (July, August, September)Arrived two hours late, didn’t leave lateWere delayed least hour, made 30 minutes flightDeparted midnight 6am (inclusive)Another useful dplyr filtering helper (). ?\nCan use simplify code needed answer previous\nchallenges?Another useful dplyr filtering helper (). ?\nCan use simplify code needed answer previous\nchallenges?many flights missing dep_time? variables \nmissing? might rows represent?many flights missing dep_time? variables \nmissing? might rows represent?NA ^ 0 missing? NA | TRUE missing?\nFALSE & NA missing? Can figure general\nrule? (NA * 0 tricky counterexample!)NA ^ 0 missing? NA | TRUE missing?\nFALSE & NA missing? Can figure general\nrule? (NA * 0 tricky counterexample!)","code":""},{"path":"transform.html","id":"arrange-rows-with-arrange","chapter":"5 Data transformation","heading":"5.3 Arrange rows with arrange()","text":"arrange() works similarly filter() except instead selecting rows, changes order. takes data frame set column names (complicated expressions) order . provide one column name, additional column used break ties values preceding columns:Use desc() re-order column descending order:Missing values always sorted end:","code":"\narrange(flights, year, month, day)\n#> # A tibble: 336,776 x 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # … with 336,770 more rows, and 11 more variables: arr_delay <dbl>,\n#> #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#> #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>\narrange(flights, desc(dep_delay))\n#> # A tibble: 336,776 x 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     9      641            900      1301     1242           1530\n#> 2  2013     6    15     1432           1935      1137     1607           2120\n#> 3  2013     1    10     1121           1635      1126     1239           1810\n#> 4  2013     9    20     1139           1845      1014     1457           2210\n#> 5  2013     7    22      845           1600      1005     1044           1815\n#> 6  2013     4    10     1100           1900       960     1342           2211\n#> # … with 336,770 more rows, and 11 more variables: arr_delay <dbl>,\n#> #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#> #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>\ndf <- tibble(x = c(5, 2, NA))\narrange(df, x)\n#> # A tibble: 3 x 1\n#>       x\n#>   <dbl>\n#> 1     2\n#> 2     5\n#> 3    NA\narrange(df, desc(x))\n#> # A tibble: 3 x 1\n#>       x\n#>   <dbl>\n#> 1     5\n#> 2     2\n#> 3    NA"},{"path":"transform.html","id":"exercises-9","chapter":"5 Data transformation","heading":"5.3.1 Exercises","text":"use arrange() sort missing values start?\n(Hint: use .na()).use arrange() sort missing values start?\n(Hint: use .na()).Sort flights find delayed flights. Find flights \nleft earliest.Sort flights find delayed flights. Find flights \nleft earliest.Sort flights find fastest (highest speed) flights.Sort flights find fastest (highest speed) flights.flights travelled farthest? travelled shortest?flights travelled farthest? travelled shortest?","code":""},{"path":"transform.html","id":"select","chapter":"5 Data transformation","heading":"5.4 Select columns with select()","text":"’s uncommon get datasets hundreds even thousands variables. case, first challenge often narrowing variables ’re actually interested . select() allows rapidly zoom useful subset using operations based names variables.select() terribly useful flights data 19 variables, can still get general idea:number helper functions can use within select():starts_with(\"abc\"): matches names begin “abc”.starts_with(\"abc\"): matches names begin “abc”.ends_with(\"xyz\"): matches names end “xyz”.ends_with(\"xyz\"): matches names end “xyz”.contains(\"ijk\"): matches names contain “ijk”.contains(\"ijk\"): matches names contain “ijk”.matches(\"(.)\\\\1\"): selects variables match regular expression.\none matches variables contain repeated characters. ’ll\nlearn regular expressions strings.matches(\"(.)\\\\1\"): selects variables match regular expression.\none matches variables contain repeated characters. ’ll\nlearn regular expressions strings.num_range(\"x\", 1:3): matches x1, x2 x3.num_range(\"x\", 1:3): matches x1, x2 x3.See ?select details.select() can used rename variables, ’s rarely useful drops variables explicitly mentioned. Instead, use rename(), variant select() keeps variables aren’t explicitly mentioned:Another option use select() conjunction everything() helper. useful handful variables ’d like move start data frame.","code":"\n# Select columns by name\nselect(flights, year, month, day)\n#> # A tibble: 336,776 x 3\n#>    year month   day\n#>   <int> <int> <int>\n#> 1  2013     1     1\n#> 2  2013     1     1\n#> 3  2013     1     1\n#> 4  2013     1     1\n#> 5  2013     1     1\n#> 6  2013     1     1\n#> # … with 336,770 more rows\n# Select all columns between year and day (inclusive)\nselect(flights, year:day)\n#> # A tibble: 336,776 x 3\n#>    year month   day\n#>   <int> <int> <int>\n#> 1  2013     1     1\n#> 2  2013     1     1\n#> 3  2013     1     1\n#> 4  2013     1     1\n#> 5  2013     1     1\n#> 6  2013     1     1\n#> # … with 336,770 more rows\n# Select all columns except those from year to day (inclusive)\nselect(flights, -(year:day))\n#> # A tibble: 336,776 x 16\n#>   dep_time sched_dep_time dep_delay arr_time sched_arr_time arr_delay carrier\n#>      <int>          <int>     <dbl>    <int>          <int>     <dbl> <chr>  \n#> 1      517            515         2      830            819        11 UA     \n#> 2      533            529         4      850            830        20 UA     \n#> 3      542            540         2      923            850        33 AA     \n#> 4      544            545        -1     1004           1022       -18 B6     \n#> 5      554            600        -6      812            837       -25 DL     \n#> 6      554            558        -4      740            728        12 UA     \n#> # … with 336,770 more rows, and 9 more variables: flight <int>, tailnum <chr>,\n#> #   origin <chr>, dest <chr>, air_time <dbl>, distance <dbl>, hour <dbl>,\n#> #   minute <dbl>, time_hour <dttm>\nrename(flights, tail_num = tailnum)\n#> # A tibble: 336,776 x 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # … with 336,770 more rows, and 11 more variables: arr_delay <dbl>,\n#> #   carrier <chr>, flight <int>, tail_num <chr>, origin <chr>, dest <chr>,\n#> #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>\nselect(flights, time_hour, air_time, everything())\n#> # A tibble: 336,776 x 19\n#>   time_hour           air_time  year month   day dep_time sched_dep_time\n#>   <dttm>                 <dbl> <int> <int> <int>    <int>          <int>\n#> 1 2013-01-01 05:00:00      227  2013     1     1      517            515\n#> 2 2013-01-01 05:00:00      227  2013     1     1      533            529\n#> 3 2013-01-01 05:00:00      160  2013     1     1      542            540\n#> 4 2013-01-01 05:00:00      183  2013     1     1      544            545\n#> 5 2013-01-01 06:00:00      116  2013     1     1      554            600\n#> 6 2013-01-01 05:00:00      150  2013     1     1      554            558\n#> # … with 336,770 more rows, and 12 more variables: dep_delay <dbl>,\n#> #   arr_time <int>, sched_arr_time <int>, arr_delay <dbl>, carrier <chr>,\n#> #   flight <int>, tailnum <chr>, origin <chr>, dest <chr>, distance <dbl>,\n#> #   hour <dbl>, minute <dbl>"},{"path":"transform.html","id":"exercises-10","chapter":"5 Data transformation","heading":"5.4.1 Exercises","text":"Brainstorm many ways possible select dep_time, dep_delay,\narr_time, arr_delay flights.Brainstorm many ways possible select dep_time, dep_delay,\narr_time, arr_delay flights.happens include name variable multiple times \nselect() call?happens include name variable multiple times \nselect() call?any_of() function ? might helpful conjunction\nvector?\n\nvars <- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")any_of() function ? might helpful conjunction\nvector?result running following code surprise ? \nselect helpers deal case default? can change default?\n\nselect(flights, contains(\"TIME\"))result running following code surprise ? \nselect helpers deal case default? can change default?","code":"\nvars <- c(\"year\", \"month\", \"day\", \"dep_delay\", \"arr_delay\")\nselect(flights, contains(\"TIME\"))"},{"path":"transform.html","id":"add-new-variables-with-mutate","chapter":"5 Data transformation","heading":"5.5 Add new variables with mutate()","text":"Besides selecting sets existing columns, ’s often useful add new columns functions existing columns. ’s job mutate().mutate() always adds new columns end dataset ’ll start creating narrower dataset can see new variables. Remember ’re RStudio, easiest way see columns View().Note can refer columns ’ve just created:want keep new variables, use transmute():","code":"\nflights_sml <- select(flights, \n  year:day, \n  ends_with(\"delay\"), \n  distance, \n  air_time\n)\nmutate(flights_sml,\n  gain = dep_delay - arr_delay,\n  speed = distance / air_time * 60\n)\n#> # A tibble: 336,776 x 9\n#>    year month   day dep_delay arr_delay distance air_time  gain speed\n#>   <int> <int> <int>     <dbl>     <dbl>    <dbl>    <dbl> <dbl> <dbl>\n#> 1  2013     1     1         2        11     1400      227    -9  370.\n#> 2  2013     1     1         4        20     1416      227   -16  374.\n#> 3  2013     1     1         2        33     1089      160   -31  408.\n#> 4  2013     1     1        -1       -18     1576      183    17  517.\n#> 5  2013     1     1        -6       -25      762      116    19  394.\n#> 6  2013     1     1        -4        12      719      150   -16  288.\n#> # … with 336,770 more rows\nmutate(flights_sml,\n  gain = dep_delay - arr_delay,\n  hours = air_time / 60,\n  gain_per_hour = gain / hours\n)\n#> # A tibble: 336,776 x 10\n#>    year month   day dep_delay arr_delay distance air_time  gain hours\n#>   <int> <int> <int>     <dbl>     <dbl>    <dbl>    <dbl> <dbl> <dbl>\n#> 1  2013     1     1         2        11     1400      227    -9  3.78\n#> 2  2013     1     1         4        20     1416      227   -16  3.78\n#> 3  2013     1     1         2        33     1089      160   -31  2.67\n#> 4  2013     1     1        -1       -18     1576      183    17  3.05\n#> 5  2013     1     1        -6       -25      762      116    19  1.93\n#> 6  2013     1     1        -4        12      719      150   -16  2.5 \n#> # … with 336,770 more rows, and 1 more variable: gain_per_hour <dbl>\ntransmute(flights,\n  gain = dep_delay - arr_delay,\n  hours = air_time / 60,\n  gain_per_hour = gain / hours\n)\n#> # A tibble: 336,776 x 3\n#>    gain hours gain_per_hour\n#>   <dbl> <dbl>         <dbl>\n#> 1    -9  3.78         -2.38\n#> 2   -16  3.78         -4.23\n#> 3   -31  2.67        -11.6 \n#> 4    17  3.05          5.57\n#> 5    19  1.93          9.83\n#> 6   -16  2.5          -6.4 \n#> # … with 336,770 more rows"},{"path":"transform.html","id":"mutate-funs","chapter":"5 Data transformation","heading":"5.5.1 Useful creation functions","text":"many functions creating new variables can use mutate(). key property function must vectorised: must take vector values input, return vector number values output. ’s way list every possible function might use, ’s selection functions frequently useful:Arithmetic operators: +, -, *, /, ^. vectorised,\nusing called “recycling rules”. one parameter shorter \n, automatically extended length. \nuseful one arguments single number: air_time / 60,\nhours * 60 + minute, etc.\nArithmetic operators also useful conjunction aggregate\nfunctions ’ll learn later. example, x / sum(x) calculates\nproportion total, y - mean(y) computes difference \nmean.Arithmetic operators: +, -, *, /, ^. vectorised,\nusing called “recycling rules”. one parameter shorter \n, automatically extended length. \nuseful one arguments single number: air_time / 60,\nhours * 60 + minute, etc.Arithmetic operators also useful conjunction aggregate\nfunctions ’ll learn later. example, x / sum(x) calculates\nproportion total, y - mean(y) computes difference \nmean.Modular arithmetic: %/% (integer division) %% (remainder), \nx == y * (x %/% y) + (x %% y). Modular arithmetic handy tool \nallows break integers pieces. example, \nflights dataset, can compute hour minute dep_time :\n\ntransmute(flights,\n  dep_time,\n  hour = dep_time %/% 100,\n  minute = dep_time %% 100\n)\n#> # tibble: 336,776 x 3\n#>   dep_time  hour minute\n#>      <int> <dbl>  <dbl>\n#> 1      517     5     17\n#> 2      533     5     33\n#> 3      542     5     42\n#> 4      544     5     44\n#> 5      554     5     54\n#> 6      554     5     54\n#> # … 336,770 rowsModular arithmetic: %/% (integer division) %% (remainder), \nx == y * (x %/% y) + (x %% y). Modular arithmetic handy tool \nallows break integers pieces. example, \nflights dataset, can compute hour minute dep_time :Logs: log(), log2(), log10(). Logarithms incredibly useful\ntransformation dealing data ranges across multiple orders \nmagnitude. also convert multiplicative relationships additive, \nfeature ’ll come back modelling.\nelse equal, recommend using log2() ’s easy \ninterpret: difference 1 log scale corresponds doubling \noriginal scale difference -1 corresponds halving.Logs: log(), log2(), log10(). Logarithms incredibly useful\ntransformation dealing data ranges across multiple orders \nmagnitude. also convert multiplicative relationships additive, \nfeature ’ll come back modelling.else equal, recommend using log2() ’s easy \ninterpret: difference 1 log scale corresponds doubling \noriginal scale difference -1 corresponds halving.Offsets: lead() lag() allow refer leading lagging\nvalues. allows compute running differences (e.g. x - lag(x))\nfind values change (x != lag(x)). useful \nconjunction group_by(), ’ll learn shortly.\n\n(x <- 1:10)\n#>  [1]  1  2  3  4  5  6  7  8  9 10\nlag(x)\n#>  [1] NA  1  2  3  4  5  6  7  8  9\nlead(x)\n#>  [1]  2  3  4  5  6  7  8  9 10 NAOffsets: lead() lag() allow refer leading lagging\nvalues. allows compute running differences (e.g. x - lag(x))\nfind values change (x != lag(x)). useful \nconjunction group_by(), ’ll learn shortly.Cumulative rolling aggregates: R provides functions running sums,\nproducts, mins maxes: cumsum(), cumprod(), cummin(), cummax();\ndplyr provides cummean() cumulative means. need rolling\naggregates (.e. sum computed rolling window), try RcppRoll\npackage.\n\nx\n#>  [1]  1  2  3  4  5  6  7  8  9 10\ncumsum(x)\n#>  [1]  1  3  6 10 15 21 28 36 45 55\ncummean(x)\n#>  [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5Cumulative rolling aggregates: R provides functions running sums,\nproducts, mins maxes: cumsum(), cumprod(), cummin(), cummax();\ndplyr provides cummean() cumulative means. need rolling\naggregates (.e. sum computed rolling window), try RcppRoll\npackage.Logical comparisons, <, <=, >, >=, !=, ==, learned \nearlier. ’re complex sequence logical operations ’s\noften good idea store interim values new variables can\ncheck step working expected.Logical comparisons, <, <=, >, >=, !=, ==, learned \nearlier. ’re complex sequence logical operations ’s\noften good idea store interim values new variables can\ncheck step working expected.Ranking: number ranking functions, \nstart min_rank(). usual type ranking\n(e.g. 1st, 2nd, 2nd, 4th). default gives smallest values small\nranks; use desc(x) give largest values smallest ranks.\n\ny <- c(1, 2, 2, NA, 3, 4)\nmin_rank(y)\n#> [1]  1  2  2 NA  4  5\nmin_rank(desc(y))\n#> [1]  5  3  3 NA  2  1\nmin_rank() doesn’t need, look variants\nrow_number(), dense_rank(), percent_rank(), cume_dist(),\nntile(). See help pages details.\n\nrow_number(y)\n#> [1]  1  2  3 NA  4  5\ndense_rank(y)\n#> [1]  1  2  2 NA  3  4\npercent_rank(y)\n#> [1] 0.00 0.25 0.25   NA 0.75 1.00\ncume_dist(y)\n#> [1] 0.2 0.6 0.6  NA 0.8 1.0Ranking: number ranking functions, \nstart min_rank(). usual type ranking\n(e.g. 1st, 2nd, 2nd, 4th). default gives smallest values small\nranks; use desc(x) give largest values smallest ranks.min_rank() doesn’t need, look variants\nrow_number(), dense_rank(), percent_rank(), cume_dist(),\nntile(). See help pages details.","code":"\ntransmute(flights,\n  dep_time,\n  hour = dep_time %/% 100,\n  minute = dep_time %% 100\n)\n#> # A tibble: 336,776 x 3\n#>   dep_time  hour minute\n#>      <int> <dbl>  <dbl>\n#> 1      517     5     17\n#> 2      533     5     33\n#> 3      542     5     42\n#> 4      544     5     44\n#> 5      554     5     54\n#> 6      554     5     54\n#> # … with 336,770 more rows\n(x <- 1:10)\n#>  [1]  1  2  3  4  5  6  7  8  9 10\nlag(x)\n#>  [1] NA  1  2  3  4  5  6  7  8  9\nlead(x)\n#>  [1]  2  3  4  5  6  7  8  9 10 NA\nx\n#>  [1]  1  2  3  4  5  6  7  8  9 10\ncumsum(x)\n#>  [1]  1  3  6 10 15 21 28 36 45 55\ncummean(x)\n#>  [1] 1.0 1.5 2.0 2.5 3.0 3.5 4.0 4.5 5.0 5.5\ny <- c(1, 2, 2, NA, 3, 4)\nmin_rank(y)\n#> [1]  1  2  2 NA  4  5\nmin_rank(desc(y))\n#> [1]  5  3  3 NA  2  1\nrow_number(y)\n#> [1]  1  2  3 NA  4  5\ndense_rank(y)\n#> [1]  1  2  2 NA  3  4\npercent_rank(y)\n#> [1] 0.00 0.25 0.25   NA 0.75 1.00\ncume_dist(y)\n#> [1] 0.2 0.6 0.6  NA 0.8 1.0"},{"path":"transform.html","id":"exercises-11","chapter":"5 Data transformation","heading":"5.5.2 Exercises","text":"Currently dep_time sched_dep_time convenient look , \nhard compute ’re really continuous numbers.\nConvert convenient representation number minutes\nsince midnight.Currently dep_time sched_dep_time convenient look , \nhard compute ’re really continuous numbers.\nConvert convenient representation number minutes\nsince midnight.Compare air_time arr_time - dep_time. expect see?\nsee? need fix ?Compare air_time arr_time - dep_time. expect see?\nsee? need fix ?Compare dep_time, sched_dep_time, dep_delay. \nexpect three numbers related?Compare dep_time, sched_dep_time, dep_delay. \nexpect three numbers related?Find 10 delayed flights using ranking function. want\nhandle ties? Carefully read documentation min_rank().Find 10 delayed flights using ranking function. want\nhandle ties? Carefully read documentation min_rank().1:3 + 1:10 return? ?1:3 + 1:10 return? ?trigonometric functions R provide?trigonometric functions R provide?","code":""},{"path":"transform.html","id":"grouped-summaries-with-summarise","chapter":"5 Data transformation","heading":"5.6 Grouped summaries with summarise()","text":"last key verb summarise(). collapses data frame single row:(’ll come back na.rm = TRUE means shortly.)summarise() terribly useful unless pair group_by(). changes unit analysis complete dataset individual groups. , use dplyr verbs grouped data frame ’ll automatically applied “group”. example, applied exactly code data frame grouped date, get average delay per date:Together group_by() summarise() provide one tools ’ll use commonly working dplyr: grouped summaries. go , need introduce powerful new idea: pipe.","code":"\nsummarise(flights, delay = mean(dep_delay, na.rm = TRUE))\n#> # A tibble: 1 x 1\n#>   delay\n#>   <dbl>\n#> 1  12.6\nby_day <- group_by(flights, year, month, day)\nsummarise(by_day, delay = mean(dep_delay, na.rm = TRUE))\n#> `summarise()` has grouped output by 'year', 'month'. You can override using the `.groups` argument.\n#> # A tibble: 365 x 4\n#> # Groups:   year, month [12]\n#>    year month   day delay\n#>   <int> <int> <int> <dbl>\n#> 1  2013     1     1 11.5 \n#> 2  2013     1     2 13.9 \n#> 3  2013     1     3 11.0 \n#> 4  2013     1     4  8.95\n#> 5  2013     1     5  5.73\n#> 6  2013     1     6  7.15\n#> # … with 359 more rows"},{"path":"transform.html","id":"combining-multiple-operations-with-the-pipe","chapter":"5 Data transformation","heading":"5.6.1 Combining multiple operations with the pipe","text":"Imagine want explore relationship distance average delay location. Using know dplyr, might write code like :three steps prepare data:Group flights destination.Group flights destination.Summarise compute distance, average delay, number flights.Summarise compute distance, average delay, number flights.Filter remove noisy points Honolulu airport, almost\ntwice far away next closest airport.Filter remove noisy points Honolulu airport, almost\ntwice far away next closest airport.code little frustrating write give intermediate data frame name, even though don’t care . Naming things hard, slows analysis.’s another way tackle problem pipe, %>%:focuses transformations, ’s transformed, makes code easier read. can read series imperative statements: group, summarise, filter. suggested reading, good way pronounce %>% reading code “”.Behind scenes, x %>% f(y) turns f(x, y), x %>% f(y) %>% g(z) turns g(f(x, y), z) . can use pipe rewrite multiple operations way can read left--right, top--bottom. ’ll use piping frequently now considerably improves readability code, ’ll come back detail pipes.Working pipe one key criteria belonging tidyverse. exception ggplot2: written pipe discovered. Unfortunately, next iteration ggplot2, ggvis, use pipe, isn’t quite ready prime time yet.","code":"\nby_dest <- group_by(flights, dest)\ndelay <- summarise(by_dest,\n  count = n(),\n  dist = mean(distance, na.rm = TRUE),\n  delay = mean(arr_delay, na.rm = TRUE)\n)\ndelay <- filter(delay, count > 20, dest != \"HNL\")\n\n# It looks like delays increase with distance up to ~750 miles \n# and then decrease. Maybe as flights get longer there's more \n# ability to make up delays in the air?\nggplot(data = delay, mapping = aes(x = dist, y = delay)) +\n  geom_point(aes(size = count), alpha = 1/3) +\n  geom_smooth(se = FALSE)\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'\ndelays <- flights %>% \n  group_by(dest) %>% \n  summarise(\n    count = n(),\n    dist = mean(distance, na.rm = TRUE),\n    delay = mean(arr_delay, na.rm = TRUE)\n  ) %>% \n  filter(count > 20, dest != \"HNL\")"},{"path":"transform.html","id":"missing-values-1","chapter":"5 Data transformation","heading":"5.6.2 Missing values","text":"may wondered na.rm argument used . happens don’t set ?get lot missing values! ’s aggregation functions obey usual rule missing values: ’s missing value input, output missing value. Fortunately, aggregation functions na.rm argument removes missing values prior computation:case, missing values represent cancelled flights, also tackle problem first removing cancelled flights. ’ll save dataset can reuse next examples.","code":"\nflights %>% \n  group_by(year, month, day) %>% \n  summarise(mean = mean(dep_delay))\n#> `summarise()` has grouped output by 'year', 'month'. You can override using the `.groups` argument.\n#> # A tibble: 365 x 4\n#> # Groups:   year, month [12]\n#>    year month   day  mean\n#>   <int> <int> <int> <dbl>\n#> 1  2013     1     1    NA\n#> 2  2013     1     2    NA\n#> 3  2013     1     3    NA\n#> 4  2013     1     4    NA\n#> 5  2013     1     5    NA\n#> 6  2013     1     6    NA\n#> # … with 359 more rows\nflights %>% \n  group_by(year, month, day) %>% \n  summarise(mean = mean(dep_delay, na.rm = TRUE))\n#> `summarise()` has grouped output by 'year', 'month'. You can override using the `.groups` argument.\n#> # A tibble: 365 x 4\n#> # Groups:   year, month [12]\n#>    year month   day  mean\n#>   <int> <int> <int> <dbl>\n#> 1  2013     1     1 11.5 \n#> 2  2013     1     2 13.9 \n#> 3  2013     1     3 11.0 \n#> 4  2013     1     4  8.95\n#> 5  2013     1     5  5.73\n#> 6  2013     1     6  7.15\n#> # … with 359 more rows\nnot_cancelled <- flights %>% \n  filter(!is.na(dep_delay), !is.na(arr_delay))\n\nnot_cancelled %>% \n  group_by(year, month, day) %>% \n  summarise(mean = mean(dep_delay))\n#> `summarise()` has grouped output by 'year', 'month'. You can override using the `.groups` argument.\n#> # A tibble: 365 x 4\n#> # Groups:   year, month [12]\n#>    year month   day  mean\n#>   <int> <int> <int> <dbl>\n#> 1  2013     1     1 11.4 \n#> 2  2013     1     2 13.7 \n#> 3  2013     1     3 10.9 \n#> 4  2013     1     4  8.97\n#> 5  2013     1     5  5.73\n#> 6  2013     1     6  7.15\n#> # … with 359 more rows"},{"path":"transform.html","id":"counts","chapter":"5 Data transformation","heading":"5.6.3 Counts","text":"Whenever aggregation, ’s always good idea include either count (n()), count non-missing values (sum(!.na(x))). way can check ’re drawing conclusions based small amounts data. example, let’s look planes (identified tail number) highest average delays:Wow, planes average delay 5 hours (300 minutes)!story actually little nuanced. can get insight draw scatterplot number flights vs. average delay:surprisingly, much greater variation average delay flights. shape plot characteristic: whenever plot mean (summary) vs. group size, ’ll see variation decreases sample size increases.looking sort plot, ’s often useful filter groups smallest numbers observations, can see pattern less extreme variation smallest groups. following code , well showing handy pattern integrating ggplot2 dplyr flows. ’s bit painful switch %>% +, get hang , ’s quite convenient.RStudio tip: useful keyboard shortcut Cmd/Ctrl + Shift + P. resends previously sent chunk editor console. convenient ’re (e.g.) exploring value n example . send whole block Cmd/Ctrl + Enter, modify value n press Cmd/Ctrl + Shift + P resend complete block.’s another common variation type pattern. Let’s look average performance batters baseball related number times ’re bat. use data Lahman package compute batting average (number hits / number attempts) every major league baseball player.plot skill batter (measured batting average, ba) number opportunities hit ball (measured bat, ab), see two patterns:, variation aggregate decreases get \ndata points., variation aggregate decreases get \ndata points.’s positive correlation skill (ba) opportunities \nhit ball (ab). teams control gets play,\nobviously ’ll pick best players.’s positive correlation skill (ba) opportunities \nhit ball (ab). teams control gets play,\nobviously ’ll pick best players.also important implications ranking. naively sort desc(ba), people best batting averages clearly lucky, skilled:can find good explanation problem http://varianceexplained.org/r/empirical_bayes_baseball/ http://www.evanmiller.org/---sort--average-rating.html.","code":"\ndelays <- not_cancelled %>% \n  group_by(tailnum) %>% \n  summarise(\n    delay = mean(arr_delay)\n  )\n\nggplot(data = delays, mapping = aes(x = delay)) + \n  geom_freqpoly(binwidth = 10)\ndelays <- not_cancelled %>% \n  group_by(tailnum) %>% \n  summarise(\n    delay = mean(arr_delay),\n    n = n()\n  )\n\nggplot(data = delays, mapping = aes(x = n, y = delay)) + \n  geom_point(alpha = 1/10)\ndelays %>% \n  filter(n > 25) %>% \n  ggplot(mapping = aes(x = n, y = delay)) + \n    geom_point(alpha = 1/10)\n# Convert to a tibble so it prints nicely\nbatting <- as_tibble(Lahman::Batting)\n\nbatters <- batting %>% \n  group_by(playerID) %>% \n  summarise(\n    ba = sum(H, na.rm = TRUE) / sum(AB, na.rm = TRUE),\n    ab = sum(AB, na.rm = TRUE)\n  )\n\nbatters %>% \n  filter(ab > 100) %>% \n  ggplot(mapping = aes(x = ab, y = ba)) +\n    geom_point() + \n    geom_smooth(se = FALSE)\n#> `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\nbatters %>% \n  arrange(desc(ba))\n#> # A tibble: 19,689 x 3\n#>   playerID     ba    ab\n#>   <chr>     <dbl> <int>\n#> 1 abramge01     1     1\n#> 2 alanirj01     1     1\n#> 3 alberan01     1     1\n#> 4 banisje01     1     1\n#> 5 bartocl01     1     1\n#> 6 bassdo01      1     1\n#> # … with 19,683 more rows"},{"path":"transform.html","id":"summarise-funs","chapter":"5 Data transformation","heading":"5.6.4 Useful summary functions","text":"Just using means, counts, sum can get long way, R provides many useful summary functions:Measures location: ’ve used mean(x), median(x) also\nuseful. mean sum divided length; median value\n50% x , 50% .\n’s sometimes useful combine aggregation logical subsetting.\nhaven’t talked sort subsetting yet, ’ll learn \nsubsetting.\n\nnot_cancelled %>% \n  group_by(year, month, day) %>% \n  summarise(\n    avg_delay1 = mean(arr_delay),\n    avg_delay2 = mean(arr_delay[arr_delay > 0]) # average positive delay\n  )\n#> `summarise()` grouped output 'year', 'month'. can override using `.groups` argument.\n#> # tibble: 365 x 5\n#> # Groups:   year, month [12]\n#>    year month   day avg_delay1 avg_delay2\n#>   <int> <int> <int>      <dbl>      <dbl>\n#> 1  2013     1     1      12.7        32.5\n#> 2  2013     1     2      12.7        32.0\n#> 3  2013     1     3       5.73       27.7\n#> 4  2013     1     4      -1.93       28.3\n#> 5  2013     1     5      -1.53       22.6\n#> 6  2013     1     6       4.24       24.4\n#> # … 359 rowsMeasures location: ’ve used mean(x), median(x) also\nuseful. mean sum divided length; median value\n50% x , 50% .’s sometimes useful combine aggregation logical subsetting.\nhaven’t talked sort subsetting yet, ’ll learn \nsubsetting.Measures spread: sd(x), IQR(x), mad(x). root mean squared deviation,\nstandard deviation sd(x), standard measure spread.\ninterquartile range IQR(x) median absolute deviation mad(x)\nrobust equivalents may useful outliers.\n\n# distance destinations variable others?\nnot_cancelled %>% \n  group_by(dest) %>% \n  summarise(distance_sd = sd(distance)) %>% \n  arrange(desc(distance_sd))\n#> # tibble: 104 x 2\n#>   dest  distance_sd\n#>   <chr>       <dbl>\n#> 1 EGE         10.5 \n#> 2 SAN         10.4 \n#> 3 SFO         10.2 \n#> 4 HNL         10.0 \n#> 5 SEA          9.98\n#> 6 LAS          9.91\n#> # … 98 rowsMeasures spread: sd(x), IQR(x), mad(x). root mean squared deviation,\nstandard deviation sd(x), standard measure spread.\ninterquartile range IQR(x) median absolute deviation mad(x)\nrobust equivalents may useful outliers.Measures rank: min(x), quantile(x, 0.25), max(x). Quantiles\ngeneralisation median. example, quantile(x, 0.25)\nfind value x greater 25% values,\nless remaining 75%.\n\n# first last flights leave day?\nnot_cancelled %>% \n  group_by(year, month, day) %>% \n  summarise(\n    first = min(dep_time),\n    last = max(dep_time)\n  )\n#> `summarise()` grouped output 'year', 'month'. can override using `.groups` argument.\n#> # tibble: 365 x 5\n#> # Groups:   year, month [12]\n#>    year month   day first  last\n#>   <int> <int> <int> <int> <int>\n#> 1  2013     1     1   517  2356\n#> 2  2013     1     2    42  2354\n#> 3  2013     1     3    32  2349\n#> 4  2013     1     4    25  2358\n#> 5  2013     1     5    14  2357\n#> 6  2013     1     6    16  2355\n#> # … 359 rowsMeasures rank: min(x), quantile(x, 0.25), max(x). Quantiles\ngeneralisation median. example, quantile(x, 0.25)\nfind value x greater 25% values,\nless remaining 75%.Measures position: first(x), nth(x, 2), last(x). work\nsimilarly x[1], x[2], x[length(x)] let set default\nvalue position exist (.e. ’re trying get 3rd\nelement group two elements). example, can\nfind first last departure day:\n\nnot_cancelled %>% \n  group_by(year, month, day) %>% \n  summarise(\n    first_dep = first(dep_time), \n    last_dep = last(dep_time)\n  )\n#> `summarise()` grouped output 'year', 'month'. can override using `.groups` argument.\n#> # tibble: 365 x 5\n#> # Groups:   year, month [12]\n#>    year month   day first_dep last_dep\n#>   <int> <int> <int>     <int>    <int>\n#> 1  2013     1     1       517     2356\n#> 2  2013     1     2        42     2354\n#> 3  2013     1     3        32     2349\n#> 4  2013     1     4        25     2358\n#> 5  2013     1     5        14     2357\n#> 6  2013     1     6        16     2355\n#> # … 359 rows\nfunctions complementary filtering ranks. Filtering gives\nvariables, observation separate row:\n\nnot_cancelled %>% \n  group_by(year, month, day) %>% \n  mutate(r = min_rank(desc(dep_time))) %>% \n  filter(r %% range(r))\n#> # tibble: 770 x 20\n#> # Groups:   year, month, day [365]\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1     2356           2359        -3      425            437\n#> 3  2013     1     2       42           2359        43      518            442\n#> 4  2013     1     2     2354           2359        -5      413            437\n#> 5  2013     1     3       32           2359        33      504            442\n#> 6  2013     1     3     2349           2359       -10      434            445\n#> # … 764 rows, 12 variables: arr_delay <dbl>, carrier <chr>,\n#> #   flight <int>, tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,\n#> #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>, r <int>Measures position: first(x), nth(x, 2), last(x). work\nsimilarly x[1], x[2], x[length(x)] let set default\nvalue position exist (.e. ’re trying get 3rd\nelement group two elements). example, can\nfind first last departure day:functions complementary filtering ranks. Filtering gives\nvariables, observation separate row:Counts: ’ve seen n(), takes arguments, returns \nsize current group. count number non-missing values, use\nsum(!.na(x)). count number distinct (unique) values, use\nn_distinct(x).\n\n# destinations carriers?\nnot_cancelled %>% \n  group_by(dest) %>% \n  summarise(carriers = n_distinct(carrier)) %>% \n  arrange(desc(carriers))\n#> # tibble: 104 x 2\n#>   dest  carriers\n#>   <chr>    <int>\n#> 1 ATL          7\n#> 2 BOS          7\n#> 3 CLT          7\n#> 4 ORD          7\n#> 5 TPA          7\n#> 6 AUS          6\n#> # … 98 rows\nCounts useful dplyr provides simple helper want \ncount:\n\nnot_cancelled %>% \n  count(dest)\n#> # tibble: 104 x 2\n#>   dest      n\n#> * <chr> <int>\n#> 1 ABQ     254\n#> 2 ACK     264\n#> 3 ALB     418\n#> 4 ANC       8\n#> 5 ATL   16837\n#> 6 AUS    2411\n#> # … 98 rows\ncan optionally provide weight variable. example, use\n“count” (sum) total number miles plane flew:\n\nnot_cancelled %>% \n  count(tailnum, wt = distance)\n#> # tibble: 4,037 x 2\n#>   tailnum      n\n#> * <chr>    <dbl>\n#> 1 D942DN    3418\n#> 2 N0EGMQ  239143\n#> 3 N10156  109664\n#> 4 N102UW   25722\n#> 5 N103US   24619\n#> 6 N104UW   24616\n#> # … 4,031 rowsCounts: ’ve seen n(), takes arguments, returns \nsize current group. count number non-missing values, use\nsum(!.na(x)). count number distinct (unique) values, use\nn_distinct(x).Counts useful dplyr provides simple helper want \ncount:can optionally provide weight variable. example, use\n“count” (sum) total number miles plane flew:Counts proportions logical values: sum(x > 10), mean(y == 0).\nused numeric functions, TRUE converted 1 FALSE 0.\nmakes sum() mean() useful: sum(x) gives number \nTRUEs x, mean(x) gives proportion.\n\n# many flights left 5am? (usually indicate delayed\n# flights previous day)\nnot_cancelled %>% \n  group_by(year, month, day) %>% \n  summarise(n_early = sum(dep_time < 500))\n#> `summarise()` grouped output 'year', 'month'. can override using `.groups` argument.\n#> # tibble: 365 x 4\n#> # Groups:   year, month [12]\n#>    year month   day n_early\n#>   <int> <int> <int>   <int>\n#> 1  2013     1     1       0\n#> 2  2013     1     2       3\n#> 3  2013     1     3       4\n#> 4  2013     1     4       3\n#> 5  2013     1     5       3\n#> 6  2013     1     6       2\n#> # … 359 rows\n\n# proportion flights delayed hour?\nnot_cancelled %>% \n  group_by(year, month, day) %>% \n  summarise(hour_prop = mean(arr_delay > 60))\n#> `summarise()` grouped output 'year', 'month'. can override using `.groups` argument.\n#> # tibble: 365 x 4\n#> # Groups:   year, month [12]\n#>    year month   day hour_prop\n#>   <int> <int> <int>     <dbl>\n#> 1  2013     1     1    0.0722\n#> 2  2013     1     2    0.0851\n#> 3  2013     1     3    0.0567\n#> 4  2013     1     4    0.0396\n#> 5  2013     1     5    0.0349\n#> 6  2013     1     6    0.0470\n#> # … 359 rowsCounts proportions logical values: sum(x > 10), mean(y == 0).\nused numeric functions, TRUE converted 1 FALSE 0.\nmakes sum() mean() useful: sum(x) gives number \nTRUEs x, mean(x) gives proportion.","code":"\nnot_cancelled %>% \n  group_by(year, month, day) %>% \n  summarise(\n    avg_delay1 = mean(arr_delay),\n    avg_delay2 = mean(arr_delay[arr_delay > 0]) # the average positive delay\n  )\n#> `summarise()` has grouped output by 'year', 'month'. You can override using the `.groups` argument.\n#> # A tibble: 365 x 5\n#> # Groups:   year, month [12]\n#>    year month   day avg_delay1 avg_delay2\n#>   <int> <int> <int>      <dbl>      <dbl>\n#> 1  2013     1     1      12.7        32.5\n#> 2  2013     1     2      12.7        32.0\n#> 3  2013     1     3       5.73       27.7\n#> 4  2013     1     4      -1.93       28.3\n#> 5  2013     1     5      -1.53       22.6\n#> 6  2013     1     6       4.24       24.4\n#> # … with 359 more rows\n# Why is distance to some destinations more variable than to others?\nnot_cancelled %>% \n  group_by(dest) %>% \n  summarise(distance_sd = sd(distance)) %>% \n  arrange(desc(distance_sd))\n#> # A tibble: 104 x 2\n#>   dest  distance_sd\n#>   <chr>       <dbl>\n#> 1 EGE         10.5 \n#> 2 SAN         10.4 \n#> 3 SFO         10.2 \n#> 4 HNL         10.0 \n#> 5 SEA          9.98\n#> 6 LAS          9.91\n#> # … with 98 more rows\n# When do the first and last flights leave each day?\nnot_cancelled %>% \n  group_by(year, month, day) %>% \n  summarise(\n    first = min(dep_time),\n    last = max(dep_time)\n  )\n#> `summarise()` has grouped output by 'year', 'month'. You can override using the `.groups` argument.\n#> # A tibble: 365 x 5\n#> # Groups:   year, month [12]\n#>    year month   day first  last\n#>   <int> <int> <int> <int> <int>\n#> 1  2013     1     1   517  2356\n#> 2  2013     1     2    42  2354\n#> 3  2013     1     3    32  2349\n#> 4  2013     1     4    25  2358\n#> 5  2013     1     5    14  2357\n#> 6  2013     1     6    16  2355\n#> # … with 359 more rows\nnot_cancelled %>% \n  group_by(year, month, day) %>% \n  summarise(\n    first_dep = first(dep_time), \n    last_dep = last(dep_time)\n  )\n#> `summarise()` has grouped output by 'year', 'month'. You can override using the `.groups` argument.\n#> # A tibble: 365 x 5\n#> # Groups:   year, month [12]\n#>    year month   day first_dep last_dep\n#>   <int> <int> <int>     <int>    <int>\n#> 1  2013     1     1       517     2356\n#> 2  2013     1     2        42     2354\n#> 3  2013     1     3        32     2349\n#> 4  2013     1     4        25     2358\n#> 5  2013     1     5        14     2357\n#> 6  2013     1     6        16     2355\n#> # … with 359 more rows\nnot_cancelled %>% \n  group_by(year, month, day) %>% \n  mutate(r = min_rank(desc(dep_time))) %>% \n  filter(r %in% range(r))\n#> # A tibble: 770 x 20\n#> # Groups:   year, month, day [365]\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1     2356           2359        -3      425            437\n#> 3  2013     1     2       42           2359        43      518            442\n#> 4  2013     1     2     2354           2359        -5      413            437\n#> 5  2013     1     3       32           2359        33      504            442\n#> 6  2013     1     3     2349           2359       -10      434            445\n#> # … with 764 more rows, and 12 more variables: arr_delay <dbl>, carrier <chr>,\n#> #   flight <int>, tailnum <chr>, origin <chr>, dest <chr>, air_time <dbl>,\n#> #   distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>, r <int>\n# Which destinations have the most carriers?\nnot_cancelled %>% \n  group_by(dest) %>% \n  summarise(carriers = n_distinct(carrier)) %>% \n  arrange(desc(carriers))\n#> # A tibble: 104 x 2\n#>   dest  carriers\n#>   <chr>    <int>\n#> 1 ATL          7\n#> 2 BOS          7\n#> 3 CLT          7\n#> 4 ORD          7\n#> 5 TPA          7\n#> 6 AUS          6\n#> # … with 98 more rows\nnot_cancelled %>% \n  count(dest)\n#> # A tibble: 104 x 2\n#>   dest      n\n#> * <chr> <int>\n#> 1 ABQ     254\n#> 2 ACK     264\n#> 3 ALB     418\n#> 4 ANC       8\n#> 5 ATL   16837\n#> 6 AUS    2411\n#> # … with 98 more rows\nnot_cancelled %>% \n  count(tailnum, wt = distance)\n#> # A tibble: 4,037 x 2\n#>   tailnum      n\n#> * <chr>    <dbl>\n#> 1 D942DN    3418\n#> 2 N0EGMQ  239143\n#> 3 N10156  109664\n#> 4 N102UW   25722\n#> 5 N103US   24619\n#> 6 N104UW   24616\n#> # … with 4,031 more rows\n# How many flights left before 5am? (these usually indicate delayed\n# flights from the previous day)\nnot_cancelled %>% \n  group_by(year, month, day) %>% \n  summarise(n_early = sum(dep_time < 500))\n#> `summarise()` has grouped output by 'year', 'month'. You can override using the `.groups` argument.\n#> # A tibble: 365 x 4\n#> # Groups:   year, month [12]\n#>    year month   day n_early\n#>   <int> <int> <int>   <int>\n#> 1  2013     1     1       0\n#> 2  2013     1     2       3\n#> 3  2013     1     3       4\n#> 4  2013     1     4       3\n#> 5  2013     1     5       3\n#> 6  2013     1     6       2\n#> # … with 359 more rows\n\n# What proportion of flights are delayed by more than an hour?\nnot_cancelled %>% \n  group_by(year, month, day) %>% \n  summarise(hour_prop = mean(arr_delay > 60))\n#> `summarise()` has grouped output by 'year', 'month'. You can override using the `.groups` argument.\n#> # A tibble: 365 x 4\n#> # Groups:   year, month [12]\n#>    year month   day hour_prop\n#>   <int> <int> <int>     <dbl>\n#> 1  2013     1     1    0.0722\n#> 2  2013     1     2    0.0851\n#> 3  2013     1     3    0.0567\n#> 4  2013     1     4    0.0396\n#> 5  2013     1     5    0.0349\n#> 6  2013     1     6    0.0470\n#> # … with 359 more rows"},{"path":"transform.html","id":"grouping-by-multiple-variables","chapter":"5 Data transformation","heading":"5.6.5 Grouping by multiple variables","text":"group multiple variables, summary peels one level grouping. makes easy progressively roll dataset:careful progressively rolling summaries: ’s OK sums counts, need think weighting means variances, ’s possible exactly rank-based statistics like median. words, sum groupwise sums overall sum, median groupwise medians overall median.","code":"\ndaily <- group_by(flights, year, month, day)\n(per_day   <- summarise(daily, flights = n()))\n#> `summarise()` has grouped output by 'year', 'month'. You can override using the `.groups` argument.\n#> # A tibble: 365 x 4\n#> # Groups:   year, month [12]\n#>    year month   day flights\n#>   <int> <int> <int>   <int>\n#> 1  2013     1     1     842\n#> 2  2013     1     2     943\n#> 3  2013     1     3     914\n#> 4  2013     1     4     915\n#> 5  2013     1     5     720\n#> 6  2013     1     6     832\n#> # … with 359 more rows\n(per_month <- summarise(per_day, flights = sum(flights)))\n#> `summarise()` has grouped output by 'year'. You can override using the `.groups` argument.\n#> # A tibble: 12 x 3\n#> # Groups:   year [1]\n#>    year month flights\n#>   <int> <int>   <int>\n#> 1  2013     1   27004\n#> 2  2013     2   24951\n#> 3  2013     3   28834\n#> 4  2013     4   28330\n#> 5  2013     5   28796\n#> 6  2013     6   28243\n#> # … with 6 more rows\n(per_year  <- summarise(per_month, flights = sum(flights)))\n#> # A tibble: 1 x 2\n#>    year flights\n#> * <int>   <int>\n#> 1  2013  336776"},{"path":"transform.html","id":"ungrouping","chapter":"5 Data transformation","heading":"5.6.6 Ungrouping","text":"need remove grouping, return operations ungrouped data, use ungroup().","code":"\ndaily %>% \n  ungroup() %>%             # no longer grouped by date\n  summarise(flights = n())  # all flights\n#> # A tibble: 1 x 1\n#>   flights\n#>     <int>\n#> 1  336776"},{"path":"transform.html","id":"exercises-12","chapter":"5 Data transformation","heading":"5.6.7 Exercises","text":"Brainstorm least 5 different ways assess typical delay\ncharacteristics group flights. Consider following scenarios:\nflight 15 minutes early 50% time, 15 minutes late 50% \ntime.\nflight always 10 minutes late.\nflight 30 minutes early 50% time, 30 minutes late 50% \ntime.\n99% time flight time. 1% time ’s 2 hours late.\nimportant: arrival delay departure delay?Brainstorm least 5 different ways assess typical delay\ncharacteristics group flights. Consider following scenarios:flight 15 minutes early 50% time, 15 minutes late 50% \ntime.flight 15 minutes early 50% time, 15 minutes late 50% \ntime.flight always 10 minutes late.flight always 10 minutes late.flight 30 minutes early 50% time, 30 minutes late 50% \ntime.flight 30 minutes early 50% time, 30 minutes late 50% \ntime.99% time flight time. 1% time ’s 2 hours late.99% time flight time. 1% time ’s 2 hours late.important: arrival delay departure delay?Come another approach give output \nnot_cancelled %>% count(dest) \nnot_cancelled %>% count(tailnum, wt = distance) (without using\ncount()).Come another approach give output \nnot_cancelled %>% count(dest) \nnot_cancelled %>% count(tailnum, wt = distance) (without using\ncount()).definition cancelled flights (.na(dep_delay) | .na(arr_delay)\n) slightly suboptimal. ? important column?definition cancelled flights (.na(dep_delay) | .na(arr_delay)\n) slightly suboptimal. ? important column?Look number cancelled flights per day. pattern?\nproportion cancelled flights related average delay?Look number cancelled flights per day. pattern?\nproportion cancelled flights related average delay?carrier worst delays? Challenge: can disentangle \neffects bad airports vs. bad carriers? /? (Hint: think \nflights %>% group_by(carrier, dest) %>% summarise(n()))carrier worst delays? Challenge: can disentangle \neffects bad airports vs. bad carriers? /? (Hint: think \nflights %>% group_by(carrier, dest) %>% summarise(n()))sort argument count() . might use ?sort argument count() . might use ?","code":""},{"path":"transform.html","id":"grouped-mutates-and-filters","chapter":"5 Data transformation","heading":"5.7 Grouped mutates (and filters)","text":"Grouping useful conjunction summarise(), can also convenient operations mutate() filter():Find worst members group:\n\nflights_sml %>% \n  group_by(year, month, day) %>%\n  filter(rank(desc(arr_delay)) < 10)\n#> # tibble: 3,306 x 7\n#> # Groups:   year, month, day [365]\n#>    year month   day dep_delay arr_delay distance air_time\n#>   <int> <int> <int>     <dbl>     <dbl>    <dbl>    <dbl>\n#> 1  2013     1     1       853       851      184       41\n#> 2  2013     1     1       290       338     1134      213\n#> 3  2013     1     1       260       263      266       46\n#> 4  2013     1     1       157       174      213       60\n#> 5  2013     1     1       216       222      708      121\n#> 6  2013     1     1       255       250      589      115\n#> # … 3,300 rowsFind worst members group:Find groups bigger threshold:\n\npopular_dests <- flights %>% \n  group_by(dest) %>% \n  filter(n() > 365)\npopular_dests\n#> # tibble: 332,577 x 19\n#> # Groups:   dest [77]\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # … 332,571 rows, 11 variables: arr_delay <dbl>,\n#> #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#> #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>Find groups bigger threshold:Standardise compute per group metrics:\n\npopular_dests %>% \n  filter(arr_delay > 0) %>% \n  mutate(prop_delay = arr_delay / sum(arr_delay)) %>% \n  select(year:day, dest, arr_delay, prop_delay)\n#> # tibble: 131,106 x 6\n#> # Groups:   dest [77]\n#>    year month   day dest  arr_delay prop_delay\n#>   <int> <int> <int> <chr>     <dbl>      <dbl>\n#> 1  2013     1     1 IAH          11  0.000111 \n#> 2  2013     1     1 IAH          20  0.000201 \n#> 3  2013     1     1 MIA          33  0.000235 \n#> 4  2013     1     1 ORD          12  0.0000424\n#> 5  2013     1     1 FLL          19  0.0000938\n#> 6  2013     1     1 ORD           8  0.0000283\n#> # … 131,100 rowsStandardise compute per group metrics:grouped filter grouped mutate followed ungrouped filter. generally avoid except quick dirty manipulations: otherwise ’s hard check ’ve done manipulation correctly.Functions work naturally grouped mutates filters known window functions (vs. summary functions used summaries). can learn useful window functions corresponding vignette: vignette(\"window-functions\").","code":"\nflights_sml %>% \n  group_by(year, month, day) %>%\n  filter(rank(desc(arr_delay)) < 10)\n#> # A tibble: 3,306 x 7\n#> # Groups:   year, month, day [365]\n#>    year month   day dep_delay arr_delay distance air_time\n#>   <int> <int> <int>     <dbl>     <dbl>    <dbl>    <dbl>\n#> 1  2013     1     1       853       851      184       41\n#> 2  2013     1     1       290       338     1134      213\n#> 3  2013     1     1       260       263      266       46\n#> 4  2013     1     1       157       174      213       60\n#> 5  2013     1     1       216       222      708      121\n#> 6  2013     1     1       255       250      589      115\n#> # … with 3,300 more rows\npopular_dests <- flights %>% \n  group_by(dest) %>% \n  filter(n() > 365)\npopular_dests\n#> # A tibble: 332,577 x 19\n#> # Groups:   dest [77]\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      517            515         2      830            819\n#> 2  2013     1     1      533            529         4      850            830\n#> 3  2013     1     1      542            540         2      923            850\n#> 4  2013     1     1      544            545        -1     1004           1022\n#> 5  2013     1     1      554            600        -6      812            837\n#> 6  2013     1     1      554            558        -4      740            728\n#> # … with 332,571 more rows, and 11 more variables: arr_delay <dbl>,\n#> #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#> #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>\npopular_dests %>% \n  filter(arr_delay > 0) %>% \n  mutate(prop_delay = arr_delay / sum(arr_delay)) %>% \n  select(year:day, dest, arr_delay, prop_delay)\n#> # A tibble: 131,106 x 6\n#> # Groups:   dest [77]\n#>    year month   day dest  arr_delay prop_delay\n#>   <int> <int> <int> <chr>     <dbl>      <dbl>\n#> 1  2013     1     1 IAH          11  0.000111 \n#> 2  2013     1     1 IAH          20  0.000201 \n#> 3  2013     1     1 MIA          33  0.000235 \n#> 4  2013     1     1 ORD          12  0.0000424\n#> 5  2013     1     1 FLL          19  0.0000938\n#> 6  2013     1     1 ORD           8  0.0000283\n#> # … with 131,100 more rows"},{"path":"transform.html","id":"exercises-13","chapter":"5 Data transformation","heading":"5.7.1 Exercises","text":"Refer back lists useful mutate filtering functions.\nDescribe operation changes combine grouping.Refer back lists useful mutate filtering functions.\nDescribe operation changes combine grouping.plane (tailnum) worst -time record?plane (tailnum) worst -time record?time day fly want avoid delays much\npossible?time day fly want avoid delays much\npossible?destination, compute total minutes delay. \nflight, compute proportion total delay destination.destination, compute total minutes delay. \nflight, compute proportion total delay destination.Delays typically temporally correlated: even problem \ncaused initial delay resolved, later flights delayed\nallow earlier flights leave. Using lag(), explore delay\nflight related delay immediately preceding flight.Delays typically temporally correlated: even problem \ncaused initial delay resolved, later flights delayed\nallow earlier flights leave. Using lag(), explore delay\nflight related delay immediately preceding flight.Look destination. Can find flights suspiciously\nfast? (.e. flights represent potential data entry error). Compute\nair time flight relative shortest flight destination.\nflights delayed air?Look destination. Can find flights suspiciously\nfast? (.e. flights represent potential data entry error). Compute\nair time flight relative shortest flight destination.\nflights delayed air?Find destinations flown least two carriers. Use \ninformation rank carriers.Find destinations flown least two carriers. Use \ninformation rank carriers.plane, count number flights first delay\ngreater 1 hour.plane, count number flights first delay\ngreater 1 hour.","code":""},{"path":"workflow-scripts.html","id":"workflow-scripts","chapter":"6 Workflow: scripts","heading":"6 Workflow: scripts","text":"far ’ve using console run code. ’s great place start, ’ll find gets cramped pretty quickly create complex ggplot2 graphics dplyr pipes. give room work, ’s great idea use script editor. Open either clicking File menu, selecting New File, R script, using keyboard shortcut Cmd/Ctrl + Shift + N. Now ’ll see four panes:script editor great place put code care . Keep experimenting console, written code works want, put script editor. RStudio automatically save contents editor quit RStudio, automatically load re-open. Nevertheless, ’s good idea save scripts regularly back .","code":""},{"path":"workflow-scripts.html","id":"running-code","chapter":"6 Workflow: scripts","heading":"6.1 Running code","text":"script editor also great place build complex ggplot2 plots long sequences dplyr manipulations. key using script editor effectively memorise one important keyboard shortcuts: Cmd/Ctrl + Enter. executes current R expression console. example, take code . cursor █, pressing Cmd/Ctrl + Enter run complete command generates not_cancelled. also move cursor next statement (beginning not_cancelled %>%). makes easy run complete script repeatedly pressing Cmd/Ctrl + Enter.Instead running expression--expression, can also execute complete script one step: Cmd/Ctrl + Shift + S. regularly great way check ’ve captured important parts code script.recommend always start script packages need. way, share code others, can easily see packages need install. Note, however, never include install.packages() setwd() script share. ’s antisocial change settings someone else’s computer!working future chapters, highly recommend starting editor practicing keyboard shortcuts. time, sending code console way become natural won’t even think .","code":"library(dplyr)\nlibrary(nycflights13)\n\nnot_cancelled <- flights %>% \n  filter(!is.na(dep_delay)█, !is.na(arr_delay))\n\nnot_cancelled %>% \n  group_by(year, month, day) %>% \n  summarise(mean = mean(dep_delay))"},{"path":"workflow-scripts.html","id":"rstudio-diagnostics","chapter":"6 Workflow: scripts","heading":"6.2 RStudio diagnostics","text":"script editor also highlight syntax errors red squiggly line cross sidebar:Hover cross see problem :RStudio also let know potential problems:","code":""},{"path":"workflow-scripts.html","id":"exercises-14","chapter":"6 Workflow: scripts","heading":"6.3 Exercises","text":"Go RStudio Tips Twitter account, https://twitter.com/rstudiotips\nfind one tip looks interesting. Practice using !Go RStudio Tips Twitter account, https://twitter.com/rstudiotips\nfind one tip looks interesting. Practice using !common mistakes RStudio diagnostics report? Read\nhttps://support.rstudio.com/hc/en-us/articles/205753617-Code-Diagnostics \nfind .common mistakes RStudio diagnostics report? Read\nhttps://support.rstudio.com/hc/en-us/articles/205753617-Code-Diagnostics \nfind .","code":""},{"path":"exploratory-data-analysis.html","id":"exploratory-data-analysis","chapter":"7 Exploratory Data Analysis","heading":"7 Exploratory Data Analysis","text":"","code":""},{"path":"exploratory-data-analysis.html","id":"introduction-3","chapter":"7 Exploratory Data Analysis","heading":"7.1 Introduction","text":"chapter show use visualisation transformation explore data systematic way, task statisticians call exploratory data analysis, EDA short. EDA iterative cycle. :Generate questions data.Generate questions data.Search answers visualising, transforming, modelling data.Search answers visualising, transforming, modelling data.Use learn refine questions /generate new questions.Use learn refine questions /generate new questions.EDA formal process strict set rules. anything, EDA state mind. initial phases EDA feel free investigate every idea occurs . ideas pan , dead ends. exploration continues, home particularly productive areas ’ll eventually write communicate others.EDA important part data analysis, even questions handed platter, always need investigate quality data. Data cleaning just one application EDA: ask questions whether data meets expectations . data cleaning, ’ll need deploy tools EDA: visualisation, transformation, modelling.","code":""},{"path":"exploratory-data-analysis.html","id":"prerequisites-3","chapter":"7 Exploratory Data Analysis","heading":"7.1.1 Prerequisites","text":"chapter ’ll combine ’ve learned dplyr ggplot2 interactively ask questions, answer data, ask new questions.","code":"\nlibrary(tidyverse)"},{"path":"exploratory-data-analysis.html","id":"questions","chapter":"7 Exploratory Data Analysis","heading":"7.2 Questions","text":"“routine statistical questions, questionable statistical\nroutines.” — Sir David Cox“Far better approximate answer right question, often\nvague, exact answer wrong question, can always made\nprecise.” — John TukeyYour goal EDA develop understanding data. easiest way use questions tools guide investigation. ask question, question focuses attention specific part dataset helps decide graphs, models, transformations make.EDA fundamentally creative process. like creative processes, key asking quality questions generate large quantity questions. difficult ask revealing questions start analysis know insights contained dataset. hand, new question ask expose new aspect data increase chance making discovery. can quickly drill interesting parts data—develop set thought-provoking questions—follow question new question based find.rule questions ask guide research. However, two types questions always useful making discoveries within data. can loosely word questions :type variation occurs within variables?type variation occurs within variables?type covariation occurs variables?type covariation occurs variables?rest chapter look two questions. ’ll explain variation covariation , ’ll show several ways answer question. make discussion easier, let’s define terms:variable quantity, quality, property can measure.variable quantity, quality, property can measure.value state variable measure . value \nvariable may change measurement measurement.value state variable measure . value \nvariable may change measurement measurement.observation set measurements made similar conditions\n(usually make measurements observation \ntime object). observation contain several values,\nassociated different variable. ’ll sometimes refer \nobservation data point.observation set measurements made similar conditions\n(usually make measurements observation \ntime object). observation contain several values,\nassociated different variable. ’ll sometimes refer \nobservation data point.Tabular data set values, associated variable \nobservation. Tabular data tidy value placed \n“cell”, variable column, observation \nrow.Tabular data set values, associated variable \nobservation. Tabular data tidy value placed \n“cell”, variable column, observation \nrow.far, data ’ve seen tidy. real-life, data isn’t tidy, ’ll come back ideas tidy data.","code":""},{"path":"exploratory-data-analysis.html","id":"variation","chapter":"7 Exploratory Data Analysis","heading":"7.3 Variation","text":"Variation tendency values variable change measurement measurement. can see variation easily real life; measure continuous variable twice, get two different results. true even measure quantities constant, like speed light. measurements include small amount error varies measurement measurement. Categorical variables can also vary measure across different subjects (e.g. eye colors different people), different times (e.g. energy levels electron different moments).\nEvery variable pattern variation, can reveal interesting information. best way understand pattern visualise distribution variable’s values.","code":""},{"path":"exploratory-data-analysis.html","id":"visualising-distributions","chapter":"7 Exploratory Data Analysis","heading":"7.3.1 Visualising distributions","text":"visualise distribution variable depend whether variable categorical continuous. variable categorical can take one small set values. R, categorical variables usually saved factors character vectors. examine distribution categorical variable, use bar chart:height bars displays many observations occurred x value. can compute values manually dplyr::count():variable continuous can take infinite set ordered values. Numbers date-times two examples continuous variables. examine distribution continuous variable, use histogram:can compute hand combining dplyr::count() ggplot2::cut_width():histogram divides x-axis equally spaced bins uses height bar display number observations fall bin. graph , tallest bar shows almost 30,000 observations carat value 0.25 0.75, left right edges bar.can set width intervals histogram binwidth argument, measured units x variable. always explore variety binwidths working histograms, different binwidths can reveal different patterns. example, graph looks zoom just diamonds size less three carats choose smaller binwidth.wish overlay multiple histograms plot, recommend using geom_freqpoly() instead geom_histogram(). geom_freqpoly() performs calculation geom_histogram(), instead displaying counts bars, uses lines instead. ’s much easier understand overlapping lines bars.challenges type plot, come back visualising categorical continuous variable.Now can visualise variation, look plots? type follow-questions ask? ’ve put together list useful types information find graphs, along follow-questions type information. key asking good follow-questions rely curiosity (want learn ?) well skepticism (misleading?).","code":"\nggplot(data = diamonds) +\n  geom_bar(mapping = aes(x = cut))\ndiamonds %>% \n  count(cut)\n#> # A tibble: 5 x 2\n#>   cut           n\n#> * <ord>     <int>\n#> 1 Fair       1610\n#> 2 Good       4906\n#> 3 Very Good 12082\n#> 4 Premium   13791\n#> 5 Ideal     21551\nggplot(data = diamonds) +\n  geom_histogram(mapping = aes(x = carat), binwidth = 0.5)\ndiamonds %>% \n  count(cut_width(carat, 0.5))\n#> # A tibble: 11 x 2\n#>   `cut_width(carat, 0.5)`     n\n#> * <fct>                   <int>\n#> 1 [-0.25,0.25]              785\n#> 2 (0.25,0.75]             29498\n#> 3 (0.75,1.25]             15977\n#> 4 (1.25,1.75]              5313\n#> 5 (1.75,2.25]              2002\n#> 6 (2.25,2.75]               322\n#> # … with 5 more rows\nsmaller <- diamonds %>% \n  filter(carat < 3)\n  \nggplot(data = smaller, mapping = aes(x = carat)) +\n  geom_histogram(binwidth = 0.1)\nggplot(data = smaller, mapping = aes(x = carat, colour = cut)) +\n  geom_freqpoly(binwidth = 0.1)"},{"path":"exploratory-data-analysis.html","id":"typical-values","chapter":"7 Exploratory Data Analysis","heading":"7.3.2 Typical values","text":"bar charts histograms, tall bars show common values variable, shorter bars show less-common values. Places bars reveal values seen data. turn information useful questions, look anything unexpected:values common? ?values common? ?values rare? ? match expectations?values rare? ? match expectations?Can see unusual patterns? might explain ?Can see unusual patterns? might explain ?example, histogram suggests several interesting questions:diamonds whole carats common fractions carats?diamonds whole carats common fractions carats?diamonds slightly right peak \nslightly left peak?diamonds slightly right peak \nslightly left peak?diamonds bigger 3 carats?diamonds bigger 3 carats?Clusters similar values suggest subgroups exist data. understand subgroups, ask:observations within cluster similar ?observations within cluster similar ?observations separate clusters different ?observations separate clusters different ?can explain describe clusters?can explain describe clusters?might appearance clusters misleading?might appearance clusters misleading?histogram shows length (minutes) 272 eruptions Old Faithful Geyser Yellowstone National Park. Eruption times appear clustered two groups: short eruptions (around 2 minutes) long eruptions (4-5 minutes), little .Many questions prompt explore relationship variables, example, see values one variable can explain behavior another variable. ’ll get shortly.","code":"\nggplot(data = smaller, mapping = aes(x = carat)) +\n  geom_histogram(binwidth = 0.01)\nggplot(data = faithful, mapping = aes(x = eruptions)) + \n  geom_histogram(binwidth = 0.25)"},{"path":"exploratory-data-analysis.html","id":"unusual-values","chapter":"7 Exploratory Data Analysis","heading":"7.3.3 Unusual values","text":"Outliers observations unusual; data points don’t seem fit pattern. Sometimes outliers data entry errors; times outliers suggest important new science. lot data, outliers sometimes difficult see histogram. example, take distribution y variable diamonds dataset. evidence outliers unusually wide limits x-axis.many observations common bins rare bins short can’t see (although maybe stare intently 0 ’ll spot something). make easy see unusual values, need zoom small values y-axis coord_cartesian():(coord_cartesian() also xlim() argument need zoom x-axis. ggplot2 also xlim() ylim() functions work slightly differently: throw away data outside limits.)allows us see three unusual values: 0, ~30, ~60. pluck dplyr:y variable measures one three dimensions diamonds, mm. know diamonds can’t width 0mm, values must incorrect. might also suspect measurements 32mm 59mm implausible: diamonds inch long, don’t cost hundreds thousands dollars!’s good practice repeat analysis without outliers. minimal effect results, can’t figure ’re , ’s reasonable replace missing values, move . However, substantial effect results, shouldn’t drop without justification. ’ll need figure caused (e.g. data entry error) disclose removed write-.","code":"\nggplot(diamonds) + \n  geom_histogram(mapping = aes(x = y), binwidth = 0.5)\nggplot(diamonds) + \n  geom_histogram(mapping = aes(x = y), binwidth = 0.5) +\n  coord_cartesian(ylim = c(0, 50))\nunusual <- diamonds %>% \n  filter(y < 3 | y > 20) %>% \n  select(price, x, y, z) %>%\n  arrange(y)\nunusual\n#> # A tibble: 9 x 4\n#>   price     x     y     z\n#>   <int> <dbl> <dbl> <dbl>\n#> 1  5139  0      0    0   \n#> 2  6381  0      0    0   \n#> 3 12800  0      0    0   \n#> 4 15686  0      0    0   \n#> 5 18034  0      0    0   \n#> 6  2130  0      0    0   \n#> 7  2130  0      0    0   \n#> 8  2075  5.15  31.8  5.12\n#> 9 12210  8.09  58.9  8.06"},{"path":"exploratory-data-analysis.html","id":"exercises-15","chapter":"7 Exploratory Data Analysis","heading":"7.3.4 Exercises","text":"Explore distribution x, y, z variables\ndiamonds. learn? Think diamond \nmight decide dimension length, width, depth.Explore distribution x, y, z variables\ndiamonds. learn? Think diamond \nmight decide dimension length, width, depth.Explore distribution price. discover anything unusual\nsurprising? (Hint: Carefully think binwidth make sure\ntry wide range values.)Explore distribution price. discover anything unusual\nsurprising? (Hint: Carefully think binwidth make sure\ntry wide range values.)many diamonds 0.99 carat? many 1 carat? \nthink cause difference?many diamonds 0.99 carat? many 1 carat? \nthink cause difference?Compare contrast coord_cartesian() vs xlim() ylim() \nzooming histogram. happens leave binwidth unset?\nhappens try zoom half bar shows?Compare contrast coord_cartesian() vs xlim() ylim() \nzooming histogram. happens leave binwidth unset?\nhappens try zoom half bar shows?","code":""},{"path":"exploratory-data-analysis.html","id":"missing-values-2","chapter":"7 Exploratory Data Analysis","heading":"7.4 Missing values","text":"’ve encountered unusual values dataset, simply want move rest analysis, two options.Drop entire row strange values:\n\ndiamonds2 <- diamonds %>% \n  filter((y, 3, 20))\ndon’t recommend option just one measurement\ninvalid, doesn’t mean measurements . Additionally, \nlow quality data, time ’ve applied approach every\nvariable might find don’t data left!Drop entire row strange values:don’t recommend option just one measurement\ninvalid, doesn’t mean measurements . Additionally, \nlow quality data, time ’ve applied approach every\nvariable might find don’t data left!Instead, recommend replacing unusual values missing values.\neasiest way use mutate() replace variable\nmodified copy. can use ifelse() function replace\nunusual values NA:\n\ndiamonds2 <- diamonds %>% \n  mutate(y = ifelse(y < 3 | y > 20, NA, y))Instead, recommend replacing unusual values missing values.\neasiest way use mutate() replace variable\nmodified copy. can use ifelse() function replace\nunusual values NA:ifelse() three arguments. first argument test logical vector. result contain value second argument, yes, test TRUE, value third argument, , false. Alternatively ifelse, use dplyr::case_when(). case_when() particularly useful inside mutate want create new variable relies complex combination existing variables.Like R, ggplot2 subscribes philosophy missing values never silently go missing. ’s obvious plot missing values, ggplot2 doesn’t include plot, warn ’ve removed:suppress warning, set na.rm = TRUE:times want understand makes observations missing values different observations recorded values. example, nycflights13::flights, missing values dep_time variable indicate flight cancelled. might want compare scheduled departure times cancelled non-cancelled times. can making new variable .na().However plot isn’t great many non-cancelled flights cancelled flights. next section ’ll explore techniques improving comparison.","code":"\ndiamonds2 <- diamonds %>% \n  filter(between(y, 3, 20))\ndiamonds2 <- diamonds %>% \n  mutate(y = ifelse(y < 3 | y > 20, NA, y))\nggplot(data = diamonds2, mapping = aes(x = x, y = y)) + \n  geom_point()\n#> Warning: Removed 9 rows containing missing values (geom_point).\nggplot(data = diamonds2, mapping = aes(x = x, y = y)) + \n  geom_point(na.rm = TRUE)\nnycflights13::flights %>% \n  mutate(\n    cancelled = is.na(dep_time),\n    sched_hour = sched_dep_time %/% 100,\n    sched_min = sched_dep_time %% 100,\n    sched_dep_time = sched_hour + sched_min / 60\n  ) %>% \n  ggplot(mapping = aes(sched_dep_time)) + \n    geom_freqpoly(mapping = aes(colour = cancelled), binwidth = 1/4)"},{"path":"exploratory-data-analysis.html","id":"exercises-16","chapter":"7 Exploratory Data Analysis","heading":"7.4.1 Exercises","text":"happens missing values histogram? happens missing\nvalues bar chart? difference?happens missing values histogram? happens missing\nvalues bar chart? difference?na.rm = TRUE mean() sum()?na.rm = TRUE mean() sum()?","code":""},{"path":"exploratory-data-analysis.html","id":"covariation","chapter":"7 Exploratory Data Analysis","heading":"7.5 Covariation","text":"variation describes behavior within variable, covariation describes behavior variables. Covariation tendency values two variables vary together related way. best way spot covariation visualise relationship two variables. depend type variables involved.","code":""},{"path":"exploratory-data-analysis.html","id":"cat-cont","chapter":"7 Exploratory Data Analysis","heading":"7.5.1 A categorical and continuous variable","text":"’s common want explore distribution continuous variable broken categorical variable, previous frequency polygon. default appearance geom_freqpoly() useful sort comparison height given count. means one groups much smaller others, ’s hard see differences shape. example, let’s explore price diamond varies quality:’s hard see difference distribution overall counts differ much:make comparison easier need swap displayed y-axis. Instead displaying count, ’ll display density, count standardised area frequency polygon one.’s something rather surprising plot - appears fair diamonds (lowest quality) highest average price! maybe ’s frequency polygons little hard interpret - ’s lot going plot.Another alternative display distribution continuous variable broken categorical variable boxplot. boxplot type visual shorthand distribution values popular among statisticians. boxplot consists :box stretches 25th percentile distribution \n75th percentile, distance known interquartile range (IQR). \nmiddle box line displays median, .e. 50th percentile,\ndistribution. three lines give sense spread \ndistribution whether distribution symmetric \nmedian skewed one side.box stretches 25th percentile distribution \n75th percentile, distance known interquartile range (IQR). \nmiddle box line displays median, .e. 50th percentile,\ndistribution. three lines give sense spread \ndistribution whether distribution symmetric \nmedian skewed one side.Visual points display observations fall 1.5 times \nIQR either edge box. outlying points unusual\nplotted individually.Visual points display observations fall 1.5 times \nIQR either edge box. outlying points unusual\nplotted individually.line (whisker) extends end box goes \nfarthest non-outlier point distribution.line (whisker) extends end box goes \nfarthest non-outlier point distribution.Let’s take look distribution price cut using geom_boxplot():see much less information distribution, boxplots much compact can easily compare (fit one plot). supports counterintuitive finding better quality diamonds cheaper average! exercises, ’ll challenged figure .cut ordered factor: fair worse good, worse good . Many categorical variables don’t intrinsic order, might want reorder make informative display. One way reorder() function.example, take class variable mpg dataset. might interested know highway mileage varies across classes:make trend easier see, can reorder class based median value hwy:long variable names, geom_boxplot() work better flip 90°. can coord_flip().","code":"\nggplot(data = diamonds, mapping = aes(x = price)) + \n  geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)\nggplot(diamonds) + \n  geom_bar(mapping = aes(x = cut))\nggplot(data = diamonds, mapping = aes(x = price, y = ..density..)) + \n  geom_freqpoly(mapping = aes(colour = cut), binwidth = 500)\nggplot(data = diamonds, mapping = aes(x = cut, y = price)) +\n  geom_boxplot()\nggplot(data = mpg, mapping = aes(x = class, y = hwy)) +\n  geom_boxplot()\nggplot(data = mpg) +\n  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy))\nggplot(data = mpg) +\n  geom_boxplot(mapping = aes(x = reorder(class, hwy, FUN = median), y = hwy)) +\n  coord_flip()"},{"path":"exploratory-data-analysis.html","id":"exercises-17","chapter":"7 Exploratory Data Analysis","heading":"7.5.1.1 Exercises","text":"Use ’ve learned improve visualisation departure times\ncancelled vs. non-cancelled flights.Use ’ve learned improve visualisation departure times\ncancelled vs. non-cancelled flights.variable diamonds dataset important predicting\nprice diamond? variable correlated cut?\ncombination two relationships lead lower quality\ndiamonds expensive?variable diamonds dataset important predicting\nprice diamond? variable correlated cut?\ncombination two relationships lead lower quality\ndiamonds expensive?Exchange x variable y variable vertical boxplot, create\nhorizontal boxplot. compare using coord_flip()?Exchange x variable y variable vertical boxplot, create\nhorizontal boxplot. compare using coord_flip()?One problem boxplots developed era \nmuch smaller datasets tend display prohibitively large\nnumber “outlying values”. One approach remedy problem \nletter value plot. Install lvplot package, try using\ngeom_lv() display distribution price vs cut. \nlearn? interpret plots?One problem boxplots developed era \nmuch smaller datasets tend display prohibitively large\nnumber “outlying values”. One approach remedy problem \nletter value plot. Install lvplot package, try using\ngeom_lv() display distribution price vs cut. \nlearn? interpret plots?Compare contrast geom_violin() facetted geom_histogram(),\ncoloured geom_freqpoly(). pros cons \nmethod?Compare contrast geom_violin() facetted geom_histogram(),\ncoloured geom_freqpoly(). pros cons \nmethod?small dataset, ’s sometimes useful use geom_jitter()\nsee relationship continuous categorical variable.\nggbeeswarm package provides number methods similar \ngeom_jitter(). List briefly describe one .small dataset, ’s sometimes useful use geom_jitter()\nsee relationship continuous categorical variable.\nggbeeswarm package provides number methods similar \ngeom_jitter(). List briefly describe one .","code":""},{"path":"exploratory-data-analysis.html","id":"two-categorical-variables","chapter":"7 Exploratory Data Analysis","heading":"7.5.2 Two categorical variables","text":"visualise covariation categorical variables, ’ll need count number observations combination. One way rely built-geom_count():size circle plot displays many observations occurred combination values. Covariation appear strong correlation specific x values specific y values.Another approach compute count dplyr:visualise geom_tile() fill aesthetic:categorical variables unordered, might want use seriation package simultaneously reorder rows columns order clearly reveal interesting patterns. larger plots, might want try d3heatmap heatmaply packages, create interactive plots.","code":"\nggplot(data = diamonds) +\n  geom_count(mapping = aes(x = cut, y = color))\ndiamonds %>% \n  count(color, cut)\n#> # A tibble: 35 x 3\n#>   color cut           n\n#>   <ord> <ord>     <int>\n#> 1 D     Fair        163\n#> 2 D     Good        662\n#> 3 D     Very Good  1513\n#> 4 D     Premium    1603\n#> 5 D     Ideal      2834\n#> 6 E     Fair        224\n#> # … with 29 more rows\ndiamonds %>% \n  count(color, cut) %>%  \n  ggplot(mapping = aes(x = color, y = cut)) +\n    geom_tile(mapping = aes(fill = n))"},{"path":"exploratory-data-analysis.html","id":"exercises-18","chapter":"7 Exploratory Data Analysis","heading":"7.5.2.1 Exercises","text":"rescale count dataset clearly show\ndistribution cut within colour, colour within cut?rescale count dataset clearly show\ndistribution cut within colour, colour within cut?Use geom_tile() together dplyr explore average flight\ndelays vary destination month year. makes \nplot difficult read? improve ?Use geom_tile() together dplyr explore average flight\ndelays vary destination month year. makes \nplot difficult read? improve ?slightly better use aes(x = color, y = cut) rather\naes(x = cut, y = color) example ?slightly better use aes(x = color, y = cut) rather\naes(x = cut, y = color) example ?","code":""},{"path":"exploratory-data-analysis.html","id":"two-continuous-variables","chapter":"7 Exploratory Data Analysis","heading":"7.5.3 Two continuous variables","text":"’ve already seen one great way visualise covariation two continuous variables: draw scatterplot geom_point(). can see covariation pattern points. example, can see exponential relationship carat size price diamond.Scatterplots become less useful size dataset grows, points begin overplot, pile areas uniform black ().\n’ve already seen one way fix problem: using alpha aesthetic add transparency.using transparency can challenging large datasets. Another solution use bin. Previously used geom_histogram() geom_freqpoly() bin one dimension. Now ’ll learn use geom_bin2d() geom_hex() bin two dimensions.geom_bin2d() geom_hex() divide coordinate plane 2d bins use fill color display many points fall bin. geom_bin2d() creates rectangular bins. geom_hex() creates hexagonal bins. need install hexbin package use geom_hex().Another option bin one continuous variable acts like categorical variable. can use one techniques visualising combination categorical continuous variable learned . example, bin carat group, display boxplot:cut_width(x, width), used , divides x bins width width. default, boxplots look roughly (apart number outliers) regardless many observations , ’s difficult tell boxplot summarises different number points. One way show make width boxplot proportional number points varwidth = TRUE.Another approach display approximately number points bin. ’s job cut_number():","code":"\nggplot(data = diamonds) +\n  geom_point(mapping = aes(x = carat, y = price))\nggplot(data = diamonds) + \n  geom_point(mapping = aes(x = carat, y = price), alpha = 1 / 100)\nggplot(data = smaller) +\n  geom_bin2d(mapping = aes(x = carat, y = price))\n\n# install.packages(\"hexbin\")\nggplot(data = smaller) +\n  geom_hex(mapping = aes(x = carat, y = price))\n#> Warning: Computation failed in `stat_binhex()`:\n#>   Package `hexbin` required for `stat_binhex`.\n#>   Please install and try again.\nggplot(data = smaller, mapping = aes(x = carat, y = price)) + \n  geom_boxplot(mapping = aes(group = cut_width(carat, 0.1)))\nggplot(data = smaller, mapping = aes(x = carat, y = price)) + \n  geom_boxplot(mapping = aes(group = cut_number(carat, 20)))"},{"path":"exploratory-data-analysis.html","id":"exercises-19","chapter":"7 Exploratory Data Analysis","heading":"7.5.3.1 Exercises","text":"Instead summarising conditional distribution boxplot, \nuse frequency polygon. need consider using\ncut_width() vs cut_number()? impact visualisation \n2d distribution carat price?Instead summarising conditional distribution boxplot, \nuse frequency polygon. need consider using\ncut_width() vs cut_number()? impact visualisation \n2d distribution carat price?Visualise distribution carat, partitioned price.Visualise distribution carat, partitioned price.price distribution large diamonds compare small\ndiamonds? expect, surprise ?price distribution large diamonds compare small\ndiamonds? expect, surprise ?Combine two techniques ’ve learned visualise \ncombined distribution cut, carat, price.Combine two techniques ’ve learned visualise \ncombined distribution cut, carat, price.Two dimensional plots reveal outliers visible one\ndimensional plots. example, points plot \nunusual combination x y values, makes points outliers\neven though x y values appear normal examined separately.\n\nggplot(data = diamonds) +\n  geom_point(mapping = aes(x = x, y = y)) +\n  coord_cartesian(xlim = c(4, 11), ylim = c(4, 11))\n\nscatterplot better display binned plot case?Two dimensional plots reveal outliers visible one\ndimensional plots. example, points plot \nunusual combination x y values, makes points outliers\neven though x y values appear normal examined separately.scatterplot better display binned plot case?","code":"\nggplot(data = diamonds) +\n  geom_point(mapping = aes(x = x, y = y)) +\n  coord_cartesian(xlim = c(4, 11), ylim = c(4, 11))"},{"path":"exploratory-data-analysis.html","id":"patterns-and-models","chapter":"7 Exploratory Data Analysis","heading":"7.6 Patterns and models","text":"Patterns data provide clues relationships. systematic relationship exists two variables appear pattern data. spot pattern, ask :pattern due coincidence (.e. random chance)?pattern due coincidence (.e. random chance)?can describe relationship implied pattern?can describe relationship implied pattern?strong relationship implied pattern?strong relationship implied pattern?variables might affect relationship?variables might affect relationship?relationship change look individual subgroups data?relationship change look individual subgroups data?scatterplot Old Faithful eruption lengths versus wait time eruptions shows pattern: longer wait times associated longer eruptions. scatterplot also displays two clusters noticed .Patterns provide one useful tools data scientists reveal covariation. think variation phenomenon creates uncertainty, covariation phenomenon reduces . two variables covary, can use values one variable make better predictions values second. covariation due causal relationship (special case), can use value one variable control value second.Models tool extracting patterns data. example, consider diamonds data. ’s hard understand relationship cut price, cut carat, carat price tightly related. ’s possible use model remove strong relationship price carat can explore subtleties remain. following code fits model predicts price carat computes residuals (difference predicted value actual value). residuals give us view price diamond, effect carat removed.’ve removed strong relationship carat price, can see expect relationship cut price: relative size, better quality diamonds expensive.’ll learn models, modelr package, work final part book, model. ’re saving modelling later understanding models work easiest tools data wrangling programming hand.","code":"\nggplot(data = faithful) + \n  geom_point(mapping = aes(x = eruptions, y = waiting))\nlibrary(modelr)\n\nmod <- lm(log(price) ~ log(carat), data = diamonds)\n\ndiamonds2 <- diamonds %>% \n  add_residuals(mod) %>% \n  mutate(resid = exp(resid))\n\nggplot(data = diamonds2) + \n  geom_point(mapping = aes(x = carat, y = resid))\nggplot(data = diamonds2) + \n  geom_boxplot(mapping = aes(x = cut, y = resid))"},{"path":"exploratory-data-analysis.html","id":"ggplot2-calls","chapter":"7 Exploratory Data Analysis","heading":"7.7 ggplot2 calls","text":"move introductory chapters, ’ll transition concise expression ggplot2 code. far ’ve explicit, helpful learning:Typically, first one two arguments function important know heart. first two arguments ggplot() data mapping, first two arguments aes() x y. remainder book, won’t supply names. saves typing, , reducing amount boilerplate, makes easier see ’s different plots. ’s really important programming concern ’ll come back functions.Rewriting previous plot concisely yields:Sometimes ’ll turn end pipeline data transformation plot. Watch transition %>% +. wish transition wasn’t necessary unfortunately ggplot2 created pipe discovered.","code":"\nggplot(data = faithful, mapping = aes(x = eruptions)) + \n  geom_freqpoly(binwidth = 0.25)\nggplot(faithful, aes(eruptions)) + \n  geom_freqpoly(binwidth = 0.25)\ndiamonds %>% \n  count(cut, clarity) %>% \n  ggplot(aes(clarity, cut, fill = n)) + \n    geom_tile()"},{"path":"exploratory-data-analysis.html","id":"learning-more","chapter":"7 Exploratory Data Analysis","heading":"7.8 Learning more","text":"want learn mechanics ggplot2, ’d highly recommend grabbing copy ggplot2 book: https://amzn.com/331924275X. ’s recently updated, includes dplyr tidyr code, much space explore facets visualisation. Unfortunately book isn’t generally available free, connection university can probably get electronic version free SpringerLink.Another useful resource R Graphics Cookbook Winston Chang. Much contents available online http://www.cookbook-r.com/Graphs/.also recommend Graphical Data Analysis R, Antony Unwin. book-length treatment similar material covered chapter, space go much greater depth.","code":""},{"path":"workflow-projects.html","id":"workflow-projects","chapter":"8 Workflow: projects","heading":"8 Workflow: projects","text":"One day need quit R, go something else return analysis next day. One day working multiple analyses simultaneously use R want keep separate. One day need bring data outside world R send numerical results figures R back world. handle real life situations, need make two decisions:analysis “real”, .e. save \nlasting record happened?analysis “real”, .e. save \nlasting record happened?analysis “live”?analysis “live”?","code":""},{"path":"workflow-projects.html","id":"what-is-real","chapter":"8 Workflow: projects","heading":"8.1 What is real?","text":"beginning R user, ’s OK consider environment (.e. objects listed environment pane) “real”. However, long run, ’ll much better consider R scripts “real”.R scripts (data files), can recreate environment. ’s much harder recreate R scripts environment! ’ll either retype lot code memory (making mistakes way) ’ll carefully mine R history.foster behaviour, highly recommend instruct RStudio preserve workspace sessions:cause short-term pain, now restart RStudio remember results code ran last time. short-term pain save long-term agony forces capture important interactions code. ’s nothing worse discovering three months fact ’ve stored results important calculation workspace, calculation code.great pair keyboard shortcuts work together make sure ’ve captured important parts code editor:Press Cmd/Ctrl + Shift + F10 restart RStudio.Press Cmd/Ctrl + Shift + S rerun current script.use pattern hundreds times week.","code":""},{"path":"workflow-projects.html","id":"where-does-your-analysis-live","chapter":"8 Workflow: projects","heading":"8.2 Where does your analysis live?","text":"R powerful notion working directory. R looks files ask load, put files ask save. RStudio shows current working directory top console:can print R code running getwd():beginning R user, ’s OK let home directory, documents directory, weird directory computer R’s working directory. ’re six chapters book, ’re longer rank beginner. soon now evolve organising analytical projects directories , working project, setting R’s working directory associated directory.recommend , can also set working directory within R:never ’s better way; way also puts path managing R work like expert.","code":"\ngetwd()\n#> [1] \"/Users/hadley/Documents/r4ds/r4ds\"\nsetwd(\"/path/to/my/CoolProject\")"},{"path":"workflow-projects.html","id":"paths-and-directories","chapter":"8 Workflow: projects","heading":"8.3 Paths and directories","text":"Paths directories little complicated two basic styles paths: Mac/Linux Windows. three chief ways differ:important difference separate components \npath. Mac Linux uses slashes (e.g. plots/diamonds.pdf) Windows\nuses backslashes (e.g. plots\\diamonds.pdf). R can work either type\n(matter platform ’re currently using), unfortunately,\nbackslashes mean something special R, get single backslash\npath, need type two backslashes! makes life frustrating,\nrecommend always using Linux/Mac style forward slashes.important difference separate components \npath. Mac Linux uses slashes (e.g. plots/diamonds.pdf) Windows\nuses backslashes (e.g. plots\\diamonds.pdf). R can work either type\n(matter platform ’re currently using), unfortunately,\nbackslashes mean something special R, get single backslash\npath, need type two backslashes! makes life frustrating,\nrecommend always using Linux/Mac style forward slashes.Absolute paths (.e. paths point place regardless \nworking directory) look different. Windows start drive\nletter (e.g. C:) two backslashes (e.g. \\\\servername) \nMac/Linux start slash “/” (e.g. /users/hadley). \nnever use absolute paths scripts, hinder sharing:\none else exactly directory configuration .Absolute paths (.e. paths point place regardless \nworking directory) look different. Windows start drive\nletter (e.g. C:) two backslashes (e.g. \\\\servername) \nMac/Linux start slash “/” (e.g. /users/hadley). \nnever use absolute paths scripts, hinder sharing:\none else exactly directory configuration .last minor difference place ~ points . ~ \nconvenient shortcut home directory. Windows doesn’t really \nnotion home directory, instead points documents\ndirectory.last minor difference place ~ points . ~ \nconvenient shortcut home directory. Windows doesn’t really \nnotion home directory, instead points documents\ndirectory.","code":""},{"path":"workflow-projects.html","id":"rstudio-projects","chapter":"8 Workflow: projects","heading":"8.4 RStudio projects","text":"R experts keep files associated project together — input data, R scripts, analytical results, figures. wise common practice RStudio built-support via projects.Let’s make project use ’re working rest book. Click File > New Project, :Call project r4ds think carefully subdirectory put project . don’t store somewhere sensible, hard find future!process complete, ’ll get new RStudio project just book. Check “home” directory project current working directory:Whenever refer file relative path look .Now enter following commands script editor, save file, calling “diamonds.R”. Next, run complete script save PDF CSV file project directory. Don’t worry details, ’ll learn later book.Quit RStudio. Inspect folder associated project — notice .Rproj file. Double-click file re-open project. Notice get back left : ’s working directory command history, files working still open. followed instructions , , however, completely fresh environment, guaranteeing ’re starting clean slate.favorite OS-specific way, search computer diamonds.pdf find PDF (surprise) also script created (diamonds.R). huge win! One day want remake figure just understand came . rigorously save figures files R code never mouse clipboard, able reproduce old work ease!","code":"\ngetwd()\n#> [1] /Users/hadley/Documents/r4ds/r4ds\nlibrary(tidyverse)\n\nggplot(diamonds, aes(carat, price)) + \n  geom_hex()\nggsave(\"diamonds.pdf\")\n\nwrite_csv(diamonds, \"diamonds.csv\")"},{"path":"workflow-projects.html","id":"summary","chapter":"8 Workflow: projects","heading":"8.5 Summary","text":"summary, RStudio projects give solid workflow serve well future:Create RStudio project data analysis project.Create RStudio project data analysis project.Keep data files ; ’ll talk loading R \ndata import.Keep data files ; ’ll talk loading R \ndata import.Keep scripts ; edit , run bits whole.Keep scripts ; edit , run bits whole.Save outputs (plots cleaned data) .Save outputs (plots cleaned data) .ever use relative paths, absolute paths.ever use relative paths, absolute paths.Everything need one place, cleanly separated projects working .","code":""},{"path":"wrangle-intro.html","id":"wrangle-intro","chapter":"9 Introduction","heading":"9 Introduction","text":"part book, ’ll learn data wrangling, art getting data R useful form visualisation modelling. Data wrangling important: without can’t work data! three main parts data wrangling:part book proceeds follows:tibbles, ’ll learn variant data frame use\nbook: tibble. ’ll learn makes different\nregular data frames, can construct “hand”.tibbles, ’ll learn variant data frame use\nbook: tibble. ’ll learn makes different\nregular data frames, can construct “hand”.data import, ’ll learn get data disk R.\n’ll focus plain-text rectangular formats, give pointers\npackages help types data.data import, ’ll learn get data disk R.\n’ll focus plain-text rectangular formats, give pointers\npackages help types data.tidy data, ’ll learn tidy data, consistent way storing\ndata makes transformation, visualisation, modelling easier.\n’ll learn underlying principles, get data \ntidy form.tidy data, ’ll learn tidy data, consistent way storing\ndata makes transformation, visualisation, modelling easier.\n’ll learn underlying principles, get data \ntidy form.Data wrangling also encompasses data transformation, ’ve already learned little . Now ’ll focus new skills three specific types data frequently encounter practice:Relational data give tools working multiple\ninterrelated datasets.Relational data give tools working multiple\ninterrelated datasets.Strings introduce regular expressions, powerful tool \nmanipulating strings.Strings introduce regular expressions, powerful tool \nmanipulating strings.Factors R stores categorical data. used variable\nfixed set possible values, want use non-alphabetical\nordering string.Factors R stores categorical data. used variable\nfixed set possible values, want use non-alphabetical\nordering string.Dates times give key tools working \ndates date-times.Dates times give key tools working \ndates date-times.","code":""},{"path":"tibbles.html","id":"tibbles","chapter":"10 Tibbles","heading":"10 Tibbles","text":"","code":""},{"path":"tibbles.html","id":"introduction-4","chapter":"10 Tibbles","heading":"10.1 Introduction","text":"Throughout book work “tibbles” instead R’s traditional data.frame. Tibbles data frames, tweak older behaviours make life little easier. R old language, things useful 10 20 years ago now get way. ’s difficult change base R without breaking existing code, innovation occurs packages. describe tibble package, provides opinionated data frames make working tidyverse little easier. places, ’ll use term tibble data frame interchangeably; want draw particular attention R’s built-data frame, ’ll call data.frames.chapter leaves wanting learn tibbles, might enjoy vignette(\"tibble\").","code":""},{"path":"tibbles.html","id":"prerequisites-4","chapter":"10 Tibbles","heading":"10.1.1 Prerequisites","text":"chapter ’ll explore tibble package, part core tidyverse.","code":"\nlibrary(tidyverse)"},{"path":"tibbles.html","id":"creating-tibbles","chapter":"10 Tibbles","heading":"10.2 Creating tibbles","text":"Almost functions ’ll use book produce tibbles, tibbles one unifying features tidyverse. R packages use regular data frames, might want coerce data frame tibble. can as_tibble():can create new tibble individual vectors tibble(). tibble() automatically recycle inputs length 1, allows refer variables just created, shown .’re already familiar data.frame(), note tibble() much less: never changes type inputs (e.g. never converts strings factors!), never changes names variables, never creates row names.’s possible tibble column names valid R variable names, aka non-syntactic names. example, might start letter, might contain unusual characters like space. refer variables, need surround backticks, `:’ll also need backticks working variables packages, like ggplot2, dplyr, tidyr.Another way create tibble tribble(), short transposed tibble. tribble() customised data entry code: column headings defined formulas (.e. start ~), entries separated commas. makes possible lay small amounts data easy read form.often add comment (line starting #), make really clear header .","code":"\nas_tibble(iris)\n#> # A tibble: 150 x 5\n#>   Sepal.Length Sepal.Width Petal.Length Petal.Width Species\n#>          <dbl>       <dbl>        <dbl>       <dbl> <fct>  \n#> 1          5.1         3.5          1.4         0.2 setosa \n#> 2          4.9         3            1.4         0.2 setosa \n#> 3          4.7         3.2          1.3         0.2 setosa \n#> 4          4.6         3.1          1.5         0.2 setosa \n#> 5          5           3.6          1.4         0.2 setosa \n#> 6          5.4         3.9          1.7         0.4 setosa \n#> # … with 144 more rows\ntibble(\n  x = 1:5, \n  y = 1, \n  z = x ^ 2 + y\n)\n#> # A tibble: 5 x 3\n#>       x     y     z\n#>   <int> <dbl> <dbl>\n#> 1     1     1     2\n#> 2     2     1     5\n#> 3     3     1    10\n#> 4     4     1    17\n#> 5     5     1    26\ntb <- tibble(\n  `:)` = \"smile\", \n  ` ` = \"space\",\n  `2000` = \"number\"\n)\ntb\n#> # A tibble: 1 x 3\n#>   `:)`  ` `   `2000`\n#>   <chr> <chr> <chr> \n#> 1 smile space number\ntribble(\n  ~x, ~y, ~z,\n  #--|--|----\n  \"a\", 2, 3.6,\n  \"b\", 1, 8.5\n)\n#> # A tibble: 2 x 3\n#>   x         y     z\n#>   <chr> <dbl> <dbl>\n#> 1 a         2   3.6\n#> 2 b         1   8.5"},{"path":"tibbles.html","id":"tibbles-vs.-data.frame","chapter":"10 Tibbles","heading":"10.3 Tibbles vs. data.frame","text":"two main differences usage tibble vs. classic data.frame: printing subsetting.","code":""},{"path":"tibbles.html","id":"printing","chapter":"10 Tibbles","heading":"10.3.1 Printing","text":"Tibbles refined print method shows first 10 rows, columns fit screen. makes much easier work large data. addition name, column reports type, nice feature borrowed str():Tibbles designed don’t accidentally overwhelm console print large data frames. sometimes need output default display. options can help.First, can explicitly print() data frame control number rows (n) width display. width = Inf display columns:can also control default print behaviour setting options:options(tibble.print_max = n, tibble.print_min = m): n\nrows, print m rows. Use options(tibble.print_min = Inf) always\nshow rows.options(tibble.print_max = n, tibble.print_min = m): n\nrows, print m rows. Use options(tibble.print_min = Inf) always\nshow rows.Use options(tibble.width = Inf) always print columns, regardless\nwidth screen.Use options(tibble.width = Inf) always print columns, regardless\nwidth screen.can see complete list options looking package help package?tibble.final option use RStudio’s built-data viewer get scrollable view complete dataset. also often useful end long chain manipulations.","code":"\ntibble(\n  a = lubridate::now() + runif(1e3) * 86400,\n  b = lubridate::today() + runif(1e3) * 30,\n  c = 1:1e3,\n  d = runif(1e3),\n  e = sample(letters, 1e3, replace = TRUE)\n)\n#> # A tibble: 1,000 x 5\n#>   a                   b              c     d e    \n#>   <dttm>              <date>     <int> <dbl> <chr>\n#> 1 2021-02-14 21:06:50 2021-02-21     1 0.368 n    \n#> 2 2021-02-15 15:12:00 2021-02-26     2 0.612 l    \n#> 3 2021-02-15 09:35:39 2021-03-08     3 0.415 p    \n#> 4 2021-02-14 22:56:56 2021-03-07     4 0.212 m    \n#> 5 2021-02-14 19:21:13 2021-03-04     5 0.733 i    \n#> 6 2021-02-15 06:22:10 2021-02-28     6 0.460 n    \n#> # … with 994 more rows\nnycflights13::flights %>% \n  print(n = 10, width = Inf)\nnycflights13::flights %>% \n  View()"},{"path":"tibbles.html","id":"subsetting","chapter":"10 Tibbles","heading":"10.3.2 Subsetting","text":"far tools ’ve learned worked complete data frames. want pull single variable, need new tools, $ [[. [[ can extract name position; $ extracts name little less typing.use pipe, ’ll need use special placeholder .:Compared data.frame, tibbles strict: never partial matching, generate warning column trying access exist.","code":"\ndf <- tibble(\n  x = runif(5),\n  y = rnorm(5)\n)\n\n# Extract by name\ndf$x\n#> [1] 0.73296674 0.23436542 0.66035540 0.03285612 0.46049161\ndf[[\"x\"]]\n#> [1] 0.73296674 0.23436542 0.66035540 0.03285612 0.46049161\n\n# Extract by position\ndf[[1]]\n#> [1] 0.73296674 0.23436542 0.66035540 0.03285612 0.46049161\ndf %>% .$x\n#> [1] 0.73296674 0.23436542 0.66035540 0.03285612 0.46049161\ndf %>% .[[\"x\"]]\n#> [1] 0.73296674 0.23436542 0.66035540 0.03285612 0.46049161"},{"path":"tibbles.html","id":"interacting-with-older-code","chapter":"10 Tibbles","heading":"10.4 Interacting with older code","text":"older functions don’t work tibbles. encounter one functions, use .data.frame() turn tibble back data.frame:main reason older functions don’t work tibble [ function. don’t use [ much book dplyr::filter() dplyr::select() allow solve problems clearer code (learn little vector subsetting). base R data frames, [ sometimes returns data frame, sometimes returns vector. tibbles, [ always returns another tibble.","code":"\nclass(as.data.frame(tb))\n#> [1] \"data.frame\""},{"path":"tibbles.html","id":"exercises-20","chapter":"10 Tibbles","heading":"10.5 Exercises","text":"can tell object tibble? (Hint: try printing mtcars,\nregular data frame).can tell object tibble? (Hint: try printing mtcars,\nregular data frame).Compare contrast following operations data.frame \nequivalent tibble. different? might default data frame\nbehaviours cause frustration?\n\ndf <- data.frame(abc = 1, xyz = \"\")\ndf$x\ndf[, \"xyz\"]\ndf[, c(\"abc\", \"xyz\")]Compare contrast following operations data.frame \nequivalent tibble. different? might default data frame\nbehaviours cause frustration?name variable stored object, e.g. var <- \"mpg\",\ncan extract reference variable tibble?name variable stored object, e.g. var <- \"mpg\",\ncan extract reference variable tibble?Practice referring non-syntactic names following data frame :\nExtracting variable called 1.\nPlotting scatterplot 1 vs 2.\nCreating new column called 3 2 divided 1.\nRenaming columns one, two three.\n\nannoying <- tibble(\n  `1` = 1:10,\n  `2` = `1` * 2 + rnorm(length(`1`))\n)Practice referring non-syntactic names following data frame :Extracting variable called 1.Extracting variable called 1.Plotting scatterplot 1 vs 2.Plotting scatterplot 1 vs 2.Creating new column called 3 2 divided 1.Creating new column called 3 2 divided 1.Renaming columns one, two three.Renaming columns one, two three.tibble::enframe() ? might use ?tibble::enframe() ? might use ?option controls many additional column names printed\nfooter tibble?option controls many additional column names printed\nfooter tibble?","code":"\ndf <- data.frame(abc = 1, xyz = \"a\")\ndf$x\ndf[, \"xyz\"]\ndf[, c(\"abc\", \"xyz\")]\nannoying <- tibble(\n  `1` = 1:10,\n  `2` = `1` * 2 + rnorm(length(`1`))\n)"},{"path":"data-import.html","id":"data-import","chapter":"11 Data import","heading":"11 Data import","text":"","code":""},{"path":"data-import.html","id":"introduction-5","chapter":"11 Data import","heading":"11.1 Introduction","text":"Working data provided R packages great way learn tools data science, point want stop learning start working data. chapter, ’ll learn read plain-text rectangular files R. , ’ll scratch surface data import, many principles translate forms data. ’ll finish pointers packages useful types data.","code":""},{"path":"data-import.html","id":"prerequisites-5","chapter":"11 Data import","heading":"11.1.1 Prerequisites","text":"chapter, ’ll learn load flat files R readr package, part core tidyverse.","code":"\nlibrary(tidyverse)"},{"path":"data-import.html","id":"getting-started","chapter":"11 Data import","heading":"11.2 Getting started","text":"readr’s functions concerned turning flat files data frames:read_csv() reads comma delimited files, read_csv2() reads semicolon\nseparated files (common countries , used decimal place),\nread_tsv() reads tab delimited files, read_delim() reads files\ndelimiter.read_csv() reads comma delimited files, read_csv2() reads semicolon\nseparated files (common countries , used decimal place),\nread_tsv() reads tab delimited files, read_delim() reads files\ndelimiter.read_fwf() reads fixed width files. can specify fields either \nwidths fwf_widths() position fwf_positions().\nread_table() reads common variation fixed width files columns\nseparated white space.read_fwf() reads fixed width files. can specify fields either \nwidths fwf_widths() position fwf_positions().\nread_table() reads common variation fixed width files columns\nseparated white space.read_log() reads Apache style log files. (also check \nwebreadr built top\nread_log() provides many helpful tools.)read_log() reads Apache style log files. (also check \nwebreadr built top\nread_log() provides many helpful tools.)functions similar syntax: ’ve mastered one, can use others ease. rest chapter ’ll focus read_csv(). csv files one common forms data storage, understand read_csv(), can easily apply knowledge functions readr.first argument read_csv() important: ’s path file read.run read_csv() prints column specification gives name type column. ’s important part readr, ’ll come back parsing file.can also supply inline csv file. useful experimenting readr creating reproducible examples share others:cases read_csv() uses first line data column names, common convention. two cases might want tweak behaviour:Sometimes lines metadata top file. can\nuse skip = n skip first n lines; use comment = \"#\" drop\nlines start (e.g.) #.\n\nread_csv(\"first line metadata\n  second line metadata\n  x,y,z\n  1,2,3\", skip = 2)\n#> # tibble: 1 x 3\n#>       x     y     z\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\n\nread_csv(\"# comment want skip\n  x,y,z\n  1,2,3\", comment = \"#\")\n#> # tibble: 1 x 3\n#>       x     y     z\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3Sometimes lines metadata top file. can\nuse skip = n skip first n lines; use comment = \"#\" drop\nlines start (e.g.) #.data might column names. can use col_names = FALSE \ntell read_csv() treat first row headings, instead\nlabel sequentially X1 Xn:\n\nread_csv(\"1,2,3\\n4,5,6\", col_names = FALSE)\n#> # tibble: 2 x 3\n#>      X1    X2    X3\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\n#> 2     4     5     6\n(\"\\n\" convenient shortcut adding new line. ’ll learn \ntypes string escape string basics.)\nAlternatively can pass col_names character vector \nused column names:\n\nread_csv(\"1,2,3\\n4,5,6\", col_names = c(\"x\", \"y\", \"z\"))\n#> # tibble: 2 x 3\n#>       x     y     z\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\n#> 2     4     5     6The data might column names. can use col_names = FALSE \ntell read_csv() treat first row headings, instead\nlabel sequentially X1 Xn:(\"\\n\" convenient shortcut adding new line. ’ll learn \ntypes string escape string basics.)Alternatively can pass col_names character vector \nused column names:Another option commonly needs tweaking na: specifies value (values) used represent missing values file:need know read ~75% CSV files ’ll encounter practice. can also easily adapt ’ve learned read tab separated files read_tsv() fixed width files read_fwf(). read challenging files, ’ll need learn readr parses column, turning R vectors.","code":"\nheights <- read_csv(\"data/heights.csv\")\n#> \n#> ── Column specification ────────────────────────────────────────────────────────\n#> cols(\n#>   earn = col_double(),\n#>   height = col_double(),\n#>   sex = col_character(),\n#>   ed = col_double(),\n#>   age = col_double(),\n#>   race = col_character()\n#> )\nread_csv(\"a,b,c\n1,2,3\n4,5,6\")\n#> # A tibble: 2 x 3\n#>       a     b     c\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\n#> 2     4     5     6\nread_csv(\"The first line of metadata\n  The second line of metadata\n  x,y,z\n  1,2,3\", skip = 2)\n#> # A tibble: 1 x 3\n#>       x     y     z\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\n\nread_csv(\"# A comment I want to skip\n  x,y,z\n  1,2,3\", comment = \"#\")\n#> # A tibble: 1 x 3\n#>       x     y     z\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\nread_csv(\"1,2,3\\n4,5,6\", col_names = FALSE)\n#> # A tibble: 2 x 3\n#>      X1    X2    X3\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\n#> 2     4     5     6\nread_csv(\"1,2,3\\n4,5,6\", col_names = c(\"x\", \"y\", \"z\"))\n#> # A tibble: 2 x 3\n#>       x     y     z\n#>   <dbl> <dbl> <dbl>\n#> 1     1     2     3\n#> 2     4     5     6\nread_csv(\"a,b,c\\n1,2,.\", na = \".\")\n#> # A tibble: 1 x 3\n#>       a     b c    \n#>   <dbl> <dbl> <lgl>\n#> 1     1     2 NA"},{"path":"data-import.html","id":"compared-to-base-r","chapter":"11 Data import","heading":"11.2.1 Compared to base R","text":"’ve used R , might wonder ’re using read.csv(). good reasons favour readr functions base equivalents:typically much faster (~10x) base equivalents.\nLong running jobs progress bar, can see ’s happening.\n’re looking raw speed, try data.table::fread(). doesn’t fit\nquite well tidyverse, can quite bit faster.typically much faster (~10x) base equivalents.\nLong running jobs progress bar, can see ’s happening.\n’re looking raw speed, try data.table::fread(). doesn’t fit\nquite well tidyverse, can quite bit faster.produce tibbles, don’t convert character vectors factors,\nuse row names, munge column names. common sources \nfrustration base R functions.produce tibbles, don’t convert character vectors factors,\nuse row names, munge column names. common sources \nfrustration base R functions.reproducible. Base R functions inherit behaviour \noperating system environment variables, import code works\ncomputer might work someone else’s.reproducible. Base R functions inherit behaviour \noperating system environment variables, import code works\ncomputer might work someone else’s.","code":""},{"path":"data-import.html","id":"exercises-21","chapter":"11 Data import","heading":"11.2.2 Exercises","text":"function use read file fields separated \n“|”?function use read file fields separated \n“|”?Apart file, skip, comment, arguments \nread_csv() read_tsv() common?Apart file, skip, comment, arguments \nread_csv() read_tsv() common?important arguments read_fwf()?important arguments read_fwf()?Sometimes strings CSV file contain commas. prevent \ncausing problems need surrounded quoting character, like\n\" '. default, read_csv() assumes quoting\ncharacter \". argument read_csv() need specify\nread following text data frame?\n\n\"x,y\\n1,',b'\"Sometimes strings CSV file contain commas. prevent \ncausing problems need surrounded quoting character, like\n\" '. default, read_csv() assumes quoting\ncharacter \". argument read_csv() need specify\nread following text data frame?Identify wrong following inline CSV files.\nhappens run code?\n\nread_csv(\",b\\n1,2,3\\n4,5,6\")\nread_csv(\",b,c\\n1,2\\n1,2,3,4\")\nread_csv(\",b\\n\\\"1\")\nread_csv(\",b\\n1,2\\na,b\")\nread_csv(\";b\\n1;3\")Identify wrong following inline CSV files.\nhappens run code?","code":"\n\"x,y\\n1,'a,b'\"\nread_csv(\"a,b\\n1,2,3\\n4,5,6\")\nread_csv(\"a,b,c\\n1,2\\n1,2,3,4\")\nread_csv(\"a,b\\n\\\"1\")\nread_csv(\"a,b\\n1,2\\na,b\")\nread_csv(\"a;b\\n1;3\")"},{"path":"data-import.html","id":"parsing-a-vector","chapter":"11 Data import","heading":"11.3 Parsing a vector","text":"get details readr reads files disk, need take little detour talk parse_*() functions. functions take character vector return specialised vector like logical, integer, date:functions useful right, also important building block readr. ’ve learned individual parsers work section, ’ll circle back see fit together parse complete file next section.Like functions tidyverse, parse_*() functions uniform: first argument character vector parse, na argument specifies strings treated missing:parsing fails, ’ll get warning:failures missing output:many parsing failures, ’ll need use problems() get complete set. returns tibble, can manipulate dplyr.Using parsers mostly matter understanding ’s available deal different types input. eight particularly important parsers:parse_logical() parse_integer() parse logicals integers\nrespectively. ’s basically nothing can go wrong \nparsers won’t describe .parse_logical() parse_integer() parse logicals integers\nrespectively. ’s basically nothing can go wrong \nparsers won’t describe .parse_double() strict numeric parser, parse_number()\nflexible numeric parser. complicated might\nexpect different parts world write numbers different\nways.parse_double() strict numeric parser, parse_number()\nflexible numeric parser. complicated might\nexpect different parts world write numbers different\nways.parse_character() seems simple shouldn’t necessary. \none complication makes quite important: character encodings.parse_character() seems simple shouldn’t necessary. \none complication makes quite important: character encodings.parse_factor() create factors, data structure R uses represent\ncategorical variables fixed known values.parse_factor() create factors, data structure R uses represent\ncategorical variables fixed known values.parse_datetime(), parse_date(), parse_time() allow \nparse various date & time specifications. complicated\nmany different ways writing dates.parse_datetime(), parse_date(), parse_time() allow \nparse various date & time specifications. complicated\nmany different ways writing dates.following sections describe parsers detail.","code":"\nstr(parse_logical(c(\"TRUE\", \"FALSE\", \"NA\")))\n#>  logi [1:3] TRUE FALSE NA\nstr(parse_integer(c(\"1\", \"2\", \"3\")))\n#>  int [1:3] 1 2 3\nstr(parse_date(c(\"2010-01-01\", \"1979-10-14\")))\n#>  Date[1:2], format: \"2010-01-01\" \"1979-10-14\"\nparse_integer(c(\"1\", \"231\", \".\", \"456\"), na = \".\")\n#> [1]   1 231  NA 456\nx <- parse_integer(c(\"123\", \"345\", \"abc\", \"123.45\"))\n#> Warning: 2 parsing failures.\n#> row col               expected actual\n#>   3  -- an integer             abc   \n#>   4  -- no trailing characters 123.45\nx\n#> [1] 123 345  NA  NA\n#> attr(,\"problems\")\n#> # A tibble: 2 x 4\n#>     row   col expected               actual\n#>   <int> <int> <chr>                  <chr> \n#> 1     3    NA an integer             abc   \n#> 2     4    NA no trailing characters 123.45\nproblems(x)\n#> # A tibble: 2 x 4\n#>     row   col expected               actual\n#>   <int> <int> <chr>                  <chr> \n#> 1     3    NA an integer             abc   \n#> 2     4    NA no trailing characters 123.45"},{"path":"data-import.html","id":"numbers","chapter":"11 Data import","heading":"11.3.1 Numbers","text":"seems like straightforward parse number, three problems make tricky:People write numbers differently different parts world.\nexample, countries use . integer fractional\nparts real number, others use ,.People write numbers differently different parts world.\nexample, countries use . integer fractional\nparts real number, others use ,.Numbers often surrounded characters provide \ncontext, like “$1000” “10%”.Numbers often surrounded characters provide \ncontext, like “$1000” “10%”.Numbers often contain “grouping” characters make easier read,\nlike “1,000,000”, grouping characters vary around world.Numbers often contain “grouping” characters make easier read,\nlike “1,000,000”, grouping characters vary around world.address first problem, readr notion “locale”, object specifies parsing options differ place place. parsing numbers, important option character use decimal mark. can override default value . creating new locale setting decimal_mark argument:readr’s default locale US-centric, generally R US-centric (.e. documentation base R written American English). alternative approach try guess defaults operating system. hard well, , importantly, makes code fragile: even works computer, might fail email colleague another country.parse_number() addresses second problem: ignores non-numeric characters number. particularly useful currencies percentages, also works extract numbers embedded text.final problem addressed combination parse_number() locale parse_number() ignore “grouping mark”:","code":"\nparse_double(\"1.23\")\n#> [1] 1.23\nparse_double(\"1,23\", locale = locale(decimal_mark = \",\"))\n#> [1] 1.23\nparse_number(\"$100\")\n#> [1] 100\nparse_number(\"20%\")\n#> [1] 20\nparse_number(\"It cost $123.45\")\n#> [1] 123.45\n# Used in America\nparse_number(\"$123,456,789\")\n#> [1] 123456789\n\n# Used in many parts of Europe\nparse_number(\"123.456.789\", locale = locale(grouping_mark = \".\"))\n#> [1] 123456789\n\n# Used in Switzerland\nparse_number(\"123'456'789\", locale = locale(grouping_mark = \"'\"))\n#> [1] 123456789"},{"path":"data-import.html","id":"readr-strings","chapter":"11 Data import","heading":"11.3.2 Strings","text":"seems like parse_character() really simple — just return input. Unfortunately life isn’t simple, multiple ways represent string. understand ’s going , need dive details computers represent strings. R, can get underlying representation string using charToRaw():hexadecimal number represents byte information: 48 H, 61 , . mapping hexadecimal number character called encoding, case encoding called ASCII. ASCII great job representing English characters, ’s American Standard Code Information Interchange.Things get complicated languages English. early days computing many competing standards encoding non-English characters, correctly interpret string needed know values encoding. example, two common encodings Latin1 (aka ISO-8859-1, used Western European languages) Latin2 (aka ISO-8859-2, used Eastern European languages). Latin1, byte b1 “±”, Latin2, ’s “ą”! Fortunately, today one standard supported almost everywhere: UTF-8. UTF-8 can encode just every character used humans today, well many extra symbols (like emoji!).readr uses UTF-8 everywhere: assumes data UTF-8 encoded read , always uses writing. good default, fail data produced older systems don’t understand UTF-8. happens , strings look weird print . Sometimes just one two characters might messed ; times ’ll get complete gibberish. example:fix problem need specify encoding parse_character():find correct encoding? ’re lucky, ’ll included somewhere data documentation. Unfortunately, ’s rarely case, readr provides guess_encoding() help figure . ’s foolproof, works better lots text (unlike ), ’s reasonable place start. Expect try different encodings find right one.first argument guess_encoding() can either path file, , case, raw vector (useful strings already R).Encodings rich complex topic, ’ve scratched surface . ’d like learn ’d recommend reading detailed explanation http://kunststube.net/encoding/.","code":"\ncharToRaw(\"Hadley\")\n#> [1] 48 61 64 6c 65 79\nx1 <- \"El Ni\\xf1o was particularly bad this year\"\nx2 <- \"\\x82\\xb1\\x82\\xf1\\x82\\xc9\\x82\\xbf\\x82\\xcd\"\n\nx1\n#> [1] \"El Ni\\xf1o was particularly bad this year\"\nx2\n#> [1] \"\\x82\\xb1\\x82\\xf1\\x82ɂ\\xbf\\x82\\xcd\"\nparse_character(x1, locale = locale(encoding = \"Latin1\"))\n#> [1] \"El Niño was particularly bad this year\"\nparse_character(x2, locale = locale(encoding = \"Shift-JIS\"))\n#> [1] \"こんにちは\"\nguess_encoding(charToRaw(x1))\n#> # A tibble: 2 x 2\n#>   encoding   confidence\n#>   <chr>           <dbl>\n#> 1 ISO-8859-1       0.46\n#> 2 ISO-8859-9       0.23\nguess_encoding(charToRaw(x2))\n#> # A tibble: 1 x 2\n#>   encoding confidence\n#>   <chr>         <dbl>\n#> 1 KOI8-R         0.42"},{"path":"data-import.html","id":"readr-factors","chapter":"11 Data import","heading":"11.3.3 Factors","text":"R uses factors represent categorical variables known set possible values. Give parse_factor() vector known levels generate warning whenever unexpected value present:many problematic entries, ’s often easier leave character vectors use tools ’ll learn strings factors clean .","code":"\nfruit <- c(\"apple\", \"banana\")\nparse_factor(c(\"apple\", \"banana\", \"bananana\"), levels = fruit)\n#> Warning: 1 parsing failure.\n#> row col           expected   actual\n#>   3  -- value in level set bananana\n#> [1] apple  banana <NA>  \n#> attr(,\"problems\")\n#> # A tibble: 1 x 4\n#>     row   col expected           actual  \n#>   <int> <int> <chr>              <chr>   \n#> 1     3    NA value in level set bananana\n#> Levels: apple banana"},{"path":"data-import.html","id":"readr-datetimes","chapter":"11 Data import","heading":"11.3.4 Dates, date-times, and times","text":"pick three parsers depending whether want date (number days since 1970-01-01), date-time (number seconds since midnight 1970-01-01), time (number seconds since midnight). called without additional arguments:parse_datetime() expects ISO8601 date-time. ISO8601 \ninternational standard components date \norganised biggest smallest: year, month, day, hour, minute,\nsecond.\n\nparse_datetime(\"2010-10-01T2010\")\n#> [1] \"2010-10-01 20:10:00 UTC\"\n# time omitted, set midnight\nparse_datetime(\"20101010\")\n#> [1] \"2010-10-10 UTC\"\nimportant date/time standard, work \ndates times frequently, recommend reading\nhttps://en.wikipedia.org/wiki/ISO_8601parse_datetime() expects ISO8601 date-time. ISO8601 \ninternational standard components date \norganised biggest smallest: year, month, day, hour, minute,\nsecond.important date/time standard, work \ndates times frequently, recommend reading\nhttps://en.wikipedia.org/wiki/ISO_8601parse_date() expects four digit year, - /, month, -\n/, day:\n\nparse_date(\"2010-10-01\")\n#> [1] \"2010-10-01\"parse_date() expects four digit year, - /, month, -\n/, day:parse_time() expects hour, :, minutes, optionally : seconds,\noptional /pm specifier:\n\nlibrary(hms)\nparse_time(\"01:10 \")\n#> 01:10:00\nparse_time(\"20:10:01\")\n#> 20:10:01\nBase R doesn’t great built class time data, use\none provided hms package.parse_time() expects hour, :, minutes, optionally : seconds,\noptional /pm specifier:Base R doesn’t great built class time data, use\none provided hms package.defaults don’t work data can supply date-time format, built following pieces:Year%Y (4 digits).\n%y (2 digits); 00-69 -> 2000-2069, 70-99 -> 1970-1999.\nMonth%m (2 digits).\n%b (abbreviated name, like “Jan”).\n%B (full name, “January”).\nDay%d (2 digits).\n%e (optional leading space).\nTime%H 0-23 hour.\n%0-12, must used %p.\n%p /PM indicator.\n%M minutes.\n%S integer seconds.\n%OS real seconds.\n%Z Time zone (name, e.g. America/Chicago). Beware abbreviations:\n’re American, note “EST” Canadian time zone \ndaylight savings time. Eastern Standard Time! ’ll\ncome back time zones.\n%z (offset UTC, e.g. +0800).\nNon-digits%. skips one non-digit character.\n%* skips number non-digits.\nbest way figure correct format create examples character vector, test one parsing functions. example:’re using %b %B non-English month names, ’ll need set lang argument locale(). See list built-languages date_names_langs(), language already included, create date_names().","code":"\nparse_datetime(\"2010-10-01T2010\")\n#> [1] \"2010-10-01 20:10:00 UTC\"\n# If time is omitted, it will be set to midnight\nparse_datetime(\"20101010\")\n#> [1] \"2010-10-10 UTC\"\nparse_date(\"2010-10-01\")\n#> [1] \"2010-10-01\"\nlibrary(hms)\nparse_time(\"01:10 am\")\n#> 01:10:00\nparse_time(\"20:10:01\")\n#> 20:10:01\nparse_date(\"01/02/15\", \"%m/%d/%y\")\n#> [1] \"2015-01-02\"\nparse_date(\"01/02/15\", \"%d/%m/%y\")\n#> [1] \"2015-02-01\"\nparse_date(\"01/02/15\", \"%y/%m/%d\")\n#> [1] \"2001-02-15\"\nparse_date(\"1 janvier 2015\", \"%d %B %Y\", locale = locale(\"fr\"))\n#> [1] \"2015-01-01\""},{"path":"data-import.html","id":"exercises-22","chapter":"11 Data import","heading":"11.3.5 Exercises","text":"important arguments locale()?important arguments locale()?happens try set decimal_mark grouping_mark\ncharacter? happens default value \ngrouping_mark set decimal_mark “,”? happens\ndefault value decimal_mark set grouping_mark\n“.”?happens try set decimal_mark grouping_mark\ncharacter? happens default value \ngrouping_mark set decimal_mark “,”? happens\ndefault value decimal_mark set grouping_mark\n“.”?didn’t discuss date_format time_format options \nlocale(). ? Construct example shows \nmight useful.didn’t discuss date_format time_format options \nlocale(). ? Construct example shows \nmight useful.live outside US, create new locale object encapsulates\nsettings types file read commonly.live outside US, create new locale object encapsulates\nsettings types file read commonly.’s difference read_csv() read_csv2()?’s difference read_csv() read_csv2()?common encodings used Europe? \ncommon encodings used Asia? googling find .common encodings used Europe? \ncommon encodings used Asia? googling find .Generate correct format string parse following\ndates times:\n\nd1 <- \"January 1, 2010\"\nd2 <- \"2015-Mar-07\"\nd3 <- \"06-Jun-2017\"\nd4 <- c(\"August 19 (2015)\", \"July 1 (2015)\")\nd5 <- \"12/30/14\" # Dec 30, 2014\nt1 <- \"1705\"\nt2 <- \"11:15:10.12 PM\"Generate correct format string parse following\ndates times:","code":"\nd1 <- \"January 1, 2010\"\nd2 <- \"2015-Mar-07\"\nd3 <- \"06-Jun-2017\"\nd4 <- c(\"August 19 (2015)\", \"July 1 (2015)\")\nd5 <- \"12/30/14\" # Dec 30, 2014\nt1 <- \"1705\"\nt2 <- \"11:15:10.12 PM\""},{"path":"data-import.html","id":"parsing-a-file","chapter":"11 Data import","heading":"11.4 Parsing a file","text":"Now ’ve learned parse individual vector, ’s time return beginning explore readr parses file. two new things ’ll learn section:readr automatically guesses type column.override default specification.","code":""},{"path":"data-import.html","id":"strategy","chapter":"11 Data import","heading":"11.4.1 Strategy","text":"readr uses heuristic figure type column: reads first 1000 rows uses (moderately conservative) heuristics figure type column. can emulate process character vector using guess_parser(), returns readr’s best guess, parse_guess() uses guess parse column:heuristic tries following types, stopping finds match:logical: contains “F”, “T”, “FALSE”, “TRUE”.integer: contains numeric characters (-).double: contains valid doubles (including numbers like 4.5e-5).number: contains valid doubles grouping mark inside.time: matches default time_format.date: matches default date_format.date-time: ISO8601 date.none rules apply, column stay vector strings.","code":"\nguess_parser(\"2010-10-01\")\n#> [1] \"date\"\nguess_parser(\"15:01\")\n#> [1] \"time\"\nguess_parser(c(\"TRUE\", \"FALSE\"))\n#> [1] \"logical\"\nguess_parser(c(\"1\", \"5\", \"9\"))\n#> [1] \"double\"\nguess_parser(c(\"12,352,561\"))\n#> [1] \"number\"\n\nstr(parse_guess(\"2010-10-10\"))\n#>  Date[1:1], format: \"2010-10-10\""},{"path":"data-import.html","id":"problems","chapter":"11 Data import","heading":"11.4.2 Problems","text":"defaults don’t always work larger files. two basic problems:first thousand rows might special case, readr guesses\ntype sufficiently general. example, might \ncolumn doubles contains integers first 1000 rows.first thousand rows might special case, readr guesses\ntype sufficiently general. example, might \ncolumn doubles contains integers first 1000 rows.column might contain lot missing values. first 1000\nrows contain NAs, readr guess ’s logical\nvector, whereas probably want parse something \nspecific.column might contain lot missing values. first 1000\nrows contain NAs, readr guess ’s logical\nvector, whereas probably want parse something \nspecific.readr contains challenging CSV illustrates problems:(Note use readr_example() finds path one files included package)two printed outputs: column specification generated looking first 1000 rows, first five parsing failures. ’s always good idea explicitly pull problems(), can explore depth:good strategy work column column problems remaining. can see lot parsing problems y column. look last rows, ’ll see ’re dates stored character vector:suggests need use date parser instead. fix call, start copying pasting column specification original call:can fix type y column specifying y date column:Every parse_xyz() function corresponding col_xyz() function. use parse_xyz() data character vector R already; use col_xyz() want tell readr load data.highly recommend always supplying col_types, building print-provided readr. ensures consistent reproducible data import script. rely default guesses data changes, readr continue read . want really strict, use stop_for_problems(): throw error stop script parsing problems.","code":"\nchallenge <- read_csv(readr_example(\"challenge.csv\"))\n#> \n#> ── Column specification ────────────────────────────────────────────────────────\n#> cols(\n#>   x = col_double(),\n#>   y = col_logical()\n#> )\n#> Warning: 1000 parsing failures.\n#>  row col           expected     actual                                                                      file\n#> 1001   y 1/0/T/F/TRUE/FALSE 2015-01-16 '/home/mxj/R/x86_64-pc-linux-gnu-library/4.0/readr/extdata/challenge.csv'\n#> 1002   y 1/0/T/F/TRUE/FALSE 2018-05-18 '/home/mxj/R/x86_64-pc-linux-gnu-library/4.0/readr/extdata/challenge.csv'\n#> 1003   y 1/0/T/F/TRUE/FALSE 2015-09-05 '/home/mxj/R/x86_64-pc-linux-gnu-library/4.0/readr/extdata/challenge.csv'\n#> 1004   y 1/0/T/F/TRUE/FALSE 2012-11-28 '/home/mxj/R/x86_64-pc-linux-gnu-library/4.0/readr/extdata/challenge.csv'\n#> 1005   y 1/0/T/F/TRUE/FALSE 2020-01-13 '/home/mxj/R/x86_64-pc-linux-gnu-library/4.0/readr/extdata/challenge.csv'\n#> .... ... .................. .......... .........................................................................\n#> See problems(...) for more details.\nproblems(challenge)\n#> # A tibble: 1,000 x 5\n#>     row col   expected       actual   file                                      \n#>   <int> <chr> <chr>          <chr>    <chr>                                     \n#> 1  1001 y     1/0/T/F/TRUE/… 2015-01… '/home/mxj/R/x86_64-pc-linux-gnu-library/…\n#> 2  1002 y     1/0/T/F/TRUE/… 2018-05… '/home/mxj/R/x86_64-pc-linux-gnu-library/…\n#> 3  1003 y     1/0/T/F/TRUE/… 2015-09… '/home/mxj/R/x86_64-pc-linux-gnu-library/…\n#> 4  1004 y     1/0/T/F/TRUE/… 2012-11… '/home/mxj/R/x86_64-pc-linux-gnu-library/…\n#> 5  1005 y     1/0/T/F/TRUE/… 2020-01… '/home/mxj/R/x86_64-pc-linux-gnu-library/…\n#> 6  1006 y     1/0/T/F/TRUE/… 2016-04… '/home/mxj/R/x86_64-pc-linux-gnu-library/…\n#> # … with 994 more rows\ntail(challenge)\n#> # A tibble: 6 x 2\n#>       x y    \n#>   <dbl> <lgl>\n#> 1 0.805 NA   \n#> 2 0.164 NA   \n#> 3 0.472 NA   \n#> 4 0.718 NA   \n#> 5 0.270 NA   \n#> 6 0.608 NA\nchallenge <- read_csv(\n  readr_example(\"challenge.csv\"), \n  col_types = cols(\n    x = col_double(),\n    y = col_logical()\n  )\n)\nchallenge <- read_csv(\n  readr_example(\"challenge.csv\"), \n  col_types = cols(\n    x = col_double(),\n    y = col_date()\n  )\n)\ntail(challenge)\n#> # A tibble: 6 x 2\n#>       x y         \n#>   <dbl> <date>    \n#> 1 0.805 2019-11-21\n#> 2 0.164 2018-03-29\n#> 3 0.472 2014-08-04\n#> 4 0.718 2015-08-16\n#> 5 0.270 2020-02-04\n#> 6 0.608 2019-01-06"},{"path":"data-import.html","id":"other-strategies","chapter":"11 Data import","heading":"11.4.3 Other strategies","text":"general strategies help parse files:previous example, just got unlucky: look just\none row default, can correctly parse one shot:\n\nchallenge2 <- read_csv(readr_example(\"challenge.csv\"), guess_max = 1001)\n#> \n#> ── Column specification ────────────────────────────────────────────────────────\n#> cols(\n#>   x = col_double(),\n#>   y = col_date(format = \"\")\n#> )\nchallenge2\n#> # tibble: 2,000 x 2\n#>       x y         \n#>   <dbl> <date>    \n#> 1   404 NA        \n#> 2  4172 NA        \n#> 3  3004 NA        \n#> 4   787 NA        \n#> 5    37 NA        \n#> 6  2332 NA        \n#> # … 1,994 rowsIn previous example, just got unlucky: look just\none row default, can correctly parse one shot:Sometimes ’s easier diagnose problems just read \ncolumns character vectors:\n\nchallenge2 <- read_csv(readr_example(\"challenge.csv\"), \n  col_types = cols(.default = col_character())\n)\nparticularly useful conjunction type_convert(),\napplies parsing heuristics character columns data\nframe.\n\ndf <- tribble(\n  ~x,  ~y,\n  \"1\", \"1.21\",\n  \"2\", \"2.32\",\n  \"3\", \"4.56\"\n)\ndf\n#> # tibble: 3 x 2\n#>   x     y    \n#>   <chr> <chr>\n#> 1 1     1.21 \n#> 2 2     2.32 \n#> 3 3     4.56\n\n# Note column types\ntype_convert(df)\n#> \n#> ── Column specification ────────────────────────────────────────────────────────\n#> cols(\n#>   x = col_double(),\n#>   y = col_double()\n#> )\n#> # tibble: 3 x 2\n#>       x     y\n#>   <dbl> <dbl>\n#> 1     1  1.21\n#> 2     2  2.32\n#> 3     3  4.56Sometimes ’s easier diagnose problems just read \ncolumns character vectors:particularly useful conjunction type_convert(),\napplies parsing heuristics character columns data\nframe.’re reading large file, might want set n_max \nsmallish number like 10,000 100,000. accelerate \niterations eliminate common problems.’re reading large file, might want set n_max \nsmallish number like 10,000 100,000. accelerate \niterations eliminate common problems.’re major parsing problems, sometimes ’s easier\njust read character vector lines read_lines(),\neven character vector length 1 read_file(). \ncan use string parsing skills ’ll learn later parse\nexotic formats.’re major parsing problems, sometimes ’s easier\njust read character vector lines read_lines(),\neven character vector length 1 read_file(). \ncan use string parsing skills ’ll learn later parse\nexotic formats.","code":"\nchallenge2 <- read_csv(readr_example(\"challenge.csv\"), guess_max = 1001)\n#> \n#> ── Column specification ────────────────────────────────────────────────────────\n#> cols(\n#>   x = col_double(),\n#>   y = col_date(format = \"\")\n#> )\nchallenge2\n#> # A tibble: 2,000 x 2\n#>       x y         \n#>   <dbl> <date>    \n#> 1   404 NA        \n#> 2  4172 NA        \n#> 3  3004 NA        \n#> 4   787 NA        \n#> 5    37 NA        \n#> 6  2332 NA        \n#> # … with 1,994 more rows\nchallenge2 <- read_csv(readr_example(\"challenge.csv\"), \n  col_types = cols(.default = col_character())\n)\ndf <- tribble(\n  ~x,  ~y,\n  \"1\", \"1.21\",\n  \"2\", \"2.32\",\n  \"3\", \"4.56\"\n)\ndf\n#> # A tibble: 3 x 2\n#>   x     y    \n#>   <chr> <chr>\n#> 1 1     1.21 \n#> 2 2     2.32 \n#> 3 3     4.56\n\n# Note the column types\ntype_convert(df)\n#> \n#> ── Column specification ────────────────────────────────────────────────────────\n#> cols(\n#>   x = col_double(),\n#>   y = col_double()\n#> )\n#> # A tibble: 3 x 2\n#>       x     y\n#>   <dbl> <dbl>\n#> 1     1  1.21\n#> 2     2  2.32\n#> 3     3  4.56"},{"path":"data-import.html","id":"writing-to-a-file","chapter":"11 Data import","heading":"11.5 Writing to a file","text":"readr also comes two useful functions writing data back disk: write_csv() write_tsv(). functions increase chances output file read back correctly :Always encoding strings UTF-8.Always encoding strings UTF-8.Saving dates date-times ISO8601 format easily\nparsed elsewhere.Saving dates date-times ISO8601 format easily\nparsed elsewhere.want export csv file Excel, use write_excel_csv() — writes special character (“byte order mark”) start file tells Excel ’re using UTF-8 encoding.important arguments x (data frame save), path (location save ). can also specify missing values written na, want append existing file.Note type information lost save csv:makes CSVs little unreliable caching interim results—need recreate column specification every time load . two alternatives:write_rds() read_rds() uniform wrappers around base\nfunctions readRDS() saveRDS(). store data R’s custom\nbinary format called RDS:\n\nwrite_rds(challenge, \"challenge.rds\")\nread_rds(\"challenge.rds\")\n#> # tibble: 2,000 x 2\n#>       x y         \n#>   <dbl> <date>    \n#> 1   404 NA        \n#> 2  4172 NA        \n#> 3  3004 NA        \n#> 4   787 NA        \n#> 5    37 NA        \n#> 6  2332 NA        \n#> # … 1,994 rowswrite_rds() read_rds() uniform wrappers around base\nfunctions readRDS() saveRDS(). store data R’s custom\nbinary format called RDS:feather package implements fast binary file format can\nshared across programming languages:\n\nlibrary(feather)\nwrite_feather(challenge, \"challenge.feather\")\nread_feather(\"challenge.feather\")\n#> # tibble: 2,000 x 2\n#>       x      y\n#>   <dbl> <date>\n#> 1   404   <NA>\n#> 2  4172   <NA>\n#> 3  3004   <NA>\n#> 4   787   <NA>\n#> 5    37   <NA>\n#> 6  2332   <NA>\n#> # ... 1,994 rowsThe feather package implements fast binary file format can\nshared across programming languages:Feather tends faster RDS usable outside R. RDS supports list-columns (’ll learn many models); feather currently .","code":"\nwrite_csv(challenge, \"challenge.csv\")\nchallenge\n#> # A tibble: 2,000 x 2\n#>       x y         \n#>   <dbl> <date>    \n#> 1   404 NA        \n#> 2  4172 NA        \n#> 3  3004 NA        \n#> 4   787 NA        \n#> 5    37 NA        \n#> 6  2332 NA        \n#> # … with 1,994 more rows\nwrite_csv(challenge, \"challenge-2.csv\")\nread_csv(\"challenge-2.csv\")\n#> \n#> ── Column specification ────────────────────────────────────────────────────────\n#> cols(\n#>   x = col_double(),\n#>   y = col_logical()\n#> )\n#> # A tibble: 2,000 x 2\n#>       x y    \n#>   <dbl> <lgl>\n#> 1   404 NA   \n#> 2  4172 NA   \n#> 3  3004 NA   \n#> 4   787 NA   \n#> 5    37 NA   \n#> 6  2332 NA   \n#> # … with 1,994 more rows\nwrite_rds(challenge, \"challenge.rds\")\nread_rds(\"challenge.rds\")\n#> # A tibble: 2,000 x 2\n#>       x y         \n#>   <dbl> <date>    \n#> 1   404 NA        \n#> 2  4172 NA        \n#> 3  3004 NA        \n#> 4   787 NA        \n#> 5    37 NA        \n#> 6  2332 NA        \n#> # … with 1,994 more rows\nlibrary(feather)\nwrite_feather(challenge, \"challenge.feather\")\nread_feather(\"challenge.feather\")\n#> # A tibble: 2,000 x 2\n#>       x      y\n#>   <dbl> <date>\n#> 1   404   <NA>\n#> 2  4172   <NA>\n#> 3  3004   <NA>\n#> 4   787   <NA>\n#> 5    37   <NA>\n#> 6  2332   <NA>\n#> # ... with 1,994 more rows"},{"path":"data-import.html","id":"other-types-of-data","chapter":"11 Data import","heading":"11.6 Other types of data","text":"get types data R, recommend starting tidyverse packages listed . ’re certainly perfect, good place start. rectangular data:haven reads SPSS, Stata, SAS files.haven reads SPSS, Stata, SAS files.readxl reads excel files (.xls .xlsx).readxl reads excel files (.xls .xlsx).DBI, along database specific backend (e.g. RMySQL,\nRSQLite, RPostgreSQL etc) allows run SQL queries \ndatabase return data frame.DBI, along database specific backend (e.g. RMySQL,\nRSQLite, RPostgreSQL etc) allows run SQL queries \ndatabase return data frame.hierarchical data: use jsonlite (Jeroen Ooms) json, xml2 XML. Jenny Bryan excellent worked examples https://jennybc.github.io/purrr-tutorial/.file types, try R data import/export manual rio package.","code":""},{"path":"tidy-data.html","id":"tidy-data","chapter":"12 Tidy data","heading":"12 Tidy data","text":"","code":""},{"path":"tidy-data.html","id":"introduction-6","chapter":"12 Tidy data","heading":"12.1 Introduction","text":"“Happy families alike; every unhappy family unhappy \nway.” –– Leo Tolstoy“Tidy datasets alike, every messy dataset messy \nway.” –– Hadley WickhamIn chapter, learn consistent way organise data R, organisation called tidy data. Getting data format requires upfront work, work pays long term. tidy data tidy tools provided packages tidyverse, spend much less time munging data one representation another, allowing spend time analytic questions hand.chapter give practical introduction tidy data accompanying tools tidyr package. ’d like learn underlying theory, might enjoy Tidy Data paper published Journal Statistical Software, http://www.jstatsoft.org/v59/i10/paper.","code":""},{"path":"tidy-data.html","id":"prerequisites-6","chapter":"12 Tidy data","heading":"12.1.1 Prerequisites","text":"chapter ’ll focus tidyr, package provides bunch tools help tidy messy datasets. tidyr member core tidyverse.","code":"\nlibrary(tidyverse)"},{"path":"tidy-data.html","id":"tidy-data-1","chapter":"12 Tidy data","heading":"12.2 Tidy data","text":"can represent underlying data multiple ways. example shows data organised four different ways. dataset shows values four variables country, year, population, cases, dataset organises values different way.representations underlying data, equally easy use. One dataset, tidy dataset, much easier work inside tidyverse.three interrelated rules make dataset tidy:variable must column.observation must row.value must cell.Figure 12.1 shows rules visually.\nFigure 12.1: Following three rules makes dataset tidy: variables columns, observations rows, values cells.\nthree rules interrelated ’s impossible satisfy two three. interrelationship leads even simpler set practical instructions:Put dataset tibble.Put variable column.example, table1 tidy. ’s representation column variable.ensure data tidy? two main advantages:’s general advantage picking one consistent way storing\ndata. consistent data structure, ’s easier learn \ntools work underlying uniformity.’s general advantage picking one consistent way storing\ndata. consistent data structure, ’s easier learn \ntools work underlying uniformity.’s specific advantage placing variables columns \nallows R’s vectorised nature shine. learned \nSections 5.5.1 5.6.4, \nbuilt-R functions work vectors values. makes transforming\ntidy data feel particularly natural.’s specific advantage placing variables columns \nallows R’s vectorised nature shine. learned \nSections 5.5.1 5.6.4, \nbuilt-R functions work vectors values. makes transforming\ntidy data feel particularly natural.dplyr, ggplot2, packages tidyverse designed work tidy data. couple small examples showing might work table1.","code":"\ntable1\n#> # A tibble: 6 x 4\n#>   country      year  cases population\n#>   <chr>       <int>  <int>      <int>\n#> 1 Afghanistan  1999    745   19987071\n#> 2 Afghanistan  2000   2666   20595360\n#> 3 Brazil       1999  37737  172006362\n#> 4 Brazil       2000  80488  174504898\n#> 5 China        1999 212258 1272915272\n#> 6 China        2000 213766 1280428583\ntable2\n#> # A tibble: 12 x 4\n#>   country      year type           count\n#>   <chr>       <int> <chr>          <int>\n#> 1 Afghanistan  1999 cases            745\n#> 2 Afghanistan  1999 population  19987071\n#> 3 Afghanistan  2000 cases           2666\n#> 4 Afghanistan  2000 population  20595360\n#> 5 Brazil       1999 cases          37737\n#> 6 Brazil       1999 population 172006362\n#> # … with 6 more rows\ntable3\n#> # A tibble: 6 x 3\n#>   country      year rate             \n#> * <chr>       <int> <chr>            \n#> 1 Afghanistan  1999 745/19987071     \n#> 2 Afghanistan  2000 2666/20595360    \n#> 3 Brazil       1999 37737/172006362  \n#> 4 Brazil       2000 80488/174504898  \n#> 5 China        1999 212258/1272915272\n#> 6 China        2000 213766/1280428583\n\n# Spread across two tibbles\ntable4a # cases\n#> # A tibble: 3 x 3\n#>   country     `1999` `2000`\n#> * <chr>        <int>  <int>\n#> 1 Afghanistan    745   2666\n#> 2 Brazil       37737  80488\n#> 3 China       212258 213766\ntable4b # population\n#> # A tibble: 3 x 3\n#>   country         `1999`     `2000`\n#> * <chr>            <int>      <int>\n#> 1 Afghanistan   19987071   20595360\n#> 2 Brazil       172006362  174504898\n#> 3 China       1272915272 1280428583\n# Compute rate per 10,000\ntable1 %>%\n  mutate(rate = cases / population * 10000)\n#> # A tibble: 6 x 5\n#>   country      year  cases population  rate\n#>   <chr>       <int>  <int>      <int> <dbl>\n#> 1 Afghanistan  1999    745   19987071 0.373\n#> 2 Afghanistan  2000   2666   20595360 1.29 \n#> 3 Brazil       1999  37737  172006362 2.19 \n#> 4 Brazil       2000  80488  174504898 4.61 \n#> 5 China        1999 212258 1272915272 1.67 \n#> 6 China        2000 213766 1280428583 1.67\n\n# Compute cases per year\ntable1 %>%\n  count(year, wt = cases)\n#> # A tibble: 2 x 2\n#>    year      n\n#> * <int>  <int>\n#> 1  1999 250740\n#> 2  2000 296920\n\n# Visualise changes over time\nggplot(table1, aes(year, cases)) +\n  geom_line(aes(group = country), colour = \"grey50\") +\n  geom_point(aes(colour = country, shape = country)) +\n  scale_x_continuous(breaks = c(1999, 2000))"},{"path":"tidy-data.html","id":"exercises-23","chapter":"12 Tidy data","heading":"12.2.1 Exercises","text":"Using prose, describe variables observations organised \nsample tables.Using prose, describe variables observations organised \nsample tables.Compute rate table2, table4a + table4b.\nneed perform four operations:\nExtract number TB cases per country per year.\nExtract matching population per country per year.\nDivide cases population, multiply 10000.\nStore back appropriate place.\nrepresentation easiest work ? hardest? ?Compute rate table2, table4a + table4b.\nneed perform four operations:Extract number TB cases per country per year.Extract matching population per country per year.Divide cases population, multiply 10000.Store back appropriate place.representation easiest work ? hardest? ?Recreate plot showing change cases time using table2\ninstead table1. need first?Recreate plot showing change cases time using table2\ninstead table1. need first?","code":""},{"path":"tidy-data.html","id":"pivoting","chapter":"12 Tidy data","heading":"12.3 Pivoting","text":"principles tidy data seem obvious might wonder ’ll ever encounter dataset isn’t tidy. Unfortunately, however, data encounter untidy. two main reasons:people aren’t familiar principles tidy data, ’s hard\nderive unless spend lot time working data.people aren’t familiar principles tidy data, ’s hard\nderive unless spend lot time working data.Data often organised facilitate use analysis. \nexample, data often organised make entry easy possible.Data often organised facilitate use analysis. \nexample, data often organised make entry easy possible.means real analyses, ’ll need tidying. first step always figure variables observations . Sometimes easy; times ’ll need consult people originally generated data.\nsecond step resolve one two common problems:One variable might spread across multiple columns.One variable might spread across multiple columns.One observation might scattered across multiple rows.One observation might scattered across multiple rows.Typically dataset suffer one problems; ’ll suffer ’re really unlucky! fix problems, ’ll need two important functions tidyr: pivot_longer() pivot_wider().","code":""},{"path":"tidy-data.html","id":"longer","chapter":"12 Tidy data","heading":"12.3.1 Longer","text":"common problem dataset column names names variables, values variable. Suppose data following format.want create following visualisation line represents country, year x-axis, cases y-axis, automatically get legend indicates line represents country.\nFigure 12.2: Number cases years country.\n’s straight-forward starting data frame country, year, cases columns row represents record country particular year.However table4a column names 1999 2000 represent values year variable, values 1999 2000 columns represent values cases variable, row represents two observations, one.tidy dataset like , need pivot offending columns new pair variables. describe operation need three parameters:set columns whose names values, variables. example,\ncolumns 1999 2000.set columns whose names values, variables. example,\ncolumns 1999 2000.name variable move column names : year.name variable move column names : year.name variable move column values : cases.name variable move column values : cases.Together parameters generate call pivot_longer():columns pivot specified dplyr::select() style notation cols argument. two columns, list individually. Note 1999 2000 non-syntactic names (don’t start letter) surround backticks. refresh memory ways select columns, see Section 5.4.year cases exist table4a put names quotes names_to values_to arguments, respectively.final result, pivoted columns dropped, get new year cases columns. Otherwise, relationships original variables preserved. Visually, shown Figure 12.3.\nFigure 12.3: Pivoting table4a “longer”, tidy form.\nstill one issue though. Take peek type year variable. expect year numeric (specifically, expect integer), however ’s showing character. values year variable came column headings table4a. can add new step pipeline using dplyr::mutate() parse variable integer readr::parse_integer(). can refer back Section 11.3 functions parsing types vectors.data longer format, can create visualisation motivated tidying exercise follows.pivot_longer() makes datasets longer increasing number rows decreasing number columns. don’t believe makes sense describe dataset “long form”. Length relative term, can say (e.g.) dataset longer dataset B.can use pivot_longer() tidy table4b similar fashion. difference variable stored cell values:combine tidied versions table4a table4b single tibble, need use dplyr::left_join(), ’ll learn Chapter 13.","code":"\ntable4a\n#> # A tibble: 3 x 3\n#>   country     `1999` `2000`\n#> * <chr>        <int>  <int>\n#> 1 Afghanistan    745   2666\n#> 2 Brazil       37737  80488\n#> 3 China       212258 213766#> # A tibble: 6 x 3\n#>   country      year  cases\n#>   <chr>       <int>  <int>\n#> 1 Afghanistan  1999    745\n#> 2 Afghanistan  2000   2666\n#> 3 Brazil       1999  37737\n#> 4 Brazil       2000  80488\n#> 5 China        1999 212258\n#> 6 China        2000 213766\ntable4a %>%\n  pivot_longer(\n    cols = c(`1999`, `2000`),\n    names_to = \"year\",\n    values_to = \"cases\"\n  )\n#> # A tibble: 6 x 3\n#>   country     year   cases\n#>   <chr>       <chr>  <int>\n#> 1 Afghanistan 1999     745\n#> 2 Afghanistan 2000    2666\n#> 3 Brazil      1999   37737\n#> 4 Brazil      2000   80488\n#> 5 China       1999  212258\n#> 6 China       2000  213766\ntable4a %>%\n  pivot_longer(\n    cols = c(`1999`, `2000`),\n    names_to = \"year\",\n    values_to = \"cases\"\n  ) %>%\n  mutate(year = parse_integer(year))\n#> # A tibble: 6 x 3\n#>   country      year  cases\n#>   <chr>       <int>  <int>\n#> 1 Afghanistan  1999    745\n#> 2 Afghanistan  2000   2666\n#> 3 Brazil       1999  37737\n#> 4 Brazil       2000  80488\n#> 5 China        1999 212258\n#> 6 China        2000 213766\ntable4a %>%\n  pivot_longer(\n    cols = c(`1999`, `2000`),\n    names_to = \"year\",\n    values_to = \"cases\",\n  ) %>%\n  mutate(year = parse_integer(year)) %>%\n  ggplot(aes(x = year, y = cases)) +\n  geom_line(aes(group = country), colour = \"grey50\") +\n  geom_point(aes(colour = country, shape = country)) +\n  scale_x_continuous(breaks = c(1999, 2000))\ntable4b %>%\n  pivot_longer(\n    cols = c(`1999`, `2000`),\n    names_to = \"year\",\n    values_to = \"population\"\n  ) %>%\n  mutate(year = parse_integer(year))\n#> # A tibble: 6 x 3\n#>   country      year population\n#>   <chr>       <int>      <int>\n#> 1 Afghanistan  1999   19987071\n#> 2 Afghanistan  2000   20595360\n#> 3 Brazil       1999  172006362\n#> 4 Brazil       2000  174504898\n#> 5 China        1999 1272915272\n#> 6 China        2000 1280428583\ntidy4a <- table4b %>%\n  pivot_longer(\n    cols = c(`1999`, `2000`),\n    names_to = \"year\",\n    values_to = \"cases\"\n  ) %>%\n  mutate(year = parse_integer(year))\ntidy4b <- table4b %>%\n  pivot_longer(\n    cols = c(`1999`, `2000`),\n    names_to = \"year\",\n    values_to = \"population\"\n  ) %>%\n  mutate(year = parse_integer(year))\nleft_join(tidy4a, tidy4b)\n#> Joining, by = c(\"country\", \"year\")\n#> # A tibble: 6 x 4\n#>   country      year      cases population\n#>   <chr>       <int>      <int>      <int>\n#> 1 Afghanistan  1999   19987071   19987071\n#> 2 Afghanistan  2000   20595360   20595360\n#> 3 Brazil       1999  172006362  172006362\n#> 4 Brazil       2000  174504898  174504898\n#> 5 China        1999 1272915272 1272915272\n#> 6 China        2000 1280428583 1280428583"},{"path":"tidy-data.html","id":"wider","chapter":"12 Tidy data","heading":"12.3.2 Wider","text":"pivot_wider() opposite pivot_longer(). use observation scattered across multiple rows. example, take table2: observation country year, observation spread across two rows.Suppose ’d like calculate rate (number cases divided population) country given year, record new column, resulting following data frame.means need data frame cases population separate columns, columns, cell hold values relevant counts. Let’s analyse representation similar way pivot_longer(). time, however, need two parameters:column take variable names : type.column take variable names : type.column take values : count.column take values : count.can use pivot_wider(), shown programmatically , visually Figure 12.4.\nFigure 12.4: Pivoting table2 “wider”, tidy form.\ndata wider format, can create data frame motivated tidying exercise follows.Earlier visualised case counts years, representation can useful visualising case rates, example.Now let’s go one step widen data record cases, population, rate 1999 2000 separate columns, following.representation rarely useful data analysis might useful basis table communication results data analysis report.achieve need add year information column headings cases, population, rate well distribute values currently three columns six columns (two columns year data ). represented Figure 12.5.\nFigure 12.5: Pivoting table2 even “wider” form. Arrows cases rate values omitted clarity.\n, ’ll take advantage fact pivot functions can operate multiple columns . first three lines following code chunk ’ve already done previous step add pipeline another pivot_wider() step values added columns come cases, population, rate column names automatically suffixed values year variable.last step achieving goal relocate columns resulting data frame columns 1999 data come 2000. can use relocate() function move 1999 columns ahead 2000 columns.might guessed names, pivot_wider() pivot_longer() complements. pivot_longer() makes wide tables narrower longer; pivot_wider() makes long tables shorter wider.","code":"\ntable2\n#> # A tibble: 12 x 4\n#>   country      year type           count\n#>   <chr>       <int> <chr>          <int>\n#> 1 Afghanistan  1999 cases            745\n#> 2 Afghanistan  1999 population  19987071\n#> 3 Afghanistan  2000 cases           2666\n#> 4 Afghanistan  2000 population  20595360\n#> 5 Brazil       1999 cases          37737\n#> 6 Brazil       1999 population 172006362\n#> # … with 6 more rows#> # A tibble: 6 x 5\n#>   country      year  cases population      rate\n#>   <chr>       <int>  <int>      <int>     <dbl>\n#> 1 Afghanistan  1999    745   19987071 0.0000373\n#> 2 Afghanistan  2000   2666   20595360 0.000129 \n#> 3 Brazil       1999  37737  172006362 0.000219 \n#> 4 Brazil       2000  80488  174504898 0.000461 \n#> 5 China        1999 212258 1272915272 0.000167 \n#> 6 China        2000 213766 1280428583 0.000167\ntable2 %>%\n  pivot_wider(names_from = type, values_from = count)\n#> # A tibble: 6 x 4\n#>   country      year  cases population\n#>   <chr>       <int>  <int>      <int>\n#> 1 Afghanistan  1999    745   19987071\n#> 2 Afghanistan  2000   2666   20595360\n#> 3 Brazil       1999  37737  172006362\n#> 4 Brazil       2000  80488  174504898\n#> 5 China        1999 212258 1272915272\n#> 6 China        2000 213766 1280428583\ntable2 %>%\n  pivot_wider(names_from = type, values_from = count) %>%\n  mutate(rate = cases / population)\n#> # A tibble: 6 x 5\n#>   country      year  cases population      rate\n#>   <chr>       <int>  <int>      <int>     <dbl>\n#> 1 Afghanistan  1999    745   19987071 0.0000373\n#> 2 Afghanistan  2000   2666   20595360 0.000129 \n#> 3 Brazil       1999  37737  172006362 0.000219 \n#> 4 Brazil       2000  80488  174504898 0.000461 \n#> 5 China        1999 212258 1272915272 0.000167 \n#> 6 China        2000 213766 1280428583 0.000167\ntable2 %>%\n  pivot_wider(names_from = type, values_from = count) %>%\n  mutate(rate = cases / population) %>%\n  ggplot(aes(x = year, y = rate)) +\n  geom_line(aes(group = country), colour = \"grey50\") +\n  geom_point(aes(colour = country, shape = country)) +\n  scale_x_continuous(breaks = c(1999, 2000))#> # A tibble: 3 x 7\n#>   country cases_1999 population_1999 rate_1999 cases_2000 population_2000\n#>   <chr>        <int>           <int>     <dbl>      <int>           <int>\n#> 1 Afghan…        745        19987071 0.0000373       2666        20595360\n#> 2 Brazil       37737       172006362 0.000219       80488       174504898\n#> 3 China       212258      1272915272 0.000167      213766      1280428583\n#> # … with 1 more variable: rate_2000 <dbl>\ntable2 %>%\n  pivot_wider(names_from = type, values_from = count) %>%\n  mutate(rate = cases / population) %>%\n  pivot_wider(\n    names_from = year,\n    values_from = c(cases, population, rate)\n  )\n#> # A tibble: 3 x 7\n#>   country cases_1999 cases_2000 population_1999 population_2000 rate_1999\n#>   <chr>        <int>      <int>           <int>           <int>     <dbl>\n#> 1 Afghan…        745       2666        19987071        20595360 0.0000373\n#> 2 Brazil       37737      80488       172006362       174504898 0.000219 \n#> 3 China       212258     213766      1272915272      1280428583 0.000167 \n#> # … with 1 more variable: rate_2000 <dbl>\ntable2 %>%\n  pivot_wider(names_from = type, values_from = count) %>%\n  mutate(rate = cases / population) %>%\n  pivot_wider(\n    names_from = year,\n    values_from = c(cases, population, rate)\n  ) %>%\n  relocate(country, contains(\"1999\"))\n#> # A tibble: 3 x 7\n#>   country cases_1999 population_1999 rate_1999 cases_2000 population_2000\n#>   <chr>        <int>           <int>     <dbl>      <int>           <int>\n#> 1 Afghan…        745        19987071 0.0000373       2666        20595360\n#> 2 Brazil       37737       172006362 0.000219       80488       174504898\n#> 3 China       212258      1272915272 0.000167      213766      1280428583\n#> # … with 1 more variable: rate_2000 <dbl>"},{"path":"tidy-data.html","id":"exercises-24","chapter":"12 Tidy data","heading":"12.3.3 Exercises","text":"pivot_longer() pivot_wider() perfectly symmetrical?\nCarefully consider following example:\n\nstocks <- tibble(\n  year   = c(2015, 2015, 2016, 2016),\n  half   = c(   1,    2,    1,    2),\n  return = c(1.88, 0.59, 0.92, 0.17)\n)\nstocks %>%\n  pivot_wider(names_from = year, values_from = return) %>%\n  pivot_longer(`2015`:`2016`, names_to = \"year\", values_to = \"return\")\n(Hint: look variable types think column names.)\npivot_longer() names_ptypes argument, e.g. \nnames_ptypes = list(year = double()). ?pivot_longer() pivot_wider() perfectly symmetrical?\nCarefully consider following example:(Hint: look variable types think column names.)pivot_longer() names_ptypes argument, e.g. \nnames_ptypes = list(year = double()). ?code fail?\n\ntable4a %>%\n  pivot_longer(c(1999, 2000), names_to = \"year\", values_to = \"cases\")\n#> Error: subset columns exist.\n#> x Locations 1999 2000 exist.\n#> ℹ 3 columns.code fail?happen widen table? ? add \nnew column uniquely identify value?\n\npeople <- tribble(\n  ~name,             ~names,  ~values,\n  #-----------------|--------|-------\n  \"Phillip Woods\",   \"age\",        45,\n  \"Phillip Woods\",   \"height\",    186,\n  \"Phillip Woods\",   \"age\",        50,\n  \"Jessica Cordero\", \"age\",        37,\n  \"Jessica Cordero\", \"height\",    156\n)happen widen table? ? add \nnew column uniquely identify value?simple tibble summarizes information whether employees \nsmall company know drive whether prefer position \nneed drive daily sales calls. Tidy table get \nformat observation employee. need make wider\nlonger? variables?\n\nemployees <- tribble(\n  ~know_drive, ~prefer, ~not_prefer,\n  \"yes\",       20,      10,\n  \"\",        NA,      12\n)simple tibble summarizes information whether employees \nsmall company know drive whether prefer position \nneed drive daily sales calls. Tidy table get \nformat observation employee. need make wider\nlonger? variables?One way summarising distribution one categorical variable based\nlevels another using dplyr::count(), e.g. following\ngives distribution drv (type drive train) level \ncyl (number cylinders) cars mpg dataset.\n\nmpg %>%\n  count(cyl, drv)\n#> # tibble: 9 x 3\n#>     cyl drv       n\n#>   <int> <chr> <int>\n#> 1     4 4        23\n#> 2     4 f        58\n#> 3     5 f         4\n#> 4     6 4        32\n#> 5     6 f        43\n#> 6     6 r         4\n#> # … 3 rows\ncontingency table another way commonly used way summarising \ninformation. Use one pivoting functions construct contingency\ntable shown based output .\n#> # tibble: 4 x 4\n#>     cyl   `4`     f     r\n#>   <int> <int> <int> <int>\n#> 1     4    23    58    NA\n#> 2     5    NA     4    NA\n#> 3     6    32    43     4\n#> 4     8    48     1    21One way summarising distribution one categorical variable based\nlevels another using dplyr::count(), e.g. following\ngives distribution drv (type drive train) level \ncyl (number cylinders) cars mpg dataset.contingency table another way commonly used way summarising \ninformation. Use one pivoting functions construct contingency\ntable shown based output .","code":"\nstocks <- tibble(\n  year   = c(2015, 2015, 2016, 2016),\n  half   = c(   1,    2,    1,    2),\n  return = c(1.88, 0.59, 0.92, 0.17)\n)\nstocks %>%\n  pivot_wider(names_from = year, values_from = return) %>%\n  pivot_longer(`2015`:`2016`, names_to = \"year\", values_to = \"return\")\ntable4a %>%\n  pivot_longer(c(1999, 2000), names_to = \"year\", values_to = \"cases\")\n#> Error: Can't subset columns that don't exist.\n#> x Locations 1999 and 2000 don't exist.\n#> ℹ There are only 3 columns.\npeople <- tribble(\n  ~name,             ~names,  ~values,\n  #-----------------|--------|-------\n  \"Phillip Woods\",   \"age\",        45,\n  \"Phillip Woods\",   \"height\",    186,\n  \"Phillip Woods\",   \"age\",        50,\n  \"Jessica Cordero\", \"age\",        37,\n  \"Jessica Cordero\", \"height\",    156\n)\nemployees <- tribble(\n  ~know_drive, ~prefer, ~not_prefer,\n  \"yes\",       20,      10,\n  \"no\",        NA,      12\n)\nmpg %>%\n  count(cyl, drv)\n#> # A tibble: 9 x 3\n#>     cyl drv       n\n#>   <int> <chr> <int>\n#> 1     4 4        23\n#> 2     4 f        58\n#> 3     5 f         4\n#> 4     6 4        32\n#> 5     6 f        43\n#> 6     6 r         4\n#> # … with 3 more rows#> # A tibble: 4 x 4\n#>     cyl   `4`     f     r\n#>   <int> <int> <int> <int>\n#> 1     4    23    58    NA\n#> 2     5    NA     4    NA\n#> 3     6    32    43     4\n#> 4     8    48     1    21"},{"path":"tidy-data.html","id":"separating","chapter":"12 Tidy data","heading":"12.4 Separating","text":"far ’ve learned tidy table2, table4a, table4b, table3. table3 different problem: one column (rate) contains two variables (cases population). fix problem, ’ll need separate() function. ’ll also learn complement separate(): unite(), use single variable spread across multiple columns.","code":""},{"path":"tidy-data.html","id":"separate","chapter":"12 Tidy data","heading":"12.4.1 Separate","text":"separate() pulls apart one column multiple columns, splitting wherever separator character appears. Take table3:rate column contains cases population variables, need split two variables. separate() takes name column separate, names columns separate , shown Figure 12.6 code .\nFigure 12.6: Separating rate cases population make table3 tidy\ndefault, separate() split values wherever sees non-alphanumeric character (.e. character isn’t number letter). example, code , separate() split values rate forward slash characters. wish use specific character separate column, can pass character sep argument separate(). example, rewrite code :(Formally, sep regular expression, ’ll learn Chapter 14.)Look carefully column types: ’ll notice cases population character columns. default behaviour separate(): leaves type column . , however, ’s useful really numbers. can ask separate() try convert better types using convert = TRUE:","code":"\ntable3\n#> # A tibble: 6 x 3\n#>   country      year rate             \n#> * <chr>       <int> <chr>            \n#> 1 Afghanistan  1999 745/19987071     \n#> 2 Afghanistan  2000 2666/20595360    \n#> 3 Brazil       1999 37737/172006362  \n#> 4 Brazil       2000 80488/174504898  \n#> 5 China        1999 212258/1272915272\n#> 6 China        2000 213766/1280428583\ntable3 %>%\n  separate(rate, into = c(\"cases\", \"population\"))\n#> # A tibble: 6 x 4\n#>   country      year cases  population\n#>   <chr>       <int> <chr>  <chr>     \n#> 1 Afghanistan  1999 745    19987071  \n#> 2 Afghanistan  2000 2666   20595360  \n#> 3 Brazil       1999 37737  172006362 \n#> 4 Brazil       2000 80488  174504898 \n#> 5 China        1999 212258 1272915272\n#> 6 China        2000 213766 1280428583\ntable3 %>%\n  separate(rate, into = c(\"cases\", \"population\"), sep = \"/\")\ntable3 %>%\n  separate(rate, into = c(\"cases\", \"population\"), convert = TRUE)\n#> # A tibble: 6 x 4\n#>   country      year  cases population\n#>   <chr>       <int>  <int>      <int>\n#> 1 Afghanistan  1999    745   19987071\n#> 2 Afghanistan  2000   2666   20595360\n#> 3 Brazil       1999  37737  172006362\n#> 4 Brazil       2000  80488  174504898\n#> 5 China        1999 212258 1272915272\n#> 6 China        2000 213766 1280428583"},{"path":"tidy-data.html","id":"unite","chapter":"12 Tidy data","heading":"12.4.2 Unite","text":"unite() inverse separate(): combines multiple columns single column. ’ll need much less frequently separate(), ’s still useful tool back pocket.can use unite() rejoin cases population columns created last example. data saved tidyr::table1. unite() takes data frame, name new variable create, set columns combine, specified dplyr::select() style:case also need use sep argument. default place underscore (_) values different columns. want \"/\" instead:","code":"\ntable1 %>%\n  unite(rate, cases, population)\n#> # A tibble: 6 x 3\n#>   country      year rate             \n#>   <chr>       <int> <chr>            \n#> 1 Afghanistan  1999 745_19987071     \n#> 2 Afghanistan  2000 2666_20595360    \n#> 3 Brazil       1999 37737_172006362  \n#> 4 Brazil       2000 80488_174504898  \n#> 5 China        1999 212258_1272915272\n#> 6 China        2000 213766_1280428583\ntable1 %>%\n  unite(rate, cases, population, sep = \"/\")\n#> # A tibble: 6 x 3\n#>   country      year rate             \n#>   <chr>       <int> <chr>            \n#> 1 Afghanistan  1999 745/19987071     \n#> 2 Afghanistan  2000 2666/20595360    \n#> 3 Brazil       1999 37737/172006362  \n#> 4 Brazil       2000 80488/174504898  \n#> 5 China        1999 212258/1272915272\n#> 6 China        2000 213766/1280428583"},{"path":"tidy-data.html","id":"exercises-25","chapter":"12 Tidy data","heading":"12.4.3 Exercises","text":"extra fill arguments separate()?\nExperiment various options following two toy datasets.\n\ntibble(x = c(\",b,c\", \"d,e,f,g\", \"h,,j\")) %>%\n  separate(x, c(\"one\", \"two\", \"three\"))\n\ntibble(x = c(\",b,c\", \"d,e\", \"f,g,\")) %>%\n  separate(x, c(\"one\", \"two\", \"three\"))extra fill arguments separate()?\nExperiment various options following two toy datasets.unite() separate() remove argument. \n? set FALSE?unite() separate() remove argument. \n? set FALSE?Compare contrast separate() extract(). \nthree variations separation (position, separator, \ngroups), one unite?Compare contrast separate() extract(). \nthree variations separation (position, separator, \ngroups), one unite?following example ’re using unite() create date column\nmonth day columns. achieve outcome\nusing mutate() paste() instead unite?\n\nevents <- tribble(\n  ~month, ~day,\n  1     , 20,\n  1     , 21,\n  1     , 22\n)\n\nevents %>%\n  unite(\"date\", month:day, sep = \"-\", remove = FALSE)following example ’re using unite() create date column\nmonth day columns. achieve outcome\nusing mutate() paste() instead unite?can also pass vector integers sep. separate() interpret\nintegers positions split . Positive values start 1 \nfar-left strings; negative value start -1 far-right \nstrings. Use separate() represent location information following\ntibble two columns: state (represented first two characters) \ncounty. two ways: using positive negative value sep.\n\nbaker <- tribble(\n  ~location,\n  \"FLBaker County\",\n  \"GABaker County\",\n  \"ORBaker County\",\n)\nbaker\n#> # tibble: 3 x 1\n#>   location      \n#>   <chr>         \n#> 1 FLBaker County\n#> 2 GABaker County\n#> 3 ORBaker CountyYou can also pass vector integers sep. separate() interpret\nintegers positions split . Positive values start 1 \nfar-left strings; negative value start -1 far-right \nstrings. Use separate() represent location information following\ntibble two columns: state (represented first two characters) \ncounty. two ways: using positive negative value sep.","code":"\ntibble(x = c(\"a,b,c\", \"d,e,f,g\", \"h,i,j\")) %>%\n  separate(x, c(\"one\", \"two\", \"three\"))\n\ntibble(x = c(\"a,b,c\", \"d,e\", \"f,g,i\")) %>%\n  separate(x, c(\"one\", \"two\", \"three\"))\nevents <- tribble(\n  ~month, ~day,\n  1     , 20,\n  1     , 21,\n  1     , 22\n)\n\nevents %>%\n  unite(\"date\", month:day, sep = \"-\", remove = FALSE)\nbaker <- tribble(\n  ~location,\n  \"FLBaker County\",\n  \"GABaker County\",\n  \"ORBaker County\",\n)\nbaker\n#> # A tibble: 3 x 1\n#>   location      \n#>   <chr>         \n#> 1 FLBaker County\n#> 2 GABaker County\n#> 3 ORBaker County"},{"path":"tidy-data.html","id":"missing-values-3","chapter":"12 Tidy data","heading":"12.5 Missing values","text":"Changing representation dataset brings important subtlety missing values. Surprisingly, value can missing one two possible ways:Explicitly, .e. flagged NA.Implicitly, .e. simply present data.Let’s illustrate idea simple data set:two missing values dataset:return fourth quarter 2015 explicitly missing, \ncell value instead contains NA.return fourth quarter 2015 explicitly missing, \ncell value instead contains NA.return first quarter 2016 implicitly missing, \nsimply appear dataset.return first quarter 2016 implicitly missing, \nsimply appear dataset.One way think difference Zen-like koan: explicit missing value presence absence; implicit missing value absence presence.way dataset represented can make implicit values explicit. example, can make implicit missing value explicit putting years columns:explicit missing values may important representations data, can set values_drop_na = TRUE pivot_longer() turn explicit missing values implicit:Another important tool making missing values explicit tidy data complete():complete() takes set columns, finds unique combinations. ensures original dataset contains values, filling explicit NAs necessary.’s one important tool know working missing values. Sometimes data source primarily used data entry, missing values indicate previous value carried forward:can fill missing values fill(). takes set columns want missing values replaced recent non-missing value (sometimes called last observation carried forward).","code":"\nstocks <- tibble(\n  year   = c(2015, 2015, 2015, 2015, 2016, 2016, 2016),\n  qtr    = c(   1,    2,    3,    4,    2,    3,    4),\n  return = c(1.88, 0.59, 0.35,   NA, 0.92, 0.17, 2.66)\n)\nstocks %>%\n  pivot_wider(names_from = year, values_from = return)\n#> # A tibble: 4 x 3\n#>     qtr `2015` `2016`\n#>   <dbl>  <dbl>  <dbl>\n#> 1     1   1.88  NA   \n#> 2     2   0.59   0.92\n#> 3     3   0.35   0.17\n#> 4     4  NA      2.66\nstocks %>%\n  pivot_wider(names_from = year, values_from = return) %>%\n  pivot_longer(\n    cols = c(`2015`, `2016`),\n    names_to = \"year\",\n    values_to = \"return\",\n    values_drop_na = TRUE\n  )\n#> # A tibble: 6 x 3\n#>     qtr year  return\n#>   <dbl> <chr>  <dbl>\n#> 1     1 2015    1.88\n#> 2     2 2015    0.59\n#> 3     2 2016    0.92\n#> 4     3 2015    0.35\n#> 5     3 2016    0.17\n#> 6     4 2016    2.66\nstocks %>%\n  complete(year, qtr)\n#> # A tibble: 8 x 3\n#>    year   qtr return\n#>   <dbl> <dbl>  <dbl>\n#> 1  2015     1   1.88\n#> 2  2015     2   0.59\n#> 3  2015     3   0.35\n#> 4  2015     4  NA   \n#> 5  2016     1  NA   \n#> 6  2016     2   0.92\n#> # … with 2 more rows\ntreatment <- tribble(\n  ~person,           ~treatment, ~response,\n  \"Derrick Whitmore\", 1,         7,\n  NA,                 2,         10,\n  NA,                 3,         9,\n  \"Katherine Burke\",  1,         4\n)\ntreatment %>%\n  fill(person)\n#> # A tibble: 4 x 3\n#>   person           treatment response\n#>   <chr>                <dbl>    <dbl>\n#> 1 Derrick Whitmore         1        7\n#> 2 Derrick Whitmore         2       10\n#> 3 Derrick Whitmore         3        9\n#> 4 Katherine Burke          1        4"},{"path":"tidy-data.html","id":"exercises-26","chapter":"12 Tidy data","heading":"12.5.1 Exercises","text":"Compare contrast fill arguments pivot_wider() complete().Compare contrast fill arguments pivot_wider() complete().direction argument fill() ?direction argument fill() ?","code":""},{"path":"tidy-data.html","id":"case-study","chapter":"12 Tidy data","heading":"12.6 Case study","text":"finish chapter, let’s pull together everything ’ve learned tackle realistic data tidying problem. tidyr::dataset contains tuberculosis (TB) cases broken year, country, age, gender, diagnosis method. data comes 2014 World Health Organization Global Tuberculosis Report, available http://www..int/tb/country/data/download/en.’s wealth epidemiological information dataset, ’s challenging work data form ’s provided:typical real-life example dataset. contains redundant columns, odd variable names, many missing values. short, dataset messy, ’ll need methodical tidy . functions like pivot_wider() pivot_longer() generally means iterative approach work well – aim accomplish one goal time, run function examine resulting data frame, go back set arguments function needed resulting data frame exactly need.best place start take good look variable names determine whether actually variables contain information captured values new column.looks like country, iso2, iso3 three variables \nredundantly specify country.looks like country, iso2, iso3 three variables \nredundantly specify country.year also variable.year also variable.first three letters variables new_sp_m014 newrel_f65\ndenote whether column contains new old cases TB. dataset,\ncolumn contains new cases, don’t really need information \ncaptured variable. remaining characters encode three variables \nnames. might able parse little thought\nexperimentation, luckily data dictionary handy. tells us:\nnext two three letters describe diagnosis TB:\nrel stands cases relapse\nep stands cases extrapulmonary TB\nsn stands cases pulmonary TB diagnosed \npulmonary smear (smear negative)\nsp stands cases pulmonary TB diagnosed \npulmonary smear (smear positive)\n\nnext letter gives sex TB patients. dataset groups\ncases males (m) females (f).\nremaining numbers gives age group. dataset groups cases \nseven age groups:\n014 = 0 – 14 years old\n1524 = 15 – 24 years old\n2534 = 25 – 34 years old\n3544 = 35 – 44 years old\n4554 = 45 – 54 years old\n5564 = 55 – 64 years old\n65 = 65 older\n\nfirst three letters variables new_sp_m014 newrel_f65\ndenote whether column contains new old cases TB. dataset,\ncolumn contains new cases, don’t really need information \ncaptured variable. remaining characters encode three variables \nnames. might able parse little thought\nexperimentation, luckily data dictionary handy. tells us:next two three letters describe diagnosis TB:\nrel stands cases relapse\nep stands cases extrapulmonary TB\nsn stands cases pulmonary TB diagnosed \npulmonary smear (smear negative)\nsp stands cases pulmonary TB diagnosed \npulmonary smear (smear positive)\nnext two three letters describe diagnosis TB:rel stands cases relapseep stands cases extrapulmonary TBsn stands cases pulmonary TB diagnosed \npulmonary smear (smear negative)sp stands cases pulmonary TB diagnosed \npulmonary smear (smear positive)next letter gives sex TB patients. dataset groups\ncases males (m) females (f).next letter gives sex TB patients. dataset groups\ncases males (m) females (f).remaining numbers gives age group. dataset groups cases \nseven age groups:\n014 = 0 – 14 years old\n1524 = 15 – 24 years old\n2534 = 25 – 34 years old\n3544 = 35 – 44 years old\n4554 = 45 – 54 years old\n5564 = 55 – 64 years old\n65 = 65 older\nremaining numbers gives age group. dataset groups cases \nseven age groups:014 = 0 – 14 years old1524 = 15 – 24 years old2534 = 25 – 34 years old3544 = 35 – 44 years old4554 = 45 – 54 years old5564 = 55 – 64 years old65 = 65 olderWe can break variables specifying multiple column names names_to either providing names_pattern specify want break regular expression containing groups (defined ()) puts group column. ’ll learn regular expressions Chapter 14, basic idea variable name like new_sp_m014, want capture sp, m, 014 separate groups, can think variable’s name new_(sp)_(m)(014). constructing appropriate regular expression need keep mind messy features variable names:variables start new_ start new without underscore separating diagnosis.diagnoses age groups indicated varying numbers characters (e.g. sp vs. rel 014 vs. 4554.)regular expression capture inconsistencies extract three groups information need new_?(.*)_(.)(.*).looks pretty good first pass, improvements can make. First, ’re seeing lots NAs cases column. can drop observations setting values_drop_na TRUE.Second, diagnosis gender characters default, however ’s good idea convert factors since categorical variables known set values. ’ll use parse_factor() function readr make conversion mutate() step add pipeline.Finally, might want recode age variable level names bit easier read bit informative. ’ll within mutate() step pipeline using forcats::fct_recode() ’ll learn Chapter 15.tidy data frame allows us explore data ease original dataset. example, can easily filter particular type TB given country sum number cases see case numbers type TB evolved years.","code":"\nwho\n#> # A tibble: 7,240 x 60\n#>   country iso2  iso3   year new_sp_m014 new_sp_m1524 new_sp_m2534 new_sp_m3544\n#>   <chr>   <chr> <chr> <int>       <int>        <int>        <int>        <int>\n#> 1 Afghan… AF    AFG    1980          NA           NA           NA           NA\n#> 2 Afghan… AF    AFG    1981          NA           NA           NA           NA\n#> 3 Afghan… AF    AFG    1982          NA           NA           NA           NA\n#> 4 Afghan… AF    AFG    1983          NA           NA           NA           NA\n#> 5 Afghan… AF    AFG    1984          NA           NA           NA           NA\n#> 6 Afghan… AF    AFG    1985          NA           NA           NA           NA\n#> # … with 7,234 more rows, and 52 more variables: new_sp_m4554 <int>,\n#> #   new_sp_m5564 <int>, new_sp_m65 <int>, new_sp_f014 <int>,\n#> #   new_sp_f1524 <int>, new_sp_f2534 <int>, new_sp_f3544 <int>,\n#> #   new_sp_f4554 <int>, new_sp_f5564 <int>, new_sp_f65 <int>,\n#> #   new_sn_m014 <int>, new_sn_m1524 <int>, new_sn_m2534 <int>,\n#> #   new_sn_m3544 <int>, new_sn_m4554 <int>, new_sn_m5564 <int>,\n#> #   new_sn_m65 <int>, new_sn_f014 <int>, new_sn_f1524 <int>,\n#> #   new_sn_f2534 <int>, new_sn_f3544 <int>, new_sn_f4554 <int>,\n#> #   new_sn_f5564 <int>, new_sn_f65 <int>, new_ep_m014 <int>,\n#> #   new_ep_m1524 <int>, new_ep_m2534 <int>, new_ep_m3544 <int>,\n#> #   new_ep_m4554 <int>, new_ep_m5564 <int>, new_ep_m65 <int>,\n#> #   new_ep_f014 <int>, new_ep_f1524 <int>, new_ep_f2534 <int>,\n#> #   new_ep_f3544 <int>, new_ep_f4554 <int>, new_ep_f5564 <int>,\n#> #   new_ep_f65 <int>, newrel_m014 <int>, newrel_m1524 <int>,\n#> #   newrel_m2534 <int>, newrel_m3544 <int>, newrel_m4554 <int>,\n#> #   newrel_m5564 <int>, newrel_m65 <int>, newrel_f014 <int>,\n#> #   newrel_f1524 <int>, newrel_f2534 <int>, newrel_f3544 <int>,\n#> #   newrel_f4554 <int>, newrel_f5564 <int>, newrel_f65 <int>\nnames(who)\n#>  [1] \"country\"      \"iso2\"         \"iso3\"         \"year\"         \"new_sp_m014\" \n#>  [6] \"new_sp_m1524\" \"new_sp_m2534\" \"new_sp_m3544\" \"new_sp_m4554\" \"new_sp_m5564\"\n#> [11] \"new_sp_m65\"   \"new_sp_f014\"  \"new_sp_f1524\" \"new_sp_f2534\" \"new_sp_f3544\"\n#> [16] \"new_sp_f4554\" \"new_sp_f5564\" \"new_sp_f65\"   \"new_sn_m014\"  \"new_sn_m1524\"\n#> [21] \"new_sn_m2534\" \"new_sn_m3544\" \"new_sn_m4554\" \"new_sn_m5564\" \"new_sn_m65\"  \n#> [26] \"new_sn_f014\"  \"new_sn_f1524\" \"new_sn_f2534\" \"new_sn_f3544\" \"new_sn_f4554\"\n#> [31] \"new_sn_f5564\" \"new_sn_f65\"   \"new_ep_m014\"  \"new_ep_m1524\" \"new_ep_m2534\"\n#> [36] \"new_ep_m3544\" \"new_ep_m4554\" \"new_ep_m5564\" \"new_ep_m65\"   \"new_ep_f014\" \n#> [41] \"new_ep_f1524\" \"new_ep_f2534\" \"new_ep_f3544\" \"new_ep_f4554\" \"new_ep_f5564\"\n#> [46] \"new_ep_f65\"   \"newrel_m014\"  \"newrel_m1524\" \"newrel_m2534\" \"newrel_m3544\"\n#> [51] \"newrel_m4554\" \"newrel_m5564\" \"newrel_m65\"   \"newrel_f014\"  \"newrel_f1524\"\n#> [56] \"newrel_f2534\" \"newrel_f3544\" \"newrel_f4554\" \"newrel_f5564\" \"newrel_f65\"\nwho %>%\n  pivot_longer(\n    cols = new_sp_m014:newrel_f65,\n    names_to = c(\"diagnosis\", \"gender\", \"age\"),\n    names_pattern = \"new_?(.*)_(.)(.*)\",\n    values_to = \"cases\"\n  )\n#> # A tibble: 405,440 x 8\n#>   country     iso2  iso3   year diagnosis gender age   cases\n#>   <chr>       <chr> <chr> <int> <chr>     <chr>  <chr> <int>\n#> 1 Afghanistan AF    AFG    1980 sp        m      014      NA\n#> 2 Afghanistan AF    AFG    1980 sp        m      1524     NA\n#> 3 Afghanistan AF    AFG    1980 sp        m      2534     NA\n#> 4 Afghanistan AF    AFG    1980 sp        m      3544     NA\n#> 5 Afghanistan AF    AFG    1980 sp        m      4554     NA\n#> 6 Afghanistan AF    AFG    1980 sp        m      5564     NA\n#> # … with 405,434 more rows\nwho %>%\n  pivot_longer(\n    cols = new_sp_m014:newrel_f65,\n    names_to = c(\"diagnosis\", \"gender\", \"age\"),\n    names_pattern = \"new_?(.*)_(.)(.*)\",\n    values_to = \"cases\",\n    values_drop_na = TRUE\n  )\n#> # A tibble: 76,046 x 8\n#>   country     iso2  iso3   year diagnosis gender age   cases\n#>   <chr>       <chr> <chr> <int> <chr>     <chr>  <chr> <int>\n#> 1 Afghanistan AF    AFG    1997 sp        m      014       0\n#> 2 Afghanistan AF    AFG    1997 sp        m      1524     10\n#> 3 Afghanistan AF    AFG    1997 sp        m      2534      6\n#> 4 Afghanistan AF    AFG    1997 sp        m      3544      3\n#> 5 Afghanistan AF    AFG    1997 sp        m      4554      5\n#> 6 Afghanistan AF    AFG    1997 sp        m      5564      2\n#> # … with 76,040 more rows\nwho %>%\n  pivot_longer(\n    cols = new_sp_m014:newrel_f65,\n    names_to = c(\"diagnosis\", \"gender\", \"age\"),\n    names_pattern = \"new_?(.*)_(.)(.*)\",\n    values_to = \"cases\",\n    values_drop_na = TRUE\n  ) %>%\n  mutate(\n    gender = parse_factor(gender, levels = c(\"f\", \"m\")),\n    age = parse_factor(\n      age,\n      levels = c(\"014\", \"1524\", \"2534\", \"3544\", \"4554\", \"5564\", \"65\"),\n      ordered = TRUE\n    )\n  )\n#> # A tibble: 76,046 x 8\n#>   country     iso2  iso3   year diagnosis gender age   cases\n#>   <chr>       <chr> <chr> <int> <chr>     <fct>  <ord> <int>\n#> 1 Afghanistan AF    AFG    1997 sp        m      014       0\n#> 2 Afghanistan AF    AFG    1997 sp        m      1524     10\n#> 3 Afghanistan AF    AFG    1997 sp        m      2534      6\n#> 4 Afghanistan AF    AFG    1997 sp        m      3544      3\n#> 5 Afghanistan AF    AFG    1997 sp        m      4554      5\n#> 6 Afghanistan AF    AFG    1997 sp        m      5564      2\n#> # … with 76,040 more rows\nwho_tidy <- who %>%\n  pivot_longer(\n    cols = new_sp_m014:newrel_f65,\n    names_to = c(\"diagnosis\", \"gender\", \"age\"),\n    names_pattern = \"new_?(.*)_(.)(.*)\",\n    values_to = \"cases\",\n    values_drop_na = TRUE\n  ) %>%\n  mutate(\n    gender = parse_factor(gender, levels = c(\"f\", \"m\")),\n    age = parse_factor(\n      age,\n      levels = c(\"014\", \"1524\", \"2534\", \"3544\", \"4554\", \"5564\", \"65\"),\n      ordered = TRUE\n    ),\n    age = fct_recode(\n      age,\n      \"0-14\"  = \"014\",\n      \"15-24\" = \"1524\",\n      \"25-34\" = \"2534\",\n      \"35-44\" = \"3544\",\n      \"45-54\" = \"4554\",\n      \"55-64\" = \"5564\",\n      \"65+\"   = \"65\"\n    )\n  )\nwho_tidy\n#> # A tibble: 76,046 x 8\n#>   country     iso2  iso3   year diagnosis gender age   cases\n#>   <chr>       <chr> <chr> <int> <chr>     <fct>  <ord> <int>\n#> 1 Afghanistan AF    AFG    1997 sp        m      0-14      0\n#> 2 Afghanistan AF    AFG    1997 sp        m      15-24    10\n#> 3 Afghanistan AF    AFG    1997 sp        m      25-34     6\n#> 4 Afghanistan AF    AFG    1997 sp        m      35-44     3\n#> 5 Afghanistan AF    AFG    1997 sp        m      45-54     5\n#> 6 Afghanistan AF    AFG    1997 sp        m      55-64     2\n#> # … with 76,040 more rows\nwho_tidy %>%\n  filter(diagnosis == \"sp\", country == \"United States of America\") %>%\n  group_by(year) %>%\n  summarise(cases_total = sum(cases)) %>%\n  ggplot(aes(x = year, y = cases_total)) +\n  geom_point() +\n  geom_smooth() +\n  labs(title = \"Number of smear positive pulmonary TB cases in the US\")\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'"},{"path":"tidy-data.html","id":"exercises-27","chapter":"12 Tidy data","heading":"12.6.1 Exercises","text":"case study set values_drop_na = TRUE just make easier \ncheck correct values. reasonable? Think \nmissing values represented dataset. implicit\nmissing values? ’s difference NA zero?case study set values_drop_na = TRUE just make easier \ncheck correct values. reasonable? Think \nmissing values represented dataset. implicit\nmissing values? ’s difference NA zero?claimed iso2 iso3 redundant country.\nConfirm claim think situations might want keep \ninformation data frame might choose discard \nredundant columns.claimed iso2 iso3 redundant country.\nConfirm claim think situations might want keep \ninformation data frame might choose discard \nredundant columns.country, year, sex compute total number cases \nTB. Make informative visualisation data.country, year, sex compute total number cases \nTB. Make informative visualisation data.","code":""},{"path":"tidy-data.html","id":"non-tidy-data","chapter":"12 Tidy data","heading":"12.7 Non-tidy data","text":"continue topics, ’s worth talking briefly non-tidy data. Earlier chapter, used pejorative term “messy” refer non-tidy data. ’s oversimplification: lots useful well-founded data structures tidy data. two main reasons use data structures:Alternative representations may substantial performance space\nadvantages.Alternative representations may substantial performance space\nadvantages.Specialised fields evolved conventions storing data\nmay quite different conventions tidy data.Specialised fields evolved conventions storing data\nmay quite different conventions tidy data.Either reasons means ’ll need something tibble (data frame). data fit naturally rectangular structure composed observations variables, think tidy data default choice. good reasons use structures; tidy data way.’d like learn non-tidy data, ’d highly recommend thoughtful blog post Jeff Leek: http://simplystatistics.org/2016/02/17/non-tidy-data.","code":""},{"path":"relational-data.html","id":"relational-data","chapter":"13 Relational data","heading":"13 Relational data","text":"","code":""},{"path":"relational-data.html","id":"introduction-7","chapter":"13 Relational data","heading":"13.1 Introduction","text":"’s rare data analysis involves single table data. Typically many tables data, must combine answer questions ’re interested . Collectively, multiple tables data called relational data relations, just individual datasets, important.Relations always defined pair tables. relations built simple idea: relations three tables always property relations pair. Sometimes elements pair can table! needed , example, table people, person reference parents.work relational data need verbs work pairs tables. three families verbs designed work relational data:Mutating joins, add new variables one data frame matching\nobservations another.Mutating joins, add new variables one data frame matching\nobservations another.Filtering joins, filter observations one data frame based \nwhether match observation table.Filtering joins, filter observations one data frame based \nwhether match observation table.Set operations, treat observations set elements.Set operations, treat observations set elements.common place find relational data relational database management system (RDBMS), term encompasses almost modern databases. ’ve used database , ’ve almost certainly used SQL. , find concepts chapter familiar, although expression dplyr little different. Generally, dplyr little easier use SQL dplyr specialised data analysis: makes common data analysis operations easier, expense making difficult things aren’t commonly needed data analysis.","code":""},{"path":"relational-data.html","id":"prerequisites-7","chapter":"13 Relational data","heading":"13.1.1 Prerequisites","text":"explore relational data nycflights13 using two-table verbs dplyr.","code":"\nlibrary(tidyverse)\nlibrary(nycflights13)"},{"path":"relational-data.html","id":"nycflights13-relational","chapter":"13 Relational data","heading":"13.2 nycflights13","text":"use nycflights13 package learn relational data. nycflights13 contains five tibbles : airlines, airports, weather planes related flights table used data transformation:airlines lets look full carrier name abbreviated\ncode:\n\nairlines\n#> # tibble: 16 x 2\n#>   carrier name                    \n#>   <chr>   <chr>                   \n#> 1 9E      Endeavor Air Inc.       \n#> 2 AA      American Airlines Inc.  \n#> 3      Alaska Airlines Inc.    \n#> 4 B6      JetBlue Airways         \n#> 5 DL      Delta Air Lines Inc.    \n#> 6 EV      ExpressJet Airlines Inc.\n#> # … 10 rowsairlines lets look full carrier name abbreviated\ncode:airports gives information airport, identified faa\nairport code:\n\nairports\n#> # tibble: 1,458 x 8\n#>   faa   name                          lat   lon   alt    tz dst   tzone         \n#>   <chr> <chr>                       <dbl> <dbl> <dbl> <dbl> <chr> <chr>         \n#> 1 04G   Lansdowne Airport            41.1 -80.6  1044    -5     America/New_Y…\n#> 2 06A   Moton Field Municipal Airp…  32.5 -85.7   264    -6     America/Chica…\n#> 3 06C   Schaumburg Regional          42.0 -88.1   801    -6     America/Chica…\n#> 4 06N   Randall Airport              41.4 -74.4   523    -5     America/New_Y…\n#> 5 09J   Jekyll Island Airport        31.1 -81.4    11    -5     America/New_Y…\n#> 6 0A9   Elizabethton Municipal Air…  36.4 -82.2  1593    -5     America/New_Y…\n#> # … 1,452 rowsairports gives information airport, identified faa\nairport code:planes gives information plane, identified tailnum:\n\nplanes\n#> # tibble: 3,322 x 9\n#>   tailnum  year type           manufacturer   model  engines seats speed engine \n#>   <chr>   <int> <chr>          <chr>          <chr>    <int> <int> <int> <chr>  \n#> 1 N10156   2004 Fixed wing mu… EMBRAER        EMB-1…       2    55    NA Turbo-…\n#> 2 N102UW   1998 Fixed wing mu… AIRBUS INDUST… A320-…       2   182    NA Turbo-…\n#> 3 N103US   1999 Fixed wing mu… AIRBUS INDUST… A320-…       2   182    NA Turbo-…\n#> 4 N104UW   1999 Fixed wing mu… AIRBUS INDUST… A320-…       2   182    NA Turbo-…\n#> 5 N10575   2002 Fixed wing mu… EMBRAER        EMB-1…       2    55    NA Turbo-…\n#> 6 N105UW   1999 Fixed wing mu… AIRBUS INDUST… A320-…       2   182    NA Turbo-…\n#> # … 3,316 rowsplanes gives information plane, identified tailnum:weather gives weather NYC airport hour:\n\nweather\n#> # tibble: 26,115 x 15\n#>   origin  year month   day  hour  temp  dewp humid wind_dir wind_speed wind_gust\n#>   <chr>  <int> <int> <int> <int> <dbl> <dbl> <dbl>    <dbl>      <dbl>     <dbl>\n#> 1 EWR     2013     1     1     1  39.0  26.1  59.4      270      10.4         NA\n#> 2 EWR     2013     1     1     2  39.0  27.0  61.6      250       8.06        NA\n#> 3 EWR     2013     1     1     3  39.0  28.0  64.4      240      11.5         NA\n#> 4 EWR     2013     1     1     4  39.9  28.0  62.2      250      12.7         NA\n#> 5 EWR     2013     1     1     5  39.0  28.0  64.4      260      12.7         NA\n#> 6 EWR     2013     1     1     6  37.9  28.0  67.2      240      11.5         NA\n#> # … 26,109 rows, 4 variables: precip <dbl>, pressure <dbl>,\n#> #   visib <dbl>, time_hour <dttm>weather gives weather NYC airport hour:One way show relationships different tables drawing:diagram little overwhelming, ’s simple compared ’ll see wild! key understanding diagrams like remember relation always concerns pair tables. don’t need understand whole thing; just need understand chain relations tables interested .nycflights13:flights connects planes via single variable, tailnum.flights connects planes via single variable, tailnum.flights connects airlines carrier variable.flights connects airlines carrier variable.flights connects airports two ways: via origin \ndest variables.flights connects airports two ways: via origin \ndest variables.flights connects weather via origin (location), \nyear, month, day hour (time).flights connects weather via origin (location), \nyear, month, day hour (time).","code":"\nairlines\n#> # A tibble: 16 x 2\n#>   carrier name                    \n#>   <chr>   <chr>                   \n#> 1 9E      Endeavor Air Inc.       \n#> 2 AA      American Airlines Inc.  \n#> 3 AS      Alaska Airlines Inc.    \n#> 4 B6      JetBlue Airways         \n#> 5 DL      Delta Air Lines Inc.    \n#> 6 EV      ExpressJet Airlines Inc.\n#> # … with 10 more rows\nairports\n#> # A tibble: 1,458 x 8\n#>   faa   name                          lat   lon   alt    tz dst   tzone         \n#>   <chr> <chr>                       <dbl> <dbl> <dbl> <dbl> <chr> <chr>         \n#> 1 04G   Lansdowne Airport            41.1 -80.6  1044    -5 A     America/New_Y…\n#> 2 06A   Moton Field Municipal Airp…  32.5 -85.7   264    -6 A     America/Chica…\n#> 3 06C   Schaumburg Regional          42.0 -88.1   801    -6 A     America/Chica…\n#> 4 06N   Randall Airport              41.4 -74.4   523    -5 A     America/New_Y…\n#> 5 09J   Jekyll Island Airport        31.1 -81.4    11    -5 A     America/New_Y…\n#> 6 0A9   Elizabethton Municipal Air…  36.4 -82.2  1593    -5 A     America/New_Y…\n#> # … with 1,452 more rows\nplanes\n#> # A tibble: 3,322 x 9\n#>   tailnum  year type           manufacturer   model  engines seats speed engine \n#>   <chr>   <int> <chr>          <chr>          <chr>    <int> <int> <int> <chr>  \n#> 1 N10156   2004 Fixed wing mu… EMBRAER        EMB-1…       2    55    NA Turbo-…\n#> 2 N102UW   1998 Fixed wing mu… AIRBUS INDUST… A320-…       2   182    NA Turbo-…\n#> 3 N103US   1999 Fixed wing mu… AIRBUS INDUST… A320-…       2   182    NA Turbo-…\n#> 4 N104UW   1999 Fixed wing mu… AIRBUS INDUST… A320-…       2   182    NA Turbo-…\n#> 5 N10575   2002 Fixed wing mu… EMBRAER        EMB-1…       2    55    NA Turbo-…\n#> 6 N105UW   1999 Fixed wing mu… AIRBUS INDUST… A320-…       2   182    NA Turbo-…\n#> # … with 3,316 more rows\nweather\n#> # A tibble: 26,115 x 15\n#>   origin  year month   day  hour  temp  dewp humid wind_dir wind_speed wind_gust\n#>   <chr>  <int> <int> <int> <int> <dbl> <dbl> <dbl>    <dbl>      <dbl>     <dbl>\n#> 1 EWR     2013     1     1     1  39.0  26.1  59.4      270      10.4         NA\n#> 2 EWR     2013     1     1     2  39.0  27.0  61.6      250       8.06        NA\n#> 3 EWR     2013     1     1     3  39.0  28.0  64.4      240      11.5         NA\n#> 4 EWR     2013     1     1     4  39.9  28.0  62.2      250      12.7         NA\n#> 5 EWR     2013     1     1     5  39.0  28.0  64.4      260      12.7         NA\n#> 6 EWR     2013     1     1     6  37.9  28.0  67.2      240      11.5         NA\n#> # … with 26,109 more rows, and 4 more variables: precip <dbl>, pressure <dbl>,\n#> #   visib <dbl>, time_hour <dttm>"},{"path":"relational-data.html","id":"exercises-28","chapter":"13 Relational data","heading":"13.2.1 Exercises","text":"Imagine wanted draw (approximately) route plane flies \norigin destination. variables need? tables\nneed combine?Imagine wanted draw (approximately) route plane flies \norigin destination. variables need? tables\nneed combine?forgot draw relationship weather airports.\nrelationship appear diagram?forgot draw relationship weather airports.\nrelationship appear diagram?weather contains information origin (NYC) airports. \ncontained weather records airports USA, additional\nrelation define flights?weather contains information origin (NYC) airports. \ncontained weather records airports USA, additional\nrelation define flights?know days year “special”, fewer people \nusual fly . might represent data data frame?\nprimary keys table? connect \nexisting tables?know days year “special”, fewer people \nusual fly . might represent data data frame?\nprimary keys table? connect \nexisting tables?","code":""},{"path":"relational-data.html","id":"keys","chapter":"13 Relational data","heading":"13.3 Keys","text":"variables used connect pair tables called keys. key variable (set variables) uniquely identifies observation. simple cases, single variable sufficient identify observation. example, plane uniquely identified tailnum. cases, multiple variables may needed. example, identify observation weather need five variables: year, month, day, hour, origin.two types keys:primary key uniquely identifies observation table.\nexample, planes$tailnum primary key uniquely identifies\nplane planes table.primary key uniquely identifies observation table.\nexample, planes$tailnum primary key uniquely identifies\nplane planes table.foreign key uniquely identifies observation another table.\nexample, flights$tailnum foreign key appears \nflights table matches flight unique plane.foreign key uniquely identifies observation another table.\nexample, flights$tailnum foreign key appears \nflights table matches flight unique plane.variable can primary key foreign key. example, origin part weather primary key, also foreign key airports table.’ve identified primary keys tables, ’s good practice verify indeed uniquely identify observation. One way count() primary keys look entries n greater one:Sometimes table doesn’t explicit primary key: row observation, combination variables reliably identifies . example, ’s primary key flights table? might think date plus flight tail number, neither unique:starting work data, naively assumed flight number used per day: make much easier communicate problems specific flight. Unfortunately case! table lacks primary key, ’s sometimes useful add one mutate() row_number(). makes easier match observations ’ve done filtering want check back original data. called surrogate key.primary key corresponding foreign key another table form relation. Relations typically one--many. example, flight one plane, plane many flights. data, ’ll occasionally see 1--1 relationship. can think special case 1--many. can model many--many relations many--1 relation plus 1--many relation. example, data ’s many--many relationship airlines airports: airline flies many airports; airport hosts many airlines.","code":"\nplanes %>% \n  count(tailnum) %>% \n  filter(n > 1)\n#> # A tibble: 0 x 2\n#> # … with 2 variables: tailnum <chr>, n <int>\n\nweather %>% \n  count(year, month, day, hour, origin) %>% \n  filter(n > 1)\n#> # A tibble: 3 x 6\n#>    year month   day  hour origin     n\n#>   <int> <int> <int> <int> <chr>  <int>\n#> 1  2013    11     3     1 EWR        2\n#> 2  2013    11     3     1 JFK        2\n#> 3  2013    11     3     1 LGA        2\nflights %>% \n  count(year, month, day, flight) %>% \n  filter(n > 1)\n#> # A tibble: 29,768 x 5\n#>    year month   day flight     n\n#>   <int> <int> <int>  <int> <int>\n#> 1  2013     1     1      1     2\n#> 2  2013     1     1      3     2\n#> 3  2013     1     1      4     2\n#> 4  2013     1     1     11     3\n#> 5  2013     1     1     15     2\n#> 6  2013     1     1     21     2\n#> # … with 29,762 more rows\n\nflights %>% \n  count(year, month, day, tailnum) %>% \n  filter(n > 1)\n#> # A tibble: 64,928 x 5\n#>    year month   day tailnum     n\n#>   <int> <int> <int> <chr>   <int>\n#> 1  2013     1     1 N0EGMQ      2\n#> 2  2013     1     1 N11189      2\n#> 3  2013     1     1 N11536      2\n#> 4  2013     1     1 N11544      3\n#> 5  2013     1     1 N11551      2\n#> 6  2013     1     1 N12540      2\n#> # … with 64,922 more rows"},{"path":"relational-data.html","id":"exercises-29","chapter":"13 Relational data","heading":"13.3.1 Exercises","text":"Add surrogate key flights.Add surrogate key flights.Identify keys following datasets\nLahman::Batting,\nbabynames::babynames\nnasaweather::atmos\nfueleconomy::vehicles\nggplot2::diamonds\n(might need install packages read documentation.)Identify keys following datasetsLahman::Batting,babynames::babynamesnasaweather::atmosfueleconomy::vehiclesggplot2::diamonds(might need install packages read documentation.)Draw diagram illustrating connections Batting,\nPeople, Salaries tables Lahman package. Draw another diagram\nshows relationship People, Managers, AwardsManagers.\ncharacterise relationship Batting,\nPitching, Fielding tables?Draw diagram illustrating connections Batting,\nPeople, Salaries tables Lahman package. Draw another diagram\nshows relationship People, Managers, AwardsManagers.characterise relationship Batting,\nPitching, Fielding tables?","code":""},{"path":"relational-data.html","id":"mutating-joins","chapter":"13 Relational data","heading":"13.4 Mutating joins","text":"first tool ’ll look combining pair tables mutating join. mutating join allows combine variables two tables. first matches observations keys, copies across variables one table .Like mutate(), join functions add variables right, lot variables already, new variables won’t get printed . examples, ’ll make easier see ’s going examples creating narrower dataset:(Remember, ’re RStudio, can also use View() avoid problem.)Imagine want add full airline name flights2 data. can combine airlines flights2 data frames left_join():result joining airlines flights2 additional variable: name. call type join mutating join. case, got place using mutate() R’s base subsetting:hard generalise need match multiple variables, takes close reading figure overall intent.following sections explain, detail, mutating joins work. ’ll start learning useful visual representation joins. ’ll use explain four mutating join functions: inner join, three outer joins. working real data, keys don’t always uniquely identify observations, next ’ll talk happens isn’t unique match. Finally, ’ll learn tell dplyr variables keys given join.","code":"\nflights2 <- flights %>% \n  select(year:day, hour, origin, dest, tailnum, carrier)\nflights2\n#> # A tibble: 336,776 x 8\n#>    year month   day  hour origin dest  tailnum carrier\n#>   <int> <int> <int> <dbl> <chr>  <chr> <chr>   <chr>  \n#> 1  2013     1     1     5 EWR    IAH   N14228  UA     \n#> 2  2013     1     1     5 LGA    IAH   N24211  UA     \n#> 3  2013     1     1     5 JFK    MIA   N619AA  AA     \n#> 4  2013     1     1     5 JFK    BQN   N804JB  B6     \n#> 5  2013     1     1     6 LGA    ATL   N668DN  DL     \n#> 6  2013     1     1     5 EWR    ORD   N39463  UA     \n#> # … with 336,770 more rows\nflights2 %>%\n  select(-origin, -dest) %>% \n  left_join(airlines, by = \"carrier\")\n#> # A tibble: 336,776 x 7\n#>    year month   day  hour tailnum carrier name                  \n#>   <int> <int> <int> <dbl> <chr>   <chr>   <chr>                 \n#> 1  2013     1     1     5 N14228  UA      United Air Lines Inc. \n#> 2  2013     1     1     5 N24211  UA      United Air Lines Inc. \n#> 3  2013     1     1     5 N619AA  AA      American Airlines Inc.\n#> 4  2013     1     1     5 N804JB  B6      JetBlue Airways       \n#> 5  2013     1     1     6 N668DN  DL      Delta Air Lines Inc.  \n#> 6  2013     1     1     5 N39463  UA      United Air Lines Inc. \n#> # … with 336,770 more rows\nflights2 %>%\n  select(-origin, -dest) %>% \n  mutate(name = airlines$name[match(carrier, airlines$carrier)])\n#> # A tibble: 336,776 x 7\n#>    year month   day  hour tailnum carrier name                  \n#>   <int> <int> <int> <dbl> <chr>   <chr>   <chr>                 \n#> 1  2013     1     1     5 N14228  UA      United Air Lines Inc. \n#> 2  2013     1     1     5 N24211  UA      United Air Lines Inc. \n#> 3  2013     1     1     5 N619AA  AA      American Airlines Inc.\n#> 4  2013     1     1     5 N804JB  B6      JetBlue Airways       \n#> 5  2013     1     1     6 N668DN  DL      Delta Air Lines Inc.  \n#> 6  2013     1     1     5 N39463  UA      United Air Lines Inc. \n#> # … with 336,770 more rows"},{"path":"relational-data.html","id":"understanding-joins","chapter":"13 Relational data","heading":"13.4.1 Understanding joins","text":"help learn joins work, ’m going use visual representation:coloured column represents “key” variable: used match rows tables. grey column represents “value” column carried along ride. examples ’ll show single key variable, idea generalises straightforward way multiple keys multiple values.join way connecting row x zero, one, rows y. following diagram shows potential match intersection pair lines.(look closely, might notice ’ve switched order key value columns x. emphasise joins match based key; value just carried along ride.)actual join, matches indicated dots. number dots = number matches = number rows output.","code":"\nx <- tribble(\n  ~key, ~val_x,\n     1, \"x1\",\n     2, \"x2\",\n     3, \"x3\"\n)\ny <- tribble(\n  ~key, ~val_y,\n     1, \"y1\",\n     2, \"y2\",\n     4, \"y3\"\n)"},{"path":"relational-data.html","id":"inner-join","chapter":"13 Relational data","heading":"13.4.2 Inner join","text":"simplest type join inner join. inner join matches pairs observations whenever keys equal:(precise, inner equijoin keys matched using equality operator. Since joins equijoins usually drop specification.)output inner join new data frame contains key, x values, y values. use tell dplyr variable key:important property inner join unmatched rows included result. means generally inner joins usually appropriate use analysis ’s easy lose observations.","code":"\nx %>% \n  inner_join(y, by = \"key\")\n#> # A tibble: 2 x 3\n#>     key val_x val_y\n#>   <dbl> <chr> <chr>\n#> 1     1 x1    y1   \n#> 2     2 x2    y2"},{"path":"relational-data.html","id":"outer-join","chapter":"13 Relational data","heading":"13.4.3 Outer joins","text":"inner join keeps observations appear tables. outer join keeps observations appear least one tables. three types outer joins:left join keeps observations x.right join keeps observations y.full join keeps observations x y.joins work adding additional “virtual” observation table. observation key always matches (key matches), value filled NA.Graphically, looks like:commonly used join left join: use whenever look additional data another table, preserves original observations even isn’t match. left join default join: use unless strong reason prefer one others.Another way depict different types joins Venn diagram:However, great representation. might jog memory join preserves observations table, suffers major limitation: Venn diagram can’t show happens keys don’t uniquely identify observation.","code":""},{"path":"relational-data.html","id":"join-matches","chapter":"13 Relational data","heading":"13.4.4 Duplicate keys","text":"far diagrams assumed keys unique. ’s always case. section explains happens keys unique. two possibilities:One table duplicate keys. useful want \nadd additional information typically one--many\nrelationship.\n\nNote ’ve put key column slightly different position\noutput. reflects key primary key y\nforeign key x.\n\nx <- tribble(\n  ~key, ~val_x,\n     1, \"x1\",\n     2, \"x2\",\n     2, \"x3\",\n     1, \"x4\"\n)\ny <- tribble(\n  ~key, ~val_y,\n     1, \"y1\",\n     2, \"y2\"\n)\nleft_join(x, y, = \"key\")\n#> # tibble: 4 x 3\n#>     key val_x val_y\n#>   <dbl> <chr> <chr>\n#> 1     1 x1    y1   \n#> 2     2 x2    y2   \n#> 3     2 x3    y2   \n#> 4     1 x4    y1One table duplicate keys. useful want \nadd additional information typically one--many\nrelationship.Note ’ve put key column slightly different position\noutput. reflects key primary key y\nforeign key x.tables duplicate keys. usually error \nneither table keys uniquely identify observation. join\nduplicated keys, get possible combinations, Cartesian product:\n\n\nx <- tribble(\n  ~key, ~val_x,\n     1, \"x1\",\n     2, \"x2\",\n     2, \"x3\",\n     3, \"x4\"\n)\ny <- tribble(\n  ~key, ~val_y,\n     1, \"y1\",\n     2, \"y2\",\n     2, \"y3\",\n     3, \"y4\"\n)\nleft_join(x, y, = \"key\")\n#> # tibble: 6 x 3\n#>     key val_x val_y\n#>   <dbl> <chr> <chr>\n#> 1     1 x1    y1   \n#> 2     2 x2    y2   \n#> 3     2 x2    y3   \n#> 4     2 x3    y2   \n#> 5     2 x3    y3   \n#> 6     3 x4    y4Both tables duplicate keys. usually error \nneither table keys uniquely identify observation. join\nduplicated keys, get possible combinations, Cartesian product:","code":"\nx <- tribble(\n  ~key, ~val_x,\n     1, \"x1\",\n     2, \"x2\",\n     2, \"x3\",\n     1, \"x4\"\n)\ny <- tribble(\n  ~key, ~val_y,\n     1, \"y1\",\n     2, \"y2\"\n)\nleft_join(x, y, by = \"key\")\n#> # A tibble: 4 x 3\n#>     key val_x val_y\n#>   <dbl> <chr> <chr>\n#> 1     1 x1    y1   \n#> 2     2 x2    y2   \n#> 3     2 x3    y2   \n#> 4     1 x4    y1\nx <- tribble(\n  ~key, ~val_x,\n     1, \"x1\",\n     2, \"x2\",\n     2, \"x3\",\n     3, \"x4\"\n)\ny <- tribble(\n  ~key, ~val_y,\n     1, \"y1\",\n     2, \"y2\",\n     2, \"y3\",\n     3, \"y4\"\n)\nleft_join(x, y, by = \"key\")\n#> # A tibble: 6 x 3\n#>     key val_x val_y\n#>   <dbl> <chr> <chr>\n#> 1     1 x1    y1   \n#> 2     2 x2    y2   \n#> 3     2 x2    y3   \n#> 4     2 x3    y2   \n#> 5     2 x3    y3   \n#> 6     3 x4    y4"},{"path":"relational-data.html","id":"join-by","chapter":"13 Relational data","heading":"13.4.5 Defining the key columns","text":"far, pairs tables always joined single variable, variable name tables. constraint encoded = \"key\". can use values connect tables ways:default, = NULL, uses variables appear tables,\ncalled natural join. example, flights weather tables\nmatch common variables: year, month, day, hour \norigin.\n\nflights2 %>% \n  left_join(weather)\n#> Joining, = c(\"year\", \"month\", \"day\", \"hour\", \"origin\")\n#> # tibble: 336,776 x 18\n#>    year month   day  hour origin dest  tailnum carrier  temp  dewp humid\n#>   <int> <int> <int> <dbl> <chr>  <chr> <chr>   <chr>   <dbl> <dbl> <dbl>\n#> 1  2013     1     1     5 EWR    IAH   N14228  UA       39.0  28.0  64.4\n#> 2  2013     1     1     5 LGA    IAH   N24211  UA       39.9  25.0  54.8\n#> 3  2013     1     1     5 JFK    MIA   N619AA  AA       39.0  27.0  61.6\n#> 4  2013     1     1     5 JFK    BQN   N804JB  B6       39.0  27.0  61.6\n#> 5  2013     1     1     6 LGA    ATL   N668DN  DL       39.9  25.0  54.8\n#> 6  2013     1     1     5 EWR    ORD   N39463  UA       39.0  28.0  64.4\n#> # … 336,770 rows, 7 variables: wind_dir <dbl>,\n#> #   wind_speed <dbl>, wind_gust <dbl>, precip <dbl>, pressure <dbl>,\n#> #   visib <dbl>, time_hour <dttm>default, = NULL, uses variables appear tables,\ncalled natural join. example, flights weather tables\nmatch common variables: year, month, day, hour \norigin.character vector, = \"x\". like natural join, uses \ncommon variables. example, flights planes \nyear variables, mean different things want join \ntailnum.\n\nflights2 %>% \n  left_join(planes, = \"tailnum\")\n#> # tibble: 336,776 x 16\n#>   year.x month   day  hour origin dest  tailnum carrier year.y type \n#>    <int> <int> <int> <dbl> <chr>  <chr> <chr>   <chr>    <int> <chr>\n#> 1   2013     1     1     5 EWR    IAH   N14228  UA        1999 Fixe…\n#> 2   2013     1     1     5 LGA    IAH   N24211  UA        1998 Fixe…\n#> 3   2013     1     1     5 JFK    MIA   N619AA  AA        1990 Fixe…\n#> 4   2013     1     1     5 JFK    BQN   N804JB  B6        2012 Fixe…\n#> 5   2013     1     1     6 LGA    ATL   N668DN  DL        1991 Fixe…\n#> 6   2013     1     1     5 EWR    ORD   N39463  UA        2012 Fixe…\n#> # … 336,770 rows, 6 variables: manufacturer <chr>,\n#> #   model <chr>, engines <int>, seats <int>, speed <int>, engine <chr>\nNote year variables (appear input data frames,\nconstrained equal) disambiguated output \nsuffix.character vector, = \"x\". like natural join, uses \ncommon variables. example, flights planes \nyear variables, mean different things want join \ntailnum.Note year variables (appear input data frames,\nconstrained equal) disambiguated output \nsuffix.named character vector: = c(\"\" = \"b\"). \nmatch variable table x variable b table y. \nvariables x used output.\nexample, want draw map need combine flights data\nairports data contains location (lat lon) \nairport. flight origin destination airport, \nneed specify one want join :\n\nflights2 %>% \n  left_join(airports, c(\"dest\" = \"faa\"))\n#> # tibble: 336,776 x 15\n#>    year month   day  hour origin dest  tailnum carrier name    lat   lon   alt\n#>   <int> <int> <int> <dbl> <chr>  <chr> <chr>   <chr>   <chr> <dbl> <dbl> <dbl>\n#> 1  2013     1     1     5 EWR    IAH   N14228  UA      Geor…  30.0 -95.3    97\n#> 2  2013     1     1     5 LGA    IAH   N24211  UA      Geor…  30.0 -95.3    97\n#> 3  2013     1     1     5 JFK    MIA   N619AA  AA      Miam…  25.8 -80.3     8\n#> 4  2013     1     1     5 JFK    BQN   N804JB  B6      <NA>   NA    NA      NA\n#> 5  2013     1     1     6 LGA    ATL   N668DN  DL      Hart…  33.6 -84.4  1026\n#> 6  2013     1     1     5 EWR    ORD   N39463  UA      Chic…  42.0 -87.9   668\n#> # … 336,770 rows, 3 variables: tz <dbl>, dst <chr>,\n#> #   tzone <chr>\n\nflights2 %>% \n  left_join(airports, c(\"origin\" = \"faa\"))\n#> # tibble: 336,776 x 15\n#>    year month   day  hour origin dest  tailnum carrier name    lat   lon   alt\n#>   <int> <int> <int> <dbl> <chr>  <chr> <chr>   <chr>   <chr> <dbl> <dbl> <dbl>\n#> 1  2013     1     1     5 EWR    IAH   N14228  UA      Newa…  40.7 -74.2    18\n#> 2  2013     1     1     5 LGA    IAH   N24211  UA      La G…  40.8 -73.9    22\n#> 3  2013     1     1     5 JFK    MIA   N619AA  AA      John…  40.6 -73.8    13\n#> 4  2013     1     1     5 JFK    BQN   N804JB  B6      John…  40.6 -73.8    13\n#> 5  2013     1     1     6 LGA    ATL   N668DN  DL      La G…  40.8 -73.9    22\n#> 6  2013     1     1     5 EWR    ORD   N39463  UA      Newa…  40.7 -74.2    18\n#> # … 336,770 rows, 3 variables: tz <dbl>, dst <chr>,\n#> #   tzone <chr>named character vector: = c(\"\" = \"b\"). \nmatch variable table x variable b table y. \nvariables x used output.example, want draw map need combine flights data\nairports data contains location (lat lon) \nairport. flight origin destination airport, \nneed specify one want join :","code":"\nflights2 %>% \n  left_join(weather)\n#> Joining, by = c(\"year\", \"month\", \"day\", \"hour\", \"origin\")\n#> # A tibble: 336,776 x 18\n#>    year month   day  hour origin dest  tailnum carrier  temp  dewp humid\n#>   <int> <int> <int> <dbl> <chr>  <chr> <chr>   <chr>   <dbl> <dbl> <dbl>\n#> 1  2013     1     1     5 EWR    IAH   N14228  UA       39.0  28.0  64.4\n#> 2  2013     1     1     5 LGA    IAH   N24211  UA       39.9  25.0  54.8\n#> 3  2013     1     1     5 JFK    MIA   N619AA  AA       39.0  27.0  61.6\n#> 4  2013     1     1     5 JFK    BQN   N804JB  B6       39.0  27.0  61.6\n#> 5  2013     1     1     6 LGA    ATL   N668DN  DL       39.9  25.0  54.8\n#> 6  2013     1     1     5 EWR    ORD   N39463  UA       39.0  28.0  64.4\n#> # … with 336,770 more rows, and 7 more variables: wind_dir <dbl>,\n#> #   wind_speed <dbl>, wind_gust <dbl>, precip <dbl>, pressure <dbl>,\n#> #   visib <dbl>, time_hour <dttm>\nflights2 %>% \n  left_join(planes, by = \"tailnum\")\n#> # A tibble: 336,776 x 16\n#>   year.x month   day  hour origin dest  tailnum carrier year.y type \n#>    <int> <int> <int> <dbl> <chr>  <chr> <chr>   <chr>    <int> <chr>\n#> 1   2013     1     1     5 EWR    IAH   N14228  UA        1999 Fixe…\n#> 2   2013     1     1     5 LGA    IAH   N24211  UA        1998 Fixe…\n#> 3   2013     1     1     5 JFK    MIA   N619AA  AA        1990 Fixe…\n#> 4   2013     1     1     5 JFK    BQN   N804JB  B6        2012 Fixe…\n#> 5   2013     1     1     6 LGA    ATL   N668DN  DL        1991 Fixe…\n#> 6   2013     1     1     5 EWR    ORD   N39463  UA        2012 Fixe…\n#> # … with 336,770 more rows, and 6 more variables: manufacturer <chr>,\n#> #   model <chr>, engines <int>, seats <int>, speed <int>, engine <chr>\nflights2 %>% \n  left_join(airports, c(\"dest\" = \"faa\"))\n#> # A tibble: 336,776 x 15\n#>    year month   day  hour origin dest  tailnum carrier name    lat   lon   alt\n#>   <int> <int> <int> <dbl> <chr>  <chr> <chr>   <chr>   <chr> <dbl> <dbl> <dbl>\n#> 1  2013     1     1     5 EWR    IAH   N14228  UA      Geor…  30.0 -95.3    97\n#> 2  2013     1     1     5 LGA    IAH   N24211  UA      Geor…  30.0 -95.3    97\n#> 3  2013     1     1     5 JFK    MIA   N619AA  AA      Miam…  25.8 -80.3     8\n#> 4  2013     1     1     5 JFK    BQN   N804JB  B6      <NA>   NA    NA      NA\n#> 5  2013     1     1     6 LGA    ATL   N668DN  DL      Hart…  33.6 -84.4  1026\n#> 6  2013     1     1     5 EWR    ORD   N39463  UA      Chic…  42.0 -87.9   668\n#> # … with 336,770 more rows, and 3 more variables: tz <dbl>, dst <chr>,\n#> #   tzone <chr>\n\nflights2 %>% \n  left_join(airports, c(\"origin\" = \"faa\"))\n#> # A tibble: 336,776 x 15\n#>    year month   day  hour origin dest  tailnum carrier name    lat   lon   alt\n#>   <int> <int> <int> <dbl> <chr>  <chr> <chr>   <chr>   <chr> <dbl> <dbl> <dbl>\n#> 1  2013     1     1     5 EWR    IAH   N14228  UA      Newa…  40.7 -74.2    18\n#> 2  2013     1     1     5 LGA    IAH   N24211  UA      La G…  40.8 -73.9    22\n#> 3  2013     1     1     5 JFK    MIA   N619AA  AA      John…  40.6 -73.8    13\n#> 4  2013     1     1     5 JFK    BQN   N804JB  B6      John…  40.6 -73.8    13\n#> 5  2013     1     1     6 LGA    ATL   N668DN  DL      La G…  40.8 -73.9    22\n#> 6  2013     1     1     5 EWR    ORD   N39463  UA      Newa…  40.7 -74.2    18\n#> # … with 336,770 more rows, and 3 more variables: tz <dbl>, dst <chr>,\n#> #   tzone <chr>"},{"path":"relational-data.html","id":"exercises-30","chapter":"13 Relational data","heading":"13.4.6 Exercises","text":"Compute average delay destination, join airports\ndata frame can show spatial distribution delays. ’s \neasy way draw map United States:\n\nairports %>%\n  semi_join(flights, c(\"faa\" = \"dest\")) %>%\n  ggplot(aes(lon, lat)) +\n    borders(\"state\") +\n    geom_point() +\n    coord_quickmap()\n(Don’t worry don’t understand semi_join() — ’ll\nlearn next.)\nmight want use size colour points display\naverage delay airport.Compute average delay destination, join airports\ndata frame can show spatial distribution delays. ’s \neasy way draw map United States:(Don’t worry don’t understand semi_join() — ’ll\nlearn next.)might want use size colour points display\naverage delay airport.Add location origin destination (.e. lat lon)\nflights.Add location origin destination (.e. lat lon)\nflights.relationship age plane delays?relationship age plane delays?weather conditions make likely see delay?weather conditions make likely see delay?happened June 13 2013? Display spatial pattern delays,\nuse Google cross-reference weather.happened June 13 2013? Display spatial pattern delays,\nuse Google cross-reference weather.","code":"\nairports %>%\n  semi_join(flights, c(\"faa\" = \"dest\")) %>%\n  ggplot(aes(lon, lat)) +\n    borders(\"state\") +\n    geom_point() +\n    coord_quickmap()"},{"path":"relational-data.html","id":"other-implementations","chapter":"13 Relational data","heading":"13.4.7 Other implementations","text":"base::merge() can perform four types mutating join:advantages specific dplyr verbs clearly convey intent code: difference joins really important concealed arguments merge(). dplyr’s joins considerably faster don’t mess order rows.SQL inspiration dplyr’s conventions, translation straightforward:Note “INNER” “OUTER” optional, often omitted.Joining different variables tables, e.g. inner_join(x, y, = c(\"\" = \"b\")) uses slightly different syntax SQL: SELECT * x INNER JOIN y x.= y.b. syntax suggests, SQL supports wider range join types dplyr can connect tables using constraints equality (sometimes called non-equijoins).","code":""},{"path":"relational-data.html","id":"filtering-joins","chapter":"13 Relational data","heading":"13.5 Filtering joins","text":"Filtering joins match observations way mutating joins, affect observations, variables. two types:semi_join(x, y) keeps observations x match y.anti_join(x, y) drops observations x match y.Semi-joins useful matching filtered summary tables back original rows. example, imagine ’ve found top ten popular destinations:Now want find flight went one destinations. construct filter :’s difficult extend approach multiple variables. example, imagine ’d found 10 days highest average delays. construct filter statement used year, month, day match back flights?Instead can use semi-join, connects two tables like mutating join, instead adding new columns, keeps rows x match y:Graphically, semi-join looks like :existence match important; doesn’t matter observation matched. means filtering joins never duplicate rows like mutating joins :inverse semi-join anti-join. anti-join keeps rows don’t match:Anti-joins useful diagnosing join mismatches. example, connecting flights planes, might interested know many flights don’t match planes:","code":"\ntop_dest <- flights %>%\n  count(dest, sort = TRUE) %>%\n  head(10)\ntop_dest\n#> # A tibble: 10 x 2\n#>   dest      n\n#>   <chr> <int>\n#> 1 ORD   17283\n#> 2 ATL   17215\n#> 3 LAX   16174\n#> 4 BOS   15508\n#> 5 MCO   14082\n#> 6 CLT   14064\n#> # … with 4 more rows\nflights %>% \n  filter(dest %in% top_dest$dest)\n#> # A tibble: 141,145 x 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      542            540         2      923            850\n#> 2  2013     1     1      554            600        -6      812            837\n#> 3  2013     1     1      554            558        -4      740            728\n#> 4  2013     1     1      555            600        -5      913            854\n#> 5  2013     1     1      557            600        -3      838            846\n#> 6  2013     1     1      558            600        -2      753            745\n#> # … with 141,139 more rows, and 11 more variables: arr_delay <dbl>,\n#> #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#> #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>\nflights %>% \n  semi_join(top_dest)\n#> Joining, by = \"dest\"\n#> # A tibble: 141,145 x 19\n#>    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n#>   <int> <int> <int>    <int>          <int>     <dbl>    <int>          <int>\n#> 1  2013     1     1      542            540         2      923            850\n#> 2  2013     1     1      554            600        -6      812            837\n#> 3  2013     1     1      554            558        -4      740            728\n#> 4  2013     1     1      555            600        -5      913            854\n#> 5  2013     1     1      557            600        -3      838            846\n#> 6  2013     1     1      558            600        -2      753            745\n#> # … with 141,139 more rows, and 11 more variables: arr_delay <dbl>,\n#> #   carrier <chr>, flight <int>, tailnum <chr>, origin <chr>, dest <chr>,\n#> #   air_time <dbl>, distance <dbl>, hour <dbl>, minute <dbl>, time_hour <dttm>\nflights %>%\n  anti_join(planes, by = \"tailnum\") %>%\n  count(tailnum, sort = TRUE)\n#> # A tibble: 722 x 2\n#>   tailnum     n\n#>   <chr>   <int>\n#> 1 <NA>     2512\n#> 2 N725MQ    575\n#> 3 N722MQ    513\n#> 4 N723MQ    507\n#> 5 N713MQ    483\n#> 6 N735MQ    396\n#> # … with 716 more rows"},{"path":"relational-data.html","id":"exercises-31","chapter":"13 Relational data","heading":"13.5.1 Exercises","text":"mean flight missing tailnum? \ntail numbers don’t matching record planes common?\n(Hint: one variable explains ~90% problems.)mean flight missing tailnum? \ntail numbers don’t matching record planes common?\n(Hint: one variable explains ~90% problems.)Filter flights show flights planes flown least 100\nflights.Filter flights show flights planes flown least 100\nflights.Combine fueleconomy::vehicles fueleconomy::common find \nrecords common models.Combine fueleconomy::vehicles fueleconomy::common find \nrecords common models.Find 48 hours (course whole year) worst\ndelays. Cross-reference weather data. Can see \npatterns?Find 48 hours (course whole year) worst\ndelays. Cross-reference weather data. Can see \npatterns?anti_join(flights, airports, = c(\"dest\" = \"faa\")) tell ?\nanti_join(airports, flights, = c(\"faa\" = \"dest\")) tell ?anti_join(flights, airports, = c(\"dest\" = \"faa\")) tell ?\nanti_join(airports, flights, = c(\"faa\" = \"dest\")) tell ?might expect ’s implicit relationship plane\nairline, plane flown single airline. Confirm\nreject hypothesis using tools ’ve learned .might expect ’s implicit relationship plane\nairline, plane flown single airline. Confirm\nreject hypothesis using tools ’ve learned .","code":""},{"path":"relational-data.html","id":"join-problems","chapter":"13 Relational data","heading":"13.6 Join problems","text":"data ’ve working chapter cleaned ’ll problems possible. data unlikely nice, things data make joins go smoothly.Start identifying variables form primary key table.\nusually based understanding data, \nempirically looking combination variables give \nunique identifier. just look variables without thinking \nmean, might get (un)lucky find combination ’s\nunique current data relationship might true \ngeneral.\nexample, altitude longitude uniquely identify airport,\ngood identifiers!\n\nairports %>% count(alt, lon) %>% filter(n > 1)\n#> # tibble: 0 x 3\n#> # … 3 variables: alt <dbl>, lon <dbl>, n <int>Start identifying variables form primary key table.\nusually based understanding data, \nempirically looking combination variables give \nunique identifier. just look variables without thinking \nmean, might get (un)lucky find combination ’s\nunique current data relationship might true \ngeneral.example, altitude longitude uniquely identify airport,\ngood identifiers!Check none variables primary key missing. \nvalue missing can’t identify observation!Check none variables primary key missing. \nvalue missing can’t identify observation!Check foreign keys match primary keys another table. \nbest way anti_join(). ’s common keys\nmatch data entry errors. Fixing often lot \nwork.\nmissing keys, ’ll need thoughtful \nuse inner vs. outer joins, carefully considering whether \nwant drop rows don’t match.Check foreign keys match primary keys another table. \nbest way anti_join(). ’s common keys\nmatch data entry errors. Fixing often lot \nwork.missing keys, ’ll need thoughtful \nuse inner vs. outer joins, carefully considering whether \nwant drop rows don’t match.aware simply checking number rows join sufficient ensure join gone smoothly. inner join duplicate keys tables, might get unlucky number dropped rows might exactly equal number duplicated rows!","code":"\nairports %>% count(alt, lon) %>% filter(n > 1)\n#> # A tibble: 0 x 3\n#> # … with 3 variables: alt <dbl>, lon <dbl>, n <int>"},{"path":"relational-data.html","id":"set-operations","chapter":"13 Relational data","heading":"13.7 Set operations","text":"final type two-table verb set operations. Generally, use least frequently, occasionally useful want break single complex filter simpler pieces. operations work complete row, comparing values every variable. expect x y inputs variables, treat observations like sets:intersect(x, y): return observations x y.union(x, y): return unique observations x y.setdiff(x, y): return observations x, y.Given simple data:four possibilities :","code":"\ndf1 <- tribble(\n  ~x, ~y,\n   1,  1,\n   2,  1\n)\ndf2 <- tribble(\n  ~x, ~y,\n   1,  1,\n   1,  2\n)\nintersect(df1, df2)\n#> # A tibble: 1 x 2\n#>       x     y\n#>   <dbl> <dbl>\n#> 1     1     1\n\n# Note that we get 3 rows, not 4\nunion(df1, df2)\n#> # A tibble: 3 x 2\n#>       x     y\n#>   <dbl> <dbl>\n#> 1     1     1\n#> 2     2     1\n#> 3     1     2\n\nsetdiff(df1, df2)\n#> # A tibble: 1 x 2\n#>       x     y\n#>   <dbl> <dbl>\n#> 1     2     1\n\nsetdiff(df2, df1)\n#> # A tibble: 1 x 2\n#>       x     y\n#>   <dbl> <dbl>\n#> 1     1     2"},{"path":"strings.html","id":"strings","chapter":"14 Strings","heading":"14 Strings","text":"","code":""},{"path":"strings.html","id":"introduction-8","chapter":"14 Strings","heading":"14.1 Introduction","text":"chapter introduces string manipulation R. ’ll learn basics strings work create hand, focus chapter regular expressions, regexps short. Regular expressions useful strings usually contain unstructured semi-structured data, regexps concise language describing patterns strings. first look regexp, ’ll think cat walked across keyboard, understanding improves soon start make sense.","code":""},{"path":"strings.html","id":"prerequisites-8","chapter":"14 Strings","heading":"14.1.1 Prerequisites","text":"chapter focus stringr package string manipulation, part core tidyverse.","code":"\nlibrary(tidyverse)"},{"path":"strings.html","id":"string-basics","chapter":"14 Strings","heading":"14.2 String basics","text":"can create strings either single quotes double quotes. Unlike languages, difference behaviour. recommend always using \", unless want create string contains multiple \".forget close quote, ’ll see +, continuation character:happen , press Escape try !include literal single double quote string can use \\ “escape” :means want include literal backslash, ’ll need double : \"\\\\\".Beware printed representation string string , printed representation shows escapes. see raw contents string, use writeLines():handful special characters. common \"\\n\", newline, \"\\t\", tab, can see complete list requesting help \": ?'\"', ?\"'\". ’ll also sometimes see strings like \"\\u00b5\", way writing non-English characters works platforms:Multiple strings often stored character vector, can create c():","code":"\nstring1 <- \"This is a string\"\nstring2 <- 'If I want to include a \"quote\" inside a string, I use single quotes'> \"This is a string without a closing quote\n+ \n+ \n+ HELP I'M STUCK\ndouble_quote <- \"\\\"\" # or '\"'\nsingle_quote <- '\\'' # or \"'\"\nx <- c(\"\\\"\", \"\\\\\")\nx\n#> [1] \"\\\"\" \"\\\\\"\nwriteLines(x)\n#> \"\n#> \\\nx <- \"\\u00b5\"\nx\n#> [1] \"µ\"\nc(\"one\", \"two\", \"three\")\n#> [1] \"one\"   \"two\"   \"three\""},{"path":"strings.html","id":"string-length","chapter":"14 Strings","heading":"14.2.1 String length","text":"Base R contains many functions work strings ’ll avoid can inconsistent, makes hard remember. Instead ’ll use functions stringr. intuitive names, start str_. example, str_length() tells number characters string:common str_ prefix particularly useful use RStudio, typing str_ trigger autocomplete, allowing see stringr functions:","code":"\nstr_length(c(\"a\", \"R for data science\", NA))\n#> [1]  1 18 NA"},{"path":"strings.html","id":"combining-strings","chapter":"14 Strings","heading":"14.2.2 Combining strings","text":"combine two strings, use str_c():Use sep argument control ’re separated:Like functions R, missing values contagious. want print \"NA\", use str_replace_na():shown , str_c() vectorised, automatically recycles shorter vectors length longest:Objects length 0 silently dropped. particularly useful conjunction :collapse vector strings single string, use collapse:","code":"\nstr_c(\"x\", \"y\")\n#> [1] \"xy\"\nstr_c(\"x\", \"y\", \"z\")\n#> [1] \"xyz\"\nstr_c(\"x\", \"y\", sep = \", \")\n#> [1] \"x, y\"\nx <- c(\"abc\", NA)\nstr_c(\"|-\", x, \"-|\")\n#> [1] \"|-abc-|\" NA\nstr_c(\"|-\", str_replace_na(x), \"-|\")\n#> [1] \"|-abc-|\" \"|-NA-|\"\nstr_c(\"prefix-\", c(\"a\", \"b\", \"c\"), \"-suffix\")\n#> [1] \"prefix-a-suffix\" \"prefix-b-suffix\" \"prefix-c-suffix\"\nname <- \"Hadley\"\ntime_of_day <- \"morning\"\nbirthday <- FALSE\n\nstr_c(\n  \"Good \", time_of_day, \" \", name,\n  if (birthday) \" and HAPPY BIRTHDAY\",\n  \".\"\n)\n#> [1] \"Good morning Hadley.\"\nstr_c(c(\"x\", \"y\", \"z\"), collapse = \", \")\n#> [1] \"x, y, z\""},{"path":"strings.html","id":"subsetting-strings","chapter":"14 Strings","heading":"14.2.3 Subsetting strings","text":"can extract parts string using str_sub(). well string, str_sub() takes start end arguments give (inclusive) position substring:Note str_sub() won’t fail string short: just return much possible:can also use assignment form str_sub() modify strings:","code":"\nx <- c(\"Apple\", \"Banana\", \"Pear\")\nstr_sub(x, 1, 3)\n#> [1] \"App\" \"Ban\" \"Pea\"\n# negative numbers count backwards from end\nstr_sub(x, -3, -1)\n#> [1] \"ple\" \"ana\" \"ear\"\nstr_sub(\"a\", 1, 5)\n#> [1] \"a\"\nstr_sub(x, 1, 1) <- str_to_lower(str_sub(x, 1, 1))\nx\n#> [1] \"apple\"  \"banana\" \"pear\""},{"path":"strings.html","id":"locales","chapter":"14 Strings","heading":"14.2.4 Locales","text":"used str_to_lower() change text lower case. can also use str_to_upper() str_to_title(). However, changing case complicated might first appear different languages different rules changing case. can pick set rules use specifying locale:locale specified ISO 639 language code, two three letter abbreviation. don’t already know code language, Wikipedia good list. leave locale blank, use current locale, provided operating system.Another important operation ’s affected locale sorting. base R order() sort() functions sort strings using current locale. want robust behaviour across different computers, may want use str_sort() str_order() take additional locale argument:","code":"\n# Turkish has two i's: with and without a dot, and it\n# has a different rule for capitalising them:\nstr_to_upper(c(\"i\", \"ı\"))\n#> [1] \"I\" \"I\"\nstr_to_upper(c(\"i\", \"ı\"), locale = \"tr\")\n#> [1] \"İ\" \"I\"\nx <- c(\"apple\", \"eggplant\", \"banana\")\n\nstr_sort(x, locale = \"en\")  # English\n#> [1] \"apple\"    \"banana\"   \"eggplant\"\n\nstr_sort(x, locale = \"haw\") # Hawaiian\n#> [1] \"apple\"    \"eggplant\" \"banana\""},{"path":"strings.html","id":"exercises-32","chapter":"14 Strings","heading":"14.2.5 Exercises","text":"code doesn’t use stringr, ’ll often see paste() paste0().\n’s difference two functions? stringr function \nequivalent ? functions differ handling \nNA?code doesn’t use stringr, ’ll often see paste() paste0().\n’s difference two functions? stringr function \nequivalent ? functions differ handling \nNA?words, describe difference sep collapse\narguments str_c().words, describe difference sep collapse\narguments str_c().Use str_length() str_sub() extract middle character \nstring. string even number characters?Use str_length() str_sub() extract middle character \nstring. string even number characters?str_wrap() ? might want use ?str_wrap() ? might want use ?str_trim() ? ’s opposite str_trim()?str_trim() ? ’s opposite str_trim()?Write function turns (e.g.) vector c(\"\", \"b\", \"c\") \nstring , b, c. Think carefully \ngiven vector length 0, 1, 2.Write function turns (e.g.) vector c(\"\", \"b\", \"c\") \nstring , b, c. Think carefully \ngiven vector length 0, 1, 2.","code":""},{"path":"strings.html","id":"matching-patterns-with-regular-expressions","chapter":"14 Strings","heading":"14.3 Matching patterns with regular expressions","text":"Regexps terse language allow describe patterns strings. take little get head around, understand , ’ll find extremely useful.learn regular expressions, ’ll use str_view() str_view_all(). functions take character vector regular expression, show match. ’ll start simple regular expressions gradually get complicated. ’ve mastered pattern matching, ’ll learn apply ideas various stringr functions.","code":""},{"path":"strings.html","id":"basic-matches","chapter":"14 Strings","heading":"14.3.1 Basic matches","text":"simplest patterns match exact strings:next step complexity ., matches character (except newline):“.” matches character, match character “.”? need use “escape” tell regular expression want match exactly, use special behaviour. Like strings, regexps use backslash, \\, escape special behaviour. match ., need regexp \\.. Unfortunately creates problem. use strings represent regular expressions, \\ also used escape symbol strings. create regular expression \\. need string \"\\\\.\".\\ used escape character regular expressions, match literal \\? Well need escape , creating regular expression \\\\. create regular expression, need use string, also needs escape \\. means match literal \\ need write \"\\\\\\\\\" — need four backslashes match one!book, ’ll write regular expression \\. strings represent regular expression \"\\\\.\".","code":"\nx <- c(\"apple\", \"banana\", \"pear\")\nstr_view(x, \"an\")\nstr_view(x, \".a.\")\n# To create the regular expression, we need \\\\\ndot <- \"\\\\.\"\n\n# But the expression itself only contains one:\nwriteLines(dot)\n#> \\.\n\n# And this tells R to look for an explicit .\nstr_view(c(\"abc\", \"a.c\", \"bef\"), \"a\\\\.c\")\nx <- \"a\\\\b\"\nwriteLines(x)\n#> a\\b\n\nstr_view(x, \"\\\\\\\\\")"},{"path":"strings.html","id":"exercises-33","chapter":"14 Strings","heading":"14.3.1.1 Exercises","text":"Explain strings don’t match \\: \"\\\", \"\\\\\", \"\\\\\\\".Explain strings don’t match \\: \"\\\", \"\\\\\", \"\\\\\\\".match sequence \"'\\?match sequence \"'\\?patterns regular expression \\..\\..\\.. match?\nrepresent string?patterns regular expression \\..\\..\\.. match?\nrepresent string?","code":""},{"path":"strings.html","id":"anchors","chapter":"14 Strings","heading":"14.3.2 Anchors","text":"default, regular expressions match part string. ’s often useful anchor regular expression matches start end string. can use:^ match start string.$ match end string.remember , try mnemonic learned Evan Misshula: begin power (^), end money ($).force regular expression match complete string, anchor ^ $:can also match boundary words \\b. don’t often use R, sometimes use ’m search RStudio want find name function ’s component functions. example, ’ll search \\bsum\\b avoid matching summarise, summary, rowsum .","code":"\nx <- c(\"apple\", \"banana\", \"pear\")\nstr_view(x, \"^a\")\nx <- c(\"apple pie\", \"apple\", \"apple cake\")\nstr_view(x, \"apple\")"},{"path":"strings.html","id":"exercises-34","chapter":"14 Strings","heading":"14.3.2.1 Exercises","text":"match literal string \"$^$\"?match literal string \"$^$\"?Given corpus common words stringr::words, create regular\nexpressions find words :\nStart “y”.\nEnd “x”\nexactly three letters long. (Don’t cheat using str_length()!)\nseven letters .\nSince list long, might want use match argument \nstr_view() show matching non-matching words.Given corpus common words stringr::words, create regular\nexpressions find words :Start “y”.End “x”exactly three letters long. (Don’t cheat using str_length()!)seven letters .Since list long, might want use match argument \nstr_view() show matching non-matching words.","code":""},{"path":"strings.html","id":"character-classes-and-alternatives","chapter":"14 Strings","heading":"14.3.3 Character classes and alternatives","text":"number special patterns match one character. ’ve already seen ., matches character apart newline. four useful tools:\\d: matches digit.\\s: matches whitespace (e.g. space, tab, newline).[abc]: matches , b, c.[^abc]: matches anything except , b, c.Remember, create regular expression containing \\d \\s, ’ll need escape \\ string, ’ll type \"\\\\d\" \"\\\\s\".character class containing single character nice alternative backslash escapes want include single metacharacter regex. Many people find readable.works () regex metacharacters: $ . | ? * + ( ) [ {. Unfortunately, characters special meaning even inside character class must handled backslash escapes: ] \\ ^ -.can use alternation pick one alternative patterns. example, abc|d..f match either ‘“abc”’, \"deaf\". Note precedence | low, abc|xyz matches abc xyz abcyz abxyz. Like mathematical expressions, precedence ever gets confusing, use parentheses make clear want:","code":"\n# Look for a literal character that normally has special meaning in a regex\nstr_view(c(\"abc\", \"a.c\", \"a*c\", \"a c\"), \"a[.]c\")\nstr_view(c(\"grey\", \"gray\"), \"gr(e|a)y\")"},{"path":"strings.html","id":"exercises-35","chapter":"14 Strings","heading":"14.3.3.1 Exercises","text":"Create regular expressions find words :\nStart vowel.\ncontain consonants. (Hint: thinking matching\n“”-vowels.)\nEnd ed, eed.\nEnd ing ise.\nCreate regular expressions find words :Start vowel.Start vowel.contain consonants. (Hint: thinking matching\n“”-vowels.)contain consonants. (Hint: thinking matching\n“”-vowels.)End ed, eed.End ed, eed.End ing ise.End ing ise.Empirically verify rule “e except c”.Empirically verify rule “e except c”.“q” always followed “u”?“q” always followed “u”?Write regular expression matches word ’s probably written\nBritish English, American English.Write regular expression matches word ’s probably written\nBritish English, American English.Create regular expression match telephone numbers commonly\nwritten country.Create regular expression match telephone numbers commonly\nwritten country.","code":""},{"path":"strings.html","id":"repetition","chapter":"14 Strings","heading":"14.3.4 Repetition","text":"next step power involves controlling many times pattern matches:?: 0 1+: 1 *: 0 moreNote precedence operators high, can write: colou?r match either American British spellings. means uses need parentheses, like bana(na)+.can also specify number matches precisely:{n}: exactly n{n,}: n {,m}: m{n,m}: n mBy default matches “greedy”: match longest string possible. can make “lazy”, matching shortest string possible putting ? . advanced feature regular expressions, ’s useful know exists:","code":"\nx <- \"1888 is the longest year in Roman numerals: MDCCCLXXXVIII\"\nstr_view(x, \"CC?\")\nstr_view(x, \"C{2}\")\nstr_view(x, 'C{2,3}?')"},{"path":"strings.html","id":"exercises-36","chapter":"14 Strings","heading":"14.3.4.1 Exercises","text":"Describe equivalents ?, +, * {m,n} form.Describe equivalents ?, +, * {m,n} form.Describe words regular expressions match:\n(read carefully see ’m using regular expression string\ndefines regular expression.)\n^.*$\n\"\\\\{.+\\\\}\"\n\\d{4}-\\d{2}-\\d{2}\n\"\\\\\\\\{4}\"\nDescribe words regular expressions match:\n(read carefully see ’m using regular expression string\ndefines regular expression.)^.*$\"\\\\{.+\\\\}\"\\d{4}-\\d{2}-\\d{2}\"\\\\\\\\{4}\"Create regular expressions find words :\nStart three consonants.\nthree vowels row.\ntwo vowel-consonant pairs row.\nCreate regular expressions find words :Start three consonants.three vowels row.two vowel-consonant pairs row.Solve beginner regexp crosswords \nhttps://regexcrossword.com/challenges/beginner.Solve beginner regexp crosswords \nhttps://regexcrossword.com/challenges/beginner.","code":""},{"path":"strings.html","id":"grouping-and-backreferences","chapter":"14 Strings","heading":"14.3.5 Grouping and backreferences","text":"Earlier, learned parentheses way disambiguate complex expressions. Parentheses also create numbered capturing group (number 1, 2 etc.). capturing group stores part string matched part regular expression inside parentheses. can refer text previously matched capturing group backreferences, like \\1, \\2 etc. example, following regular expression finds fruits repeated pair letters.(Shortly, ’ll also see ’re useful conjunction str_match().)","code":"\nstr_view(fruit, \"(..)\\\\1\", match = TRUE)"},{"path":"strings.html","id":"exercises-37","chapter":"14 Strings","heading":"14.3.5.1 Exercises","text":"Describe, words, expressions match:\n(.)\\1\\1\n\"(.)(.)\\\\2\\\\1\"\n(..)\\1\n\"(.).\\\\1.\\\\1\"\n\"(.)(.)(.).*\\\\3\\\\2\\\\1\"\nDescribe, words, expressions match:(.)\\1\\1\"(.)(.)\\\\2\\\\1\"(..)\\1\"(.).\\\\1.\\\\1\"\"(.)(.)(.).*\\\\3\\\\2\\\\1\"Construct regular expressions match words :\nStart end character.\nContain repeated pair letters\n(e.g. “church” contains “ch” repeated twice.)\nContain one letter repeated least three places\n(e.g. “eleven” contains three “e”s.)\nConstruct regular expressions match words :Start end character.Start end character.Contain repeated pair letters\n(e.g. “church” contains “ch” repeated twice.)Contain repeated pair letters\n(e.g. “church” contains “ch” repeated twice.)Contain one letter repeated least three places\n(e.g. “eleven” contains three “e”s.)Contain one letter repeated least three places\n(e.g. “eleven” contains three “e”s.)","code":""},{"path":"strings.html","id":"tools","chapter":"14 Strings","heading":"14.4 Tools","text":"Now ’ve learned basics regular expressions, ’s time learn apply real problems. section ’ll learn wide array stringr functions let :Determine strings match pattern.Find positions matches.Extract content matches.Replace matches new values.Split string based match.word caution continue: regular expressions powerful, ’s easy try solve every problem single regular expression. words Jamie Zawinski:people, confronted problem, think “know, ’ll use regular\nexpressions.” Now two problems.cautionary tale, check regular expression checks email address valid:somewhat pathological example (email addresses actually surprisingly complex), used real code. See Stack Overflow discussion http://stackoverflow.com//201378 details.Don’t forget ’re programming language tools disposal. Instead creating one complex regular expression, ’s often easier write series simpler regexps. get stuck trying create single regexp solves problem, take step back think break problem smaller pieces, solving challenge moving onto next one.","code":"(?:(?:\\r\\n)?[ \\t])*(?:(?:(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t]\n)+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\"(?:[^\\\"\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*\"(?:(?:\n\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(\n?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\"(?:[^\\\"\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \n\\t]))*\"(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\0\n31]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\\n](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+\n(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:\n(?:\\r\\n)?[ \\t])*))*|(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z\n|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\"(?:[^\\\"\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*\"(?:(?:\\r\\n)\n?[ \\t])*)*\\<(?:(?:\\r\\n)?[ \\t])*(?:@(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\\nr\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[\n \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)\n?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t]\n)*))*(?:,@(?:(?:\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[\n \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*\n)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t]\n)+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*)\n*:(?:(?:\\r\\n)?[ \\t])*)?(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+\n|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\"(?:[^\\\"\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*\"(?:(?:\\r\n\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\n\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\"(?:[^\\\"\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t\n]))*\"(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031\n]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](\n?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?\n:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?\n:\\r\\n)?[ \\t])*))*\\>(?:(?:\\r\\n)?[ \\t])*)|(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?\n:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\"(?:[^\\\"\\r\\\\]|\\\\.|(?:(?:\\r\\n)?\n[ \\t]))*\"(?:(?:\\r\\n)?[ \\t])*)*:(?:(?:\\r\\n)?[ \\t])*(?:(?:(?:[^()<>@,;:\\\\\".\\[\\] \n\\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\"(?:[^\\\"\\r\\\\]|\n\\\\.|(?:(?:\\r\\n)?[ \\t]))*\"(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()<>\n@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\"\n(?:[^\\\"\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*\"(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t]\n)*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\n\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?\n:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\n\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*|(?:[^()<>@,;:\\\\\".\\[\\] \\000-\n\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\"(?:[^\\\"\\r\\\\]|\\\\.|(\n?:(?:\\r\\n)?[ \\t]))*\"(?:(?:\\r\\n)?[ \\t])*)*\\<(?:(?:\\r\\n)?[ \\t])*(?:@(?:[^()<>@,;\n:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\\[([\n^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\"\n.\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\\[([^\\[\\\n]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*(?:,@(?:(?:\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\".\\\n[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\\[([^\\[\\]\\\nr\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \n\\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]\n|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*)*:(?:(?:\\r\\n)?[ \\t])*)?(?:[^()<>@,;:\\\\\".\\[\\] \\0\n00-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\"(?:[^\\\"\\r\\\\]|\\\\\n.|(?:(?:\\r\\n)?[ \\t]))*\"(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()<>@,\n;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\"(?\n:[^\\\"\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*\"(?:(?:\\r\\n)?[ \\t])*))*@(?:(?:\\r\\n)?[ \\t])*\n(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\n\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t])*(?:[\n^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]\n]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*\\>(?:(?:\\r\\n)?[ \\t])*)(?:,\\s*(\n?:(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\n\".\\[\\]]))|\"(?:[^\\\"\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*\"(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(\n?:\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\n\\[\"()<>@,;:\\\\\".\\[\\]]))|\"(?:[^\\\"\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*\"(?:(?:\\r\\n)?[ \\t\n])*))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t\n])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?\n:\\.(?:(?:\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\n\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*|(?:\n[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\\n]]))|\"(?:[^\\\"\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*\"(?:(?:\\r\\n)?[ \\t])*)*\\<(?:(?:\\r\\n)\n?[ \\t])*(?:@(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"\n()<>@,;:\\\\\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)\n?[ \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>\n@,;:\\\\\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*(?:,@(?:(?:\\r\\n)?[\n \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,\n;:\\\\\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\\r\\n)?[ \\t]\n)*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\n\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*)*:(?:(?:\\r\\n)?[ \\t])*)?\n(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\n\\[\\]]))|\"(?:[^\\\"\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*\"(?:(?:\\r\\n)?[ \\t])*)(?:\\.(?:(?:\n\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z|(?=[\\[\n\"()<>@,;:\\\\\".\\[\\]]))|\"(?:[^\\\"\\r\\\\]|\\\\.|(?:(?:\\r\\n)?[ \\t]))*\"(?:(?:\\r\\n)?[ \\t])\n*))*@(?:(?:\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])\n+|\\Z|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*)(?:\\\n.(?:(?:\\r\\n)?[ \\t])*(?:[^()<>@,;:\\\\\".\\[\\] \\000-\\031]+(?:(?:(?:\\r\\n)?[ \\t])+|\\Z\n|(?=[\\[\"()<>@,;:\\\\\".\\[\\]]))|\\[([^\\[\\]\\r\\\\]|\\\\.)*\\](?:(?:\\r\\n)?[ \\t])*))*\\>(?:(\n?:\\r\\n)?[ \\t])*))*)?;\\s*)"},{"path":"strings.html","id":"detect-matches","chapter":"14 Strings","heading":"14.4.1 Detect matches","text":"determine character vector matches pattern, use str_detect(). returns logical vector length input:Remember use logical vector numeric context, FALSE becomes 0 TRUE becomes 1. makes sum() mean() useful want answer questions matches across larger vector:complex logical conditions (e.g. match b c unless d) ’s often easier combine multiple str_detect() calls logical operators, rather trying create single regular expression. example, two ways find words don’t contain vowels:results identical, think first approach significantly easier understand. regular expression gets overly complicated, try breaking smaller pieces, giving piece name, combining pieces logical operations.common use str_detect() select elements match pattern. can logical subsetting, convenient str_subset() wrapper:Typically, however, strings one column data frame, ’ll want use filter instead:variation str_detect() str_count(): rather simple yes , tells many matches string:’s natural use str_count() mutate():Note matches never overlap. example, \"abababa\", many times pattern \"aba\" match? Regular expressions say two, three:Note use str_view_all(). ’ll shortly learn, many stringr functions come pairs: one function works single match, works matches. second function suffix _all.","code":"\nx <- c(\"apple\", \"banana\", \"pear\")\nstr_detect(x, \"e\")\n#> [1]  TRUE FALSE  TRUE\n# How many common words start with t?\nsum(str_detect(words, \"^t\"))\n#> [1] 65\n# What proportion of common words end with a vowel?\nmean(str_detect(words, \"[aeiou]$\"))\n#> [1] 0.2765306\n# Find all words containing at least one vowel, and negate\nno_vowels_1 <- !str_detect(words, \"[aeiou]\")\n# Find all words consisting only of consonants (non-vowels)\nno_vowels_2 <- str_detect(words, \"^[^aeiou]+$\")\nidentical(no_vowels_1, no_vowels_2)\n#> [1] TRUE\nwords[str_detect(words, \"x$\")]\n#> [1] \"box\" \"sex\" \"six\" \"tax\"\nstr_subset(words, \"x$\")\n#> [1] \"box\" \"sex\" \"six\" \"tax\"\ndf <- tibble(\n  word = words, \n  i = seq_along(word)\n)\ndf %>% \n  filter(str_detect(word, \"x$\"))\n#> # A tibble: 4 x 2\n#>   word      i\n#>   <chr> <int>\n#> 1 box     108\n#> 2 sex     747\n#> 3 six     772\n#> 4 tax     841\nx <- c(\"apple\", \"banana\", \"pear\")\nstr_count(x, \"a\")\n#> [1] 1 3 1\n\n# On average, how many vowels per word?\nmean(str_count(words, \"[aeiou]\"))\n#> [1] 1.991837\ndf %>% \n  mutate(\n    vowels = str_count(word, \"[aeiou]\"),\n    consonants = str_count(word, \"[^aeiou]\")\n  )\n#> # A tibble: 980 x 4\n#>   word         i vowels consonants\n#>   <chr>    <int>  <int>      <int>\n#> 1 a            1      1          0\n#> 2 able         2      2          2\n#> 3 about        3      3          2\n#> 4 absolute     4      4          4\n#> 5 accept       5      2          4\n#> 6 account      6      3          4\n#> # … with 974 more rows\nstr_count(\"abababa\", \"aba\")\n#> [1] 2\nstr_view_all(\"abababa\", \"aba\")"},{"path":"strings.html","id":"exercises-38","chapter":"14 Strings","heading":"14.4.1.1 Exercises","text":"following challenges, try solving using single\nregular expression, combination multiple str_detect() calls.\nFind words start end x.\nFind words start vowel end consonant.\nwords contain least one different\nvowel?\nfollowing challenges, try solving using single\nregular expression, combination multiple str_detect() calls.Find words start end x.Find words start end x.Find words start vowel end consonant.Find words start vowel end consonant.words contain least one different\nvowel?words contain least one different\nvowel?word highest number vowels? word highest\nproportion vowels? (Hint: denominator?)word highest number vowels? word highest\nproportion vowels? (Hint: denominator?)","code":""},{"path":"strings.html","id":"extract-matches","chapter":"14 Strings","heading":"14.4.2 Extract matches","text":"extract actual text match, use str_extract(). show , ’re going need complicated example. ’m going use Harvard sentences, designed test VOIP systems, also useful practicing regexps. provided stringr::sentences:Imagine want find sentences contain colour. first create vector colour names, turn single regular expression:Now can select sentences contain colour, extract colour figure one :Note str_extract() extracts first match. can see easily first selecting sentences 1 match:common pattern stringr functions, working single match allows use much simpler data structures. get matches, use str_extract_all(). returns list:’ll learn lists lists iteration.use simplify = TRUE, str_extract_all() return matrix short matches expanded length longest:","code":"\nlength(sentences)\n#> [1] 720\nhead(sentences)\n#> [1] \"The birch canoe slid on the smooth planks.\" \n#> [2] \"Glue the sheet to the dark blue background.\"\n#> [3] \"It's easy to tell the depth of a well.\"     \n#> [4] \"These days a chicken leg is a rare dish.\"   \n#> [5] \"Rice is often served in round bowls.\"       \n#> [6] \"The juice of lemons makes fine punch.\"\ncolours <- c(\"red\", \"orange\", \"yellow\", \"green\", \"blue\", \"purple\")\ncolour_match <- str_c(colours, collapse = \"|\")\ncolour_match\n#> [1] \"red|orange|yellow|green|blue|purple\"\nhas_colour <- str_subset(sentences, colour_match)\nmatches <- str_extract(has_colour, colour_match)\nhead(matches)\n#> [1] \"blue\" \"blue\" \"red\"  \"red\"  \"red\"  \"blue\"\nmore <- sentences[str_count(sentences, colour_match) > 1]\nstr_view_all(more, colour_match)\nstr_extract_all(more, colour_match)\n#> [[1]]\n#> [1] \"blue\" \"red\" \n#> \n#> [[2]]\n#> [1] \"green\" \"red\"  \n#> \n#> [[3]]\n#> [1] \"orange\" \"red\"\nstr_extract_all(more, colour_match, simplify = TRUE)\n#>      [,1]     [,2] \n#> [1,] \"blue\"   \"red\"\n#> [2,] \"green\"  \"red\"\n#> [3,] \"orange\" \"red\"\n\nx <- c(\"a\", \"a b\", \"a b c\")\nstr_extract_all(x, \"[a-z]\", simplify = TRUE)\n#>      [,1] [,2] [,3]\n#> [1,] \"a\"  \"\"   \"\"  \n#> [2,] \"a\"  \"b\"  \"\"  \n#> [3,] \"a\"  \"b\"  \"c\""},{"path":"strings.html","id":"exercises-39","chapter":"14 Strings","heading":"14.4.2.1 Exercises","text":"previous example, might noticed regular\nexpression matched “flickered”, colour. Modify \nregex fix problem.previous example, might noticed regular\nexpression matched “flickered”, colour. Modify \nregex fix problem.Harvard sentences data, extract:\nfirst word sentence.\nwords ending ing.\nplurals.\nHarvard sentences data, extract:first word sentence.words ending ing.plurals.","code":""},{"path":"strings.html","id":"grouped-matches","chapter":"14 Strings","heading":"14.4.3 Grouped matches","text":"Earlier chapter talked use parentheses clarifying precedence backreferences matching. can also use parentheses extract parts complex match. example, imagine want extract nouns sentences. heuristic, ’ll look word comes “” “”. Defining “word” regular expression little tricky, use simple approximation: sequence least one character isn’t space.str_extract() gives us complete match; str_match() gives individual component. Instead character vector, returns matrix, one column complete match followed one column group:(Unsurprisingly, heuristic detecting nouns poor, also picks adjectives like smooth parked.)data tibble, ’s often easier use tidyr::extract(). works like str_match() requires name matches, placed new columns:Like str_extract(), want matches string, ’ll need str_match_all().","code":"\nnoun <- \"(a|the) ([^ ]+)\"\n\nhas_noun <- sentences %>%\n  str_subset(noun) %>%\n  head(10)\nhas_noun %>% \n  str_extract(noun)\n#>  [1] \"the smooth\" \"the sheet\"  \"the depth\"  \"a chicken\"  \"the parked\"\n#>  [6] \"the sun\"    \"the huge\"   \"the ball\"   \"the woman\"  \"a helps\"\nhas_noun %>% \n  str_match(noun)\n#>       [,1]         [,2]  [,3]     \n#>  [1,] \"the smooth\" \"the\" \"smooth\" \n#>  [2,] \"the sheet\"  \"the\" \"sheet\"  \n#>  [3,] \"the depth\"  \"the\" \"depth\"  \n#>  [4,] \"a chicken\"  \"a\"   \"chicken\"\n#>  [5,] \"the parked\" \"the\" \"parked\" \n#>  [6,] \"the sun\"    \"the\" \"sun\"    \n#>  [7,] \"the huge\"   \"the\" \"huge\"   \n#>  [8,] \"the ball\"   \"the\" \"ball\"   \n#>  [9,] \"the woman\"  \"the\" \"woman\"  \n#> [10,] \"a helps\"    \"a\"   \"helps\"\ntibble(sentence = sentences) %>% \n  tidyr::extract(\n    sentence, c(\"article\", \"noun\"), \"(a|the) ([^ ]+)\", \n    remove = FALSE\n  )\n#> # A tibble: 720 x 3\n#>   sentence                                    article noun   \n#>   <chr>                                       <chr>   <chr>  \n#> 1 The birch canoe slid on the smooth planks.  the     smooth \n#> 2 Glue the sheet to the dark blue background. the     sheet  \n#> 3 It's easy to tell the depth of a well.      the     depth  \n#> 4 These days a chicken leg is a rare dish.    a       chicken\n#> 5 Rice is often served in round bowls.        <NA>    <NA>   \n#> 6 The juice of lemons makes fine punch.       <NA>    <NA>   \n#> # … with 714 more rows"},{"path":"strings.html","id":"exercises-40","chapter":"14 Strings","heading":"14.4.3.1 Exercises","text":"Find words come “number” like “one”, “two”, “three” etc.\nPull number word.Find words come “number” like “one”, “two”, “three” etc.\nPull number word.Find contractions. Separate pieces \napostrophe.Find contractions. Separate pieces \napostrophe.","code":""},{"path":"strings.html","id":"replacing-matches","chapter":"14 Strings","heading":"14.4.4 Replacing matches","text":"str_replace() str_replace_all() allow replace matches new strings. simplest use replace pattern fixed string:str_replace_all() can perform multiple replacements supplying named vector:Instead replacing fixed string can use backreferences insert components match. following code, flip order second third words.","code":"\nx <- c(\"apple\", \"pear\", \"banana\")\nstr_replace(x, \"[aeiou]\", \"-\")\n#> [1] \"-pple\"  \"p-ar\"   \"b-nana\"\nstr_replace_all(x, \"[aeiou]\", \"-\")\n#> [1] \"-ppl-\"  \"p--r\"   \"b-n-n-\"\nx <- c(\"1 house\", \"2 cars\", \"3 people\")\nstr_replace_all(x, c(\"1\" = \"one\", \"2\" = \"two\", \"3\" = \"three\"))\n#> [1] \"one house\"    \"two cars\"     \"three people\"\nsentences %>% \n  str_replace(\"([^ ]+) ([^ ]+) ([^ ]+)\", \"\\\\1 \\\\3 \\\\2\") %>% \n  head(5)\n#> [1] \"The canoe birch slid on the smooth planks.\" \n#> [2] \"Glue sheet the to the dark blue background.\"\n#> [3] \"It's to easy tell the depth of a well.\"     \n#> [4] \"These a days chicken leg is a rare dish.\"   \n#> [5] \"Rice often is served in round bowls.\""},{"path":"strings.html","id":"exercises-41","chapter":"14 Strings","heading":"14.4.4.1 Exercises","text":"Replace forward slashes string backslashes.Replace forward slashes string backslashes.Implement simple version str_to_lower() using replace_all().Implement simple version str_to_lower() using replace_all().Switch first last letters words. strings\nstill words?Switch first last letters words. strings\nstill words?","code":""},{"path":"strings.html","id":"splitting","chapter":"14 Strings","heading":"14.4.5 Splitting","text":"Use str_split() split string pieces. example, split sentences words:component might contain different number pieces, returns list. ’re working length-1 vector, easiest thing just extract first element list:Otherwise, like stringr functions return list, can use simplify = TRUE return matrix:can also request maximum number pieces:Instead splitting strings patterns, can also split character, line, sentence word boundary()s:","code":"\nsentences %>%\n  head(5) %>% \n  str_split(\" \")\n#> [[1]]\n#> [1] \"The\"     \"birch\"   \"canoe\"   \"slid\"    \"on\"      \"the\"     \"smooth\" \n#> [8] \"planks.\"\n#> \n#> [[2]]\n#> [1] \"Glue\"        \"the\"         \"sheet\"       \"to\"          \"the\"        \n#> [6] \"dark\"        \"blue\"        \"background.\"\n#> \n#> [[3]]\n#> [1] \"It's\"  \"easy\"  \"to\"    \"tell\"  \"the\"   \"depth\" \"of\"    \"a\"     \"well.\"\n#> \n#> [[4]]\n#> [1] \"These\"   \"days\"    \"a\"       \"chicken\" \"leg\"     \"is\"      \"a\"      \n#> [8] \"rare\"    \"dish.\"  \n#> \n#> [[5]]\n#> [1] \"Rice\"   \"is\"     \"often\"  \"served\" \"in\"     \"round\"  \"bowls.\"\n\"a|b|c|d\" %>% \n  str_split(\"\\\\|\") %>% \n  .[[1]]\n#> [1] \"a\" \"b\" \"c\" \"d\"\nsentences %>%\n  head(5) %>% \n  str_split(\" \", simplify = TRUE)\n#>      [,1]    [,2]    [,3]    [,4]      [,5]  [,6]    [,7]     [,8]         \n#> [1,] \"The\"   \"birch\" \"canoe\" \"slid\"    \"on\"  \"the\"   \"smooth\" \"planks.\"    \n#> [2,] \"Glue\"  \"the\"   \"sheet\" \"to\"      \"the\" \"dark\"  \"blue\"   \"background.\"\n#> [3,] \"It's\"  \"easy\"  \"to\"    \"tell\"    \"the\" \"depth\" \"of\"     \"a\"          \n#> [4,] \"These\" \"days\"  \"a\"     \"chicken\" \"leg\" \"is\"    \"a\"      \"rare\"       \n#> [5,] \"Rice\"  \"is\"    \"often\" \"served\"  \"in\"  \"round\" \"bowls.\" \"\"           \n#>      [,9]   \n#> [1,] \"\"     \n#> [2,] \"\"     \n#> [3,] \"well.\"\n#> [4,] \"dish.\"\n#> [5,] \"\"\nfields <- c(\"Name: Hadley\", \"Country: NZ\", \"Age: 35\")\nfields %>% str_split(\": \", n = 2, simplify = TRUE)\n#>      [,1]      [,2]    \n#> [1,] \"Name\"    \"Hadley\"\n#> [2,] \"Country\" \"NZ\"    \n#> [3,] \"Age\"     \"35\"\nx <- \"This is a sentence.  This is another sentence.\"\nstr_view_all(x, boundary(\"word\"))"},{"path":"strings.html","id":"exercises-42","chapter":"14 Strings","heading":"14.4.5.1 Exercises","text":"Split string like \"apples, pears, bananas\" individual\ncomponents.Split string like \"apples, pears, bananas\" individual\ncomponents.better split boundary(\"word\") \" \"?better split boundary(\"word\") \" \"?splitting empty string (\"\") ? Experiment, \nread documentation.splitting empty string (\"\") ? Experiment, \nread documentation.","code":""},{"path":"strings.html","id":"find-matches","chapter":"14 Strings","heading":"14.4.6 Find matches","text":"str_locate() str_locate_all() give starting ending positions match. particularly useful none functions exactly want. can use str_locate() find matching pattern, str_sub() extract /modify .","code":""},{"path":"strings.html","id":"other-types-of-pattern","chapter":"14 Strings","heading":"14.5 Other types of pattern","text":"use pattern ’s string, ’s automatically wrapped call regex():can use arguments regex() control details match:ignore_case = TRUE allows characters match either uppercase \nlowercase forms. always uses current locale.\n\nbananas <- c(\"banana\", \"Banana\", \"BANANA\")\nstr_view(bananas, \"banana\")\n\n{\"x\":{\"html\":\"<ul>\\n  <li><span class='match'>banana<\\/span><\\/li>\\n  <li>Banana<\\/li>\\n  <li>BANANA<\\/li>\\n<\\/ul>\"},\"evals\":[],\"jsHooks\":[]}\nstr_view(bananas, regex(\"banana\", ignore_case = TRUE))\n\n{\"x\":{\"html\":\"<ul>\\n  <li><span class='match'>banana<\\/span><\\/li>\\n  <li><span class='match'>Banana<\\/span><\\/li>\\n  <li><span class='match'>BANANA<\\/span><\\/li>\\n<\\/ul>\"},\"evals\":[],\"jsHooks\":[]}ignore_case = TRUE allows characters match either uppercase \nlowercase forms. always uses current locale.multiline = TRUE allows ^ $ match start end \nline rather start end complete string.\n\nx <- \"Line 1\\nLine 2\\nLine 3\"\nstr_extract_all(x, \"^Line\")[[1]]\n#> [1] \"Line\"\nstr_extract_all(x, regex(\"^Line\", multiline = TRUE))[[1]]\n#> [1] \"Line\" \"Line\" \"Line\"multiline = TRUE allows ^ $ match start end \nline rather start end complete string.comments = TRUE allows use comments white space make\ncomplex regular expressions understandable. Spaces ignored, \neverything #. match literal space, ’ll need escape :\n\"\\\\ \".\n\nphone <- regex(\"\n  \\\\(?     # optional opening parens\n  (\\\\d{3}) # area code\n  [) -]?   # optional closing parens, space, dash\n  (\\\\d{3}) # another three numbers\n  [ -]?    # optional space dash\n  (\\\\d{3}) # three numbers\n  \", comments = TRUE)\n\nstr_match(\"514-791-8141\", phone)\n#>      [,1]          [,2]  [,3]  [,4] \n#> [1,] \"514-791-814\" \"514\" \"791\" \"814\"comments = TRUE allows use comments white space make\ncomplex regular expressions understandable. Spaces ignored, \neverything #. match literal space, ’ll need escape :\n\"\\\\ \".dotall = TRUE allows . match everything, including \\n.dotall = TRUE allows . match everything, including \\n.three functions can use instead regex():fixed(): matches exactly specified sequence bytes. ignores\nspecial regular expressions operates low level.\nallows avoid complex escaping can much faster \nregular expressions. following microbenchmark shows ’s \n3x faster simple example.\n\nmicrobenchmark::microbenchmark(\n  fixed = str_detect(sentences, fixed(\"\")),\n  regex = str_detect(sentences, \"\"),\n  times = 20\n)\n#> Unit: microseconds\n#>   expr     min       lq     mean   median       uq     max neval\n#>  fixed  30.588  34.7910  70.4371  44.2885  58.9810 513.445    20\n#>  regex 177.213 182.5005 204.4351 185.9085 208.8135 340.413    20\nBeware using fixed() non-English data. problematic \noften multiple ways representing character. \nexample, two ways define “á”: either single character \n“” plus accent:\n\na1 <- \"\\u00e1\"\na2 <- \"\\u0301\"\nc(a1, a2)\n#> [1] \"á\" \"́\"\na1 == a2\n#> [1] FALSE\nrender identically, ’re defined differently,\nfixed() doesn’t find match. Instead, can use coll(), defined\nnext, respect human character comparison rules:\n\nstr_detect(a1, fixed(a2))\n#> [1] FALSE\nstr_detect(a1, coll(a2))\n#> [1] TRUEfixed(): matches exactly specified sequence bytes. ignores\nspecial regular expressions operates low level.\nallows avoid complex escaping can much faster \nregular expressions. following microbenchmark shows ’s \n3x faster simple example.Beware using fixed() non-English data. problematic \noften multiple ways representing character. \nexample, two ways define “á”: either single character \n“” plus accent:render identically, ’re defined differently,\nfixed() doesn’t find match. Instead, can use coll(), defined\nnext, respect human character comparison rules:coll(): compare strings using standard collation rules. \nuseful case insensitive matching. Note coll() takes \nlocale parameter controls rules used comparing\ncharacters. Unfortunately different parts world use different rules!\n\n# means also need aware difference\n# case insensitive matches:\n<- c(\"\", \"İ\", \"\", \"ı\")\n\n#> [1] \"\" \"İ\" \"\" \"ı\"\n\nstr_subset(, coll(\"\", ignore_case = TRUE))\n#> [1] \"\" \"\"\nstr_subset(, coll(\"\", ignore_case = TRUE, locale = \"tr\"))\n#> [1] \"İ\" \"\"\nfixed() regex() ignore_case arguments, \nallow pick locale: always use default locale.\ncan see following code; stringi\nlater.\n\nstringi::stri_locale_info()\n#> $Language\n#> [1] \"en\"\n#> \n#> $Country\n#> [1] \"US\"\n#> \n#> $Variant\n#> [1] \"\"\n#> \n#> $Name\n#> [1] \"en_US\"\ndownside coll() speed; rules recognising \ncharacters complicated, coll() relatively slow\ncompared regex() fixed().coll(): compare strings using standard collation rules. \nuseful case insensitive matching. Note coll() takes \nlocale parameter controls rules used comparing\ncharacters. Unfortunately different parts world use different rules!fixed() regex() ignore_case arguments, \nallow pick locale: always use default locale.\ncan see following code; stringi\nlater.downside coll() speed; rules recognising \ncharacters complicated, coll() relatively slow\ncompared regex() fixed().saw str_split() can use boundary() match boundaries.\ncan also use functions:\n\nx <- \"sentence.\"\nstr_view_all(x, boundary(\"word\"))\n\n{\"x\":{\"html\":\"<ul>\\n  <li><span class='match'><\\/span> <span class='match'><\\/span> <span class='match'><\\/span> <span class='match'>sentence<\\/span>.<\\/li>\\n<\\/ul>\"},\"evals\":[],\"jsHooks\":[]}\nstr_extract_all(x, boundary(\"word\"))\n#> [[1]]\n#> [1] \"\"     \"\"       \"\"        \"sentence\"saw str_split() can use boundary() match boundaries.\ncan also use functions:","code":"\n# The regular call:\nstr_view(fruit, \"nana\")\n# Is shorthand for\nstr_view(fruit, regex(\"nana\"))\nbananas <- c(\"banana\", \"Banana\", \"BANANA\")\nstr_view(bananas, \"banana\")\nx <- \"Line 1\\nLine 2\\nLine 3\"\nstr_extract_all(x, \"^Line\")[[1]]\n#> [1] \"Line\"\nstr_extract_all(x, regex(\"^Line\", multiline = TRUE))[[1]]\n#> [1] \"Line\" \"Line\" \"Line\"\nphone <- regex(\"\n  \\\\(?     # optional opening parens\n  (\\\\d{3}) # area code\n  [) -]?   # optional closing parens, space, or dash\n  (\\\\d{3}) # another three numbers\n  [ -]?    # optional space or dash\n  (\\\\d{3}) # three more numbers\n  \", comments = TRUE)\n\nstr_match(\"514-791-8141\", phone)\n#>      [,1]          [,2]  [,3]  [,4] \n#> [1,] \"514-791-814\" \"514\" \"791\" \"814\"\nmicrobenchmark::microbenchmark(\n  fixed = str_detect(sentences, fixed(\"the\")),\n  regex = str_detect(sentences, \"the\"),\n  times = 20\n)\n#> Unit: microseconds\n#>   expr     min       lq     mean   median       uq     max neval\n#>  fixed  30.588  34.7910  70.4371  44.2885  58.9810 513.445    20\n#>  regex 177.213 182.5005 204.4351 185.9085 208.8135 340.413    20\na1 <- \"\\u00e1\"\na2 <- \"a\\u0301\"\nc(a1, a2)\n#> [1] \"á\" \"á\"\na1 == a2\n#> [1] FALSE\nstr_detect(a1, fixed(a2))\n#> [1] FALSE\nstr_detect(a1, coll(a2))\n#> [1] TRUE\n# That means you also need to be aware of the difference\n# when doing case insensitive matches:\ni <- c(\"I\", \"İ\", \"i\", \"ı\")\ni\n#> [1] \"I\" \"İ\" \"i\" \"ı\"\n\nstr_subset(i, coll(\"i\", ignore_case = TRUE))\n#> [1] \"I\" \"i\"\nstr_subset(i, coll(\"i\", ignore_case = TRUE, locale = \"tr\"))\n#> [1] \"İ\" \"i\"\nstringi::stri_locale_info()\n#> $Language\n#> [1] \"en\"\n#> \n#> $Country\n#> [1] \"US\"\n#> \n#> $Variant\n#> [1] \"\"\n#> \n#> $Name\n#> [1] \"en_US\"\nx <- \"This is a sentence.\"\nstr_view_all(x, boundary(\"word\"))"},{"path":"strings.html","id":"exercises-43","chapter":"14 Strings","heading":"14.5.1 Exercises","text":"find strings containing \\ regex() vs.\nfixed()?find strings containing \\ regex() vs.\nfixed()?five common words sentences?five common words sentences?","code":""},{"path":"strings.html","id":"other-uses-of-regular-expressions","chapter":"14 Strings","heading":"14.6 Other uses of regular expressions","text":"two useful function base R also use regular expressions:apropos() searches objects available global environment. \nuseful can’t quite remember name function.\n\napropos(\"replace\")\n#> [1] \"%+replace%\"       \"replace\"          \"replace_na\"       \"setReplaceMethod\"\n#> [5] \"str_replace\"      \"str_replace_all\"  \"str_replace_na\"   \"theme_replace\"apropos() searches objects available global environment. \nuseful can’t quite remember name function.dir() lists files directory. pattern argument takes\nregular expression returns file names match pattern.\nexample, can find R Markdown files current\ndirectory :\n\nhead(dir(pattern = \"\\\\.Rmd$\"))\n#> [1] \"communicate-plots.Rmd\" \"communicate.Rmd\"       \"datetimes.Rmd\"        \n#> [4] \"EDA.Rmd\"               \"explore.Rmd\"           \"factors.Rmd\"\n(’re comfortable “globs” like *.Rmd, can convert\nregular expressions glob2rx()):dir() lists files directory. pattern argument takes\nregular expression returns file names match pattern.\nexample, can find R Markdown files current\ndirectory :(’re comfortable “globs” like *.Rmd, can convert\nregular expressions glob2rx()):","code":"\napropos(\"replace\")\n#> [1] \"%+replace%\"       \"replace\"          \"replace_na\"       \"setReplaceMethod\"\n#> [5] \"str_replace\"      \"str_replace_all\"  \"str_replace_na\"   \"theme_replace\"\nhead(dir(pattern = \"\\\\.Rmd$\"))\n#> [1] \"communicate-plots.Rmd\" \"communicate.Rmd\"       \"datetimes.Rmd\"        \n#> [4] \"EDA.Rmd\"               \"explore.Rmd\"           \"factors.Rmd\""},{"path":"strings.html","id":"stringi","chapter":"14 Strings","heading":"14.7 stringi","text":"stringr built top stringi package. stringr useful ’re learning exposes minimal set functions, carefully picked handle common string manipulation functions. stringi, hand, designed comprehensive. contains almost every function might ever need: stringi 250 functions stringr’s 49.find struggling something stringr, ’s worth taking look stringi. packages work similarly, able translate stringr knowledge natural way. main difference prefix: str_ vs. stri_.","code":""},{"path":"strings.html","id":"exercises-44","chapter":"14 Strings","heading":"14.7.1 Exercises","text":"Find stringi functions :\nCount number words.\nFind duplicated strings.\nGenerate random text.\nFind stringi functions :Count number words.Find duplicated strings.Generate random text.control language stri_sort() uses \nsorting?control language stri_sort() uses \nsorting?","code":""},{"path":"factors.html","id":"factors","chapter":"15 Factors","heading":"15 Factors","text":"","code":""},{"path":"factors.html","id":"introduction-9","chapter":"15 Factors","heading":"15.1 Introduction","text":"R, factors used work categorical variables, variables fixed known set possible values. also useful want display character vectors non-alphabetical order.Historically, factors much easier work characters. result, many functions base R automatically convert characters factors. means factors often crop places ’re actually helpful. Fortunately, don’t need worry tidyverse, can focus situations factors genuinely useful.","code":""},{"path":"factors.html","id":"prerequisites-9","chapter":"15 Factors","heading":"15.1.1 Prerequisites","text":"work factors, ’ll use forcats package, part core tidyverse. provides tools dealing categorical variables (’s anagram factors!) using wide range helpers working factors.","code":"\nlibrary(tidyverse)"},{"path":"factors.html","id":"learning-more-1","chapter":"15 Factors","heading":"15.1.2 Learning more","text":"want learn factors, recommend reading Amelia McNamara Nicholas Horton’s paper, Wrangling categorical data R. paper lays history discussed stringsAsFactors: unauthorized biography stringsAsFactors = <sigh>, compares tidy approaches categorical data outlined book base R methods. early version paper help motivate scope forcats package; thanks Amelia & Nick!","code":""},{"path":"factors.html","id":"creating-factors","chapter":"15 Factors","heading":"15.2 Creating factors","text":"Imagine variable records month:Using string record variable two problems:twelve possible months, ’s nothing saving \ntypos:\n\nx2 <- c(\"Dec\", \"Apr\", \"Jam\", \"Mar\")twelve possible months, ’s nothing saving \ntypos:doesn’t sort useful way:\n\nsort(x1)\n#> [1] \"Apr\" \"Dec\" \"Jan\" \"Mar\"doesn’t sort useful way:can fix problems factor. create factor must start creating list valid levels:Now can create factor:values set silently converted NA:want warning, can use readr::parse_factor():omit levels, ’ll taken data alphabetical order:Sometimes ’d prefer order levels match order first appearance data. can creating factor setting levels unique(x), fact, fct_inorder():ever need access set valid levels directly, can levels():","code":"\nx1 <- c(\"Dec\", \"Apr\", \"Jan\", \"Mar\")\nx2 <- c(\"Dec\", \"Apr\", \"Jam\", \"Mar\")\nsort(x1)\n#> [1] \"Apr\" \"Dec\" \"Jan\" \"Mar\"\nmonth_levels <- c(\n  \"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\", \n  \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\"\n)\ny1 <- factor(x1, levels = month_levels)\ny1\n#> [1] Dec Apr Jan Mar\n#> Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\nsort(y1)\n#> [1] Jan Mar Apr Dec\n#> Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\ny2 <- factor(x2, levels = month_levels)\ny2\n#> [1] Dec  Apr  <NA> Mar \n#> Levels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\ny2 <- parse_factor(x2, levels = month_levels)\n#> Warning: 1 parsing failure.\n#> row col           expected actual\n#>   3  -- value in level set    Jam\nfactor(x1)\n#> [1] Dec Apr Jan Mar\n#> Levels: Apr Dec Jan Mar\nf1 <- factor(x1, levels = unique(x1))\nf1\n#> [1] Dec Apr Jan Mar\n#> Levels: Dec Apr Jan Mar\n\nf2 <- x1 %>% factor() %>% fct_inorder()\nf2\n#> [1] Dec Apr Jan Mar\n#> Levels: Dec Apr Jan Mar\nlevels(f2)\n#> [1] \"Dec\" \"Apr\" \"Jan\" \"Mar\""},{"path":"factors.html","id":"general-social-survey","chapter":"15 Factors","heading":"15.3 General Social Survey","text":"rest chapter, ’re going focus forcats::gss_cat. ’s sample data General Social Survey, long-running US survey conducted independent research organization NORC University Chicago. survey thousands questions, gss_cat ’ve selected handful illustrate common challenges ’ll encounter working factors.(Remember, since dataset provided package, can get information variables ?gss_cat.)factors stored tibble, can’t see levels easily. One way see count():bar chart:default, ggplot2 drop levels don’t values. can force display :levels represent valid values simply occur dataset. dplyr::count() set .drop option FALSE, show .working factors, two common operations changing order levels, changing values levels. operations described sections .","code":"\ngss_cat\n#> # A tibble: 21,483 x 9\n#>    year marital      age race  rincome    partyid     relig     denom    tvhours\n#>   <int> <fct>      <int> <fct> <fct>      <fct>       <fct>     <fct>      <int>\n#> 1  2000 Never mar…    26 White $8000 to … Ind,near r… Protesta… Souther…      12\n#> 2  2000 Divorced      48 White $8000 to … Not str re… Protesta… Baptist…      NA\n#> 3  2000 Widowed       67 White Not appli… Independent Protesta… No deno…       2\n#> 4  2000 Never mar…    39 White Not appli… Ind,near r… Orthodox… Not app…       4\n#> 5  2000 Divorced      25 White Not appli… Not str de… None      Not app…       1\n#> 6  2000 Married       25 White $20000 - … Strong dem… Protesta… Souther…      NA\n#> # … with 21,477 more rows\ngss_cat %>%\n  count(race)\n#> # A tibble: 3 x 2\n#>   race      n\n#> * <fct> <int>\n#> 1 Other  1959\n#> 2 Black  3129\n#> 3 White 16395\nggplot(gss_cat, aes(race)) +\n  geom_bar()\nggplot(gss_cat, aes(race)) +\n  geom_bar() +\n  scale_x_discrete(drop = FALSE)\ngss_cat %>% \n  count(race, \n        .drop = FALSE)\n#> # A tibble: 4 x 2\n#>   race               n\n#> * <fct>          <int>\n#> 1 Other           1959\n#> 2 Black           3129\n#> 3 White          16395\n#> 4 Not applicable     0"},{"path":"factors.html","id":"exercise","chapter":"15 Factors","heading":"15.3.1 Exercise","text":"Explore distribution rincome (reported income). makes \ndefault bar chart hard understand? improve plot?Explore distribution rincome (reported income). makes \ndefault bar chart hard understand? improve plot?common relig survey? ’s \ncommon partyid?common relig survey? ’s \ncommon partyid?relig denom (denomination) apply ? can find\ntable? can find visualisation?relig denom (denomination) apply ? can find\ntable? can find visualisation?","code":""},{"path":"factors.html","id":"modifying-factor-order","chapter":"15 Factors","heading":"15.4 Modifying factor order","text":"’s often useful change order factor levels visualisation. example, imagine want explore average number hours spent watching TV per day across religions:difficult interpret plot ’s overall pattern. can improve reordering levels relig using fct_reorder(). fct_reorder() takes three arguments:f, factor whose levels want modify.x, numeric vector want use reorder levels.Optionally, fun, function ’s used multiple values \nx value f. default value median.Reordering religion makes much easier see people “Don’t know” category watch much TV, Hinduism & Eastern religions watch much less.start making complicated transformations, ’d recommend moving aes() separate mutate() step. example, rewrite plot :create similar plot looking average age varies across reported income level?, arbitrarily reordering levels isn’t good idea! ’s rincome already principled order shouldn’t mess . Reserve fct_reorder() factors whose levels arbitrarily ordered.However, make sense pull “applicable” front special levels. can use fct_relevel(). takes factor, f, number levels want move front line.think average age “applicable” high?Another type reordering useful colouring lines plot. fct_reorder2() reorders factor y values associated largest x values. makes plot easier read line colours line legend.Finally, bar plots, can use fct_infreq() order levels increasing frequency: simplest type reordering doesn’t need extra variables. may want combine fct_rev().","code":"\nrelig_summary <- gss_cat %>%\n  group_by(relig) %>%\n  summarise(\n    age = mean(age, na.rm = TRUE),\n    tvhours = mean(tvhours, na.rm = TRUE),\n    n = n()\n  )\n\nggplot(relig_summary, aes(tvhours, relig)) + geom_point()\nggplot(relig_summary, aes(tvhours, fct_reorder(relig, tvhours))) +\n  geom_point()\nrelig_summary %>%\n  mutate(relig = fct_reorder(relig, tvhours)) %>%\n  ggplot(aes(tvhours, relig)) +\n    geom_point()\nrincome_summary <- gss_cat %>%\n  group_by(rincome) %>%\n  summarise(\n    age = mean(age, na.rm = TRUE),\n    tvhours = mean(tvhours, na.rm = TRUE),\n    n = n()\n  )\n\nggplot(rincome_summary, aes(age, fct_reorder(rincome, age))) + geom_point()\nggplot(rincome_summary, aes(age, fct_relevel(rincome, \"Not applicable\"))) +\n  geom_point()\nby_age <- gss_cat %>%\n  filter(!is.na(age)) %>%\n  count(age, marital) %>%\n  group_by(age) %>%\n  mutate(prop = n / sum(n))\n\nggplot(by_age, aes(age, prop, colour = marital)) +\n  geom_line(na.rm = TRUE)\n\nggplot(by_age, aes(age, prop, colour = fct_reorder2(marital, age, prop))) +\n  geom_line() +\n  labs(colour = \"marital\")\ngss_cat %>%\n  mutate(marital = marital %>% fct_infreq() %>% fct_rev()) %>%\n  ggplot(aes(marital)) +\n    geom_bar()"},{"path":"factors.html","id":"exercises-45","chapter":"15 Factors","heading":"15.4.1 Exercises","text":"suspiciously high numbers tvhours. mean good\nsummary?suspiciously high numbers tvhours. mean good\nsummary?factor gss_cat identify whether order levels \narbitrary principled.factor gss_cat identify whether order levels \narbitrary principled.moving “applicable” front levels move \nbottom plot?moving “applicable” front levels move \nbottom plot?","code":""},{"path":"factors.html","id":"modifying-factor-levels","chapter":"15 Factors","heading":"15.5 Modifying factor levels","text":"powerful changing orders levels changing values. allows clarify labels publication, collapse levels high-level displays. general powerful tool fct_recode(). allows recode, change, value level. example, take gss_cat$partyid:levels terse inconsistent. Let’s tweak longer use parallel construction.fct_recode() leave levels aren’t explicitly mentioned , warn accidentally refer level doesn’t exist.combine groups, can assign multiple old levels new level:must use technique care: group together categories truly different end misleading results.want collapse lot levels, fct_collapse() useful variant fct_recode(). new variable, can provide vector old levels:Sometimes just want lump together small groups make plot table simpler. ’s job fct_lump():default behaviour progressively lump together smallest groups, ensuring aggregate still smallest group. case ’s helpful: true majority Americans survey Protestant, ’ve probably collapsed.Instead, can use n parameter specify many groups (excluding ) want keep:","code":"\ngss_cat %>% count(partyid)\n#> # A tibble: 10 x 2\n#>   partyid                n\n#> * <fct>              <int>\n#> 1 No answer            154\n#> 2 Don't know             1\n#> 3 Other party          393\n#> 4 Strong republican   2314\n#> 5 Not str republican  3032\n#> 6 Ind,near rep        1791\n#> # … with 4 more rows\ngss_cat %>%\n  mutate(partyid = fct_recode(partyid,\n    \"Republican, strong\"    = \"Strong republican\",\n    \"Republican, weak\"      = \"Not str republican\",\n    \"Independent, near rep\" = \"Ind,near rep\",\n    \"Independent, near dem\" = \"Ind,near dem\",\n    \"Democrat, weak\"        = \"Not str democrat\",\n    \"Democrat, strong\"      = \"Strong democrat\"\n  )) %>%\n  count(partyid)\n#> # A tibble: 10 x 2\n#>   partyid                   n\n#> * <fct>                 <int>\n#> 1 No answer               154\n#> 2 Don't know                1\n#> 3 Other party             393\n#> 4 Republican, strong     2314\n#> 5 Republican, weak       3032\n#> 6 Independent, near rep  1791\n#> # … with 4 more rows\ngss_cat %>%\n  mutate(partyid = fct_recode(partyid,\n    \"Republican, strong\"    = \"Strong republican\",\n    \"Republican, weak\"      = \"Not str republican\",\n    \"Independent, near rep\" = \"Ind,near rep\",\n    \"Independent, near dem\" = \"Ind,near dem\",\n    \"Democrat, weak\"        = \"Not str democrat\",\n    \"Democrat, strong\"      = \"Strong democrat\",\n    \"Other\"                 = \"No answer\",\n    \"Other\"                 = \"Don't know\",\n    \"Other\"                 = \"Other party\"\n  )) %>%\n  count(partyid)\n#> # A tibble: 8 x 2\n#>   partyid                   n\n#> * <fct>                 <int>\n#> 1 Other                   548\n#> 2 Republican, strong     2314\n#> 3 Republican, weak       3032\n#> 4 Independent, near rep  1791\n#> 5 Independent            4119\n#> 6 Independent, near dem  2499\n#> # … with 2 more rows\ngss_cat %>%\n  mutate(partyid = fct_collapse(partyid,\n    other = c(\"No answer\", \"Don't know\", \"Other party\"),\n    rep = c(\"Strong republican\", \"Not str republican\"),\n    ind = c(\"Ind,near rep\", \"Independent\", \"Ind,near dem\"),\n    dem = c(\"Not str democrat\", \"Strong democrat\")\n  )) %>%\n  count(partyid)\n#> # A tibble: 4 x 2\n#>   partyid     n\n#> * <fct>   <int>\n#> 1 other     548\n#> 2 rep      5346\n#> 3 ind      8409\n#> 4 dem      7180\ngss_cat %>%\n  mutate(relig = fct_lump(relig)) %>%\n  count(relig)\n#> # A tibble: 2 x 2\n#>   relig          n\n#> * <fct>      <int>\n#> 1 Protestant 10846\n#> 2 Other      10637\ngss_cat %>%\n  mutate(relig = fct_lump(relig, n = 10)) %>%\n  count(relig, sort = TRUE) %>%\n  print(n = Inf)\n#> # A tibble: 10 x 2\n#>    relig                       n\n#>    <fct>                   <int>\n#>  1 Protestant              10846\n#>  2 Catholic                 5124\n#>  3 None                     3523\n#>  4 Christian                 689\n#>  5 Other                     458\n#>  6 Jewish                    388\n#>  7 Buddhism                  147\n#>  8 Inter-nondenominational   109\n#>  9 Moslem/islam              104\n#> 10 Orthodox-christian         95"},{"path":"factors.html","id":"exercises-46","chapter":"15 Factors","heading":"15.5.1 Exercises","text":"proportions people identifying Democrat, Republican, \nIndependent changed time?proportions people identifying Democrat, Republican, \nIndependent changed time?collapse rincome small set categories?collapse rincome small set categories?Notice 9 groups (excluding ) fct_lump example . 10? (Hint: type ?fct_lump, find default argument other_level “”.)Notice 9 groups (excluding ) fct_lump example . 10? (Hint: type ?fct_lump, find default argument other_level “”.)","code":""},{"path":"dates-and-times.html","id":"dates-and-times","chapter":"16 Dates and times","heading":"16 Dates and times","text":"","code":""},{"path":"dates-and-times.html","id":"introduction-10","chapter":"16 Dates and times","heading":"16.1 Introduction","text":"chapter show work dates times R. first glance, dates times seem simple. use time regular life, don’t seem cause much confusion. However, learn dates times, complicated seem get. warm , try three seemingly simple questions:every year 365 days?every day 24 hours?every minute 60 seconds?’m sure know every year 365 days, know full rule determining year leap year? (three parts.) might remembered many parts world use daylight savings time (DST), days 23 hours, others 25. might known minutes 61 seconds every now leap seconds added Earth’s rotation gradually slowing .Dates times hard reconcile two physical phenomena (rotation Earth orbit around sun) whole raft geopolitical phenomena including months, time zones, DST. chapter won’t teach every last detail dates times, give solid grounding practical skills help common data analysis challenges.","code":""},{"path":"dates-and-times.html","id":"prerequisites-10","chapter":"16 Dates and times","heading":"16.1.1 Prerequisites","text":"chapter focus lubridate package, makes easier work dates times R. lubridate part core tidyverse need ’re working dates/times. also need nycflights13 practice data.","code":"\nlibrary(tidyverse)\n\nlibrary(lubridate)\nlibrary(nycflights13)"},{"path":"dates-and-times.html","id":"creating-datetimes","chapter":"16 Dates and times","heading":"16.2 Creating date/times","text":"three types date/time data refer instant time:date. Tibbles print <date>.date. Tibbles print <date>.time within day. Tibbles print <time>.time within day. Tibbles print <time>.date-time date plus time: uniquely identifies \ninstant time (typically nearest second). Tibbles print \n<dttm>. Elsewhere R called POSIXct, don’t think\n’s useful name.date-time date plus time: uniquely identifies \ninstant time (typically nearest second). Tibbles print \n<dttm>. Elsewhere R called POSIXct, don’t think\n’s useful name.chapter going focus dates date-times R doesn’t native class storing times. need one, can use hms package.always use simplest possible data type works needs. means can use date instead date-time, . Date-times substantially complicated need handle time zones, ’ll come back end chapter.get current date date-time can use today() now():Otherwise, three ways ’re likely create date/time:string.individual date-time components.existing date/time object.work follows.","code":"\ntoday()\n#> [1] \"2021-02-14\"\nnow()\n#> [1] \"2021-02-14 19:10:55 HKT\""},{"path":"dates-and-times.html","id":"from-strings","chapter":"16 Dates and times","heading":"16.2.1 From strings","text":"Date/time data often comes strings. ’ve seen one approach parsing strings date-times date-times. Another approach use helpers provided lubridate. automatically work format specify order component. use , identify order year, month, day appear dates, arrange “y”, “m”, “d” order. gives name lubridate function parse date. example:functions also take unquoted numbers. concise way create single date/time object, might need filtering date/time data. ymd() short unambiguous:ymd() friends create dates. create date-time, add underscore one “h”, “m”, “s” name parsing function:can also force creation date-time date supplying timezone:","code":"\nymd(\"2017-01-31\")\n#> [1] \"2017-01-31\"\nmdy(\"January 31st, 2017\")\n#> [1] \"2017-01-31\"\ndmy(\"31-Jan-2017\")\n#> [1] \"2017-01-31\"\nymd(20170131)\n#> [1] \"2017-01-31\"\nymd_hms(\"2017-01-31 20:11:59\")\n#> [1] \"2017-01-31 20:11:59 UTC\"\nmdy_hm(\"01/31/2017 08:01\")\n#> [1] \"2017-01-31 08:01:00 UTC\"\nymd(20170131, tz = \"UTC\")\n#> [1] \"2017-01-31 UTC\""},{"path":"dates-and-times.html","id":"from-individual-components","chapter":"16 Dates and times","heading":"16.2.2 From individual components","text":"Instead single string, sometimes ’ll individual components date-time spread across multiple columns. flights data:create date/time sort input, use make_date() dates, make_datetime() date-times:Let’s thing four time columns flights. times represented slightly odd format, use modulus arithmetic pull hour minute components. ’ve created date-time variables, focus variables ’ll explore rest chapter.data, can visualise distribution departure times across year:within single day:Note use date-times numeric context (like histogram), 1 means 1 second, binwidth 86400 means one day. dates, 1 means 1 day.","code":"\nflights %>% \n  select(year, month, day, hour, minute)\n#> # A tibble: 336,776 x 5\n#>    year month   day  hour minute\n#>   <int> <int> <int> <dbl>  <dbl>\n#> 1  2013     1     1     5     15\n#> 2  2013     1     1     5     29\n#> 3  2013     1     1     5     40\n#> 4  2013     1     1     5     45\n#> 5  2013     1     1     6      0\n#> 6  2013     1     1     5     58\n#> # … with 336,770 more rows\nflights %>% \n  select(year, month, day, hour, minute) %>% \n  mutate(departure = make_datetime(year, month, day, hour, minute))\n#> # A tibble: 336,776 x 6\n#>    year month   day  hour minute departure          \n#>   <int> <int> <int> <dbl>  <dbl> <dttm>             \n#> 1  2013     1     1     5     15 2013-01-01 05:15:00\n#> 2  2013     1     1     5     29 2013-01-01 05:29:00\n#> 3  2013     1     1     5     40 2013-01-01 05:40:00\n#> 4  2013     1     1     5     45 2013-01-01 05:45:00\n#> 5  2013     1     1     6      0 2013-01-01 06:00:00\n#> 6  2013     1     1     5     58 2013-01-01 05:58:00\n#> # … with 336,770 more rows\nmake_datetime_100 <- function(year, month, day, time) {\n  make_datetime(year, month, day, time %/% 100, time %% 100)\n}\n\nflights_dt <- flights %>% \n  filter(!is.na(dep_time), !is.na(arr_time)) %>% \n  mutate(\n    dep_time = make_datetime_100(year, month, day, dep_time),\n    arr_time = make_datetime_100(year, month, day, arr_time),\n    sched_dep_time = make_datetime_100(year, month, day, sched_dep_time),\n    sched_arr_time = make_datetime_100(year, month, day, sched_arr_time)\n  ) %>% \n  select(origin, dest, ends_with(\"delay\"), ends_with(\"time\"))\n\nflights_dt\n#> # A tibble: 328,063 x 9\n#>   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n#>   <chr>  <chr>     <dbl>     <dbl> <dttm>              <dttm>             \n#> 1 EWR    IAH           2        11 2013-01-01 05:17:00 2013-01-01 05:15:00\n#> 2 LGA    IAH           4        20 2013-01-01 05:33:00 2013-01-01 05:29:00\n#> 3 JFK    MIA           2        33 2013-01-01 05:42:00 2013-01-01 05:40:00\n#> 4 JFK    BQN          -1       -18 2013-01-01 05:44:00 2013-01-01 05:45:00\n#> 5 LGA    ATL          -6       -25 2013-01-01 05:54:00 2013-01-01 06:00:00\n#> 6 EWR    ORD          -4        12 2013-01-01 05:54:00 2013-01-01 05:58:00\n#> # … with 328,057 more rows, and 3 more variables: arr_time <dttm>,\n#> #   sched_arr_time <dttm>, air_time <dbl>\nflights_dt %>% \n  ggplot(aes(dep_time)) + \n  geom_freqpoly(binwidth = 86400) # 86400 seconds = 1 day\nflights_dt %>% \n  filter(dep_time < ymd(20130102)) %>% \n  ggplot(aes(dep_time)) + \n  geom_freqpoly(binwidth = 600) # 600 s = 10 minutes"},{"path":"dates-and-times.html","id":"from-other-types","chapter":"16 Dates and times","heading":"16.2.3 From other types","text":"may want switch date-time date. ’s job as_datetime() as_date():Sometimes ’ll get date/times numeric offsets “Unix Epoch”, 1970-01-01. offset seconds, use as_datetime(); ’s days, use as_date().","code":"\nas_datetime(today())\n#> [1] \"2021-02-14 UTC\"\nas_date(now())\n#> [1] \"2021-02-14\"\nas_datetime(60 * 60 * 10)\n#> [1] \"1970-01-01 10:00:00 UTC\"\nas_date(365 * 10 + 2)\n#> [1] \"1980-01-01\""},{"path":"dates-and-times.html","id":"exercises-47","chapter":"16 Dates and times","heading":"16.2.4 Exercises","text":"happens parse string contains invalid dates?\n\nymd(c(\"2010-10-10\", \"bananas\"))happens parse string contains invalid dates?tzone argument today() ? important?tzone argument today() ? important?Use appropriate lubridate function parse following dates:\n\nd1 <- \"January 1, 2010\"\nd2 <- \"2015-Mar-07\"\nd3 <- \"06-Jun-2017\"\nd4 <- c(\"August 19 (2015)\", \"July 1 (2015)\")\nd5 <- \"12/30/14\" # Dec 30, 2014Use appropriate lubridate function parse following dates:","code":"\nymd(c(\"2010-10-10\", \"bananas\"))\nd1 <- \"January 1, 2010\"\nd2 <- \"2015-Mar-07\"\nd3 <- \"06-Jun-2017\"\nd4 <- c(\"August 19 (2015)\", \"July 1 (2015)\")\nd5 <- \"12/30/14\" # Dec 30, 2014"},{"path":"dates-and-times.html","id":"date-time-components","chapter":"16 Dates and times","heading":"16.3 Date-time components","text":"Now know get date-time data R’s date-time data structures, let’s explore can . section focus accessor functions let get set individual components. next section look arithmetic works date-times.","code":""},{"path":"dates-and-times.html","id":"getting-components","chapter":"16 Dates and times","heading":"16.3.1 Getting components","text":"can pull individual parts date accessor functions year(), month(), mday() (day month), yday() (day year), wday() (day week), hour(), minute(), second().month() wday() can set label = TRUE return abbreviated name month day week. Set abbr = FALSE return full name.can use wday() see flights depart week weekend:’s interesting pattern look average departure delay minute within hour. looks like flights leaving minutes 20-30 50-60 much lower delays rest hour!Interestingly, look scheduled departure time don’t see strong pattern:see pattern actual departure times? Well, like much data collected humans, ’s strong bias towards flights leaving “nice” departure times. Always alert sort pattern whenever work data involves human judgement!","code":"\ndatetime <- ymd_hms(\"2016-07-08 12:34:56\")\n\nyear(datetime)\n#> [1] 2016\nmonth(datetime)\n#> [1] 7\nmday(datetime)\n#> [1] 8\n\nyday(datetime)\n#> [1] 190\nwday(datetime)\n#> [1] 6\nmonth(datetime, label = TRUE)\n#> [1] Jul\n#> 12 Levels: Jan < Feb < Mar < Apr < May < Jun < Jul < Aug < Sep < ... < Dec\nwday(datetime, label = TRUE, abbr = FALSE)\n#> [1] Friday\n#> 7 Levels: Sunday < Monday < Tuesday < Wednesday < Thursday < ... < Saturday\nflights_dt %>% \n  mutate(wday = wday(dep_time, label = TRUE)) %>% \n  ggplot(aes(x = wday)) +\n    geom_bar()\nflights_dt %>% \n  mutate(minute = minute(dep_time)) %>% \n  group_by(minute) %>% \n  summarise(\n    avg_delay = mean(arr_delay, na.rm = TRUE),\n    n = n()) %>% \n  ggplot(aes(minute, avg_delay)) +\n    geom_line()\nsched_dep <- flights_dt %>% \n  mutate(minute = minute(sched_dep_time)) %>% \n  group_by(minute) %>% \n  summarise(\n    avg_delay = mean(arr_delay, na.rm = TRUE),\n    n = n())\n\nggplot(sched_dep, aes(minute, avg_delay)) +\n  geom_line()\nggplot(sched_dep, aes(minute, n)) +\n  geom_line()"},{"path":"dates-and-times.html","id":"rounding","chapter":"16 Dates and times","heading":"16.3.2 Rounding","text":"alternative approach plotting individual components round date nearby unit time, floor_date(), round_date(), ceiling_date(). function takes vector dates adjust name unit round (floor), round (ceiling), round . , example, allows us plot number flights per week:Computing difference rounded unrounded date can particularly useful.","code":"\nflights_dt %>% \n  count(week = floor_date(dep_time, \"week\")) %>% \n  ggplot(aes(week, n)) +\n    geom_line()"},{"path":"dates-and-times.html","id":"setting-components","chapter":"16 Dates and times","heading":"16.3.3 Setting components","text":"can also use accessor function set components date/time:Alternatively, rather modifying place, can create new date-time update(). also allows set multiple values .values big, roll-:can use update() show distribution flights across course day every day year:Setting larger components date constant powerful technique allows explore patterns smaller components.","code":"\n(datetime <- ymd_hms(\"2016-07-08 12:34:56\"))\n#> [1] \"2016-07-08 12:34:56 UTC\"\n\nyear(datetime) <- 2020\ndatetime\n#> [1] \"2020-07-08 12:34:56 UTC\"\nmonth(datetime) <- 01\ndatetime\n#> [1] \"2020-01-08 12:34:56 UTC\"\nhour(datetime) <- hour(datetime) + 1\ndatetime\n#> [1] \"2020-01-08 13:34:56 UTC\"\nupdate(datetime, year = 2020, month = 2, mday = 2, hour = 2)\n#> [1] \"2020-02-02 02:34:56 UTC\"\nymd(\"2015-02-01\") %>% \n  update(mday = 30)\n#> [1] \"2015-03-02\"\nymd(\"2015-02-01\") %>% \n  update(hour = 400)\n#> [1] \"2015-02-17 16:00:00 UTC\"\nflights_dt %>% \n  mutate(dep_hour = update(dep_time, yday = 1)) %>% \n  ggplot(aes(dep_hour)) +\n    geom_freqpoly(binwidth = 300)"},{"path":"dates-and-times.html","id":"exercises-48","chapter":"16 Dates and times","heading":"16.3.4 Exercises","text":"distribution flight times within day change \ncourse year?distribution flight times within day change \ncourse year?Compare dep_time, sched_dep_time dep_delay. consistent?\nExplain findings.Compare dep_time, sched_dep_time dep_delay. consistent?\nExplain findings.Compare air_time duration departure arrival.\nExplain findings. (Hint: consider location airport.)Compare air_time duration departure arrival.\nExplain findings. (Hint: consider location airport.)average delay time change course day?\nuse dep_time sched_dep_time? ?average delay time change course day?\nuse dep_time sched_dep_time? ?day week leave want minimise \nchance delay?day week leave want minimise \nchance delay?makes distribution diamonds$carat \nflights$sched_dep_time similar?makes distribution diamonds$carat \nflights$sched_dep_time similar?Confirm hypothesis early departures flights minutes\n20-30 50-60 caused scheduled flights leave early.\nHint: create binary variable tells whether flight\ndelayed.Confirm hypothesis early departures flights minutes\n20-30 50-60 caused scheduled flights leave early.\nHint: create binary variable tells whether flight\ndelayed.","code":""},{"path":"dates-and-times.html","id":"time-spans","chapter":"16 Dates and times","heading":"16.4 Time spans","text":"Next ’ll learn arithmetic dates works, including subtraction, addition, division. Along way, ’ll learn three important classes represent time spans:durations, represent exact number seconds.periods, represent human units like weeks months.intervals, represent starting ending point.","code":""},{"path":"dates-and-times.html","id":"durations","chapter":"16 Dates and times","heading":"16.4.1 Durations","text":"R, subtract two dates, get difftime object:difftime class object records time span seconds, minutes, hours, days, weeks. ambiguity can make difftimes little painful work , lubridate provides alternative always uses seconds: duration.Durations come bunch convenient constructors:Durations always record time span seconds. Larger units created converting minutes, hours, days, weeks, years seconds standard rate (60 seconds minute, 60 minutes hour, 24 hours day, 7 days week, 365 days year).can add multiply durations:can add subtract durations days:However, durations represent exact number seconds, sometimes might get unexpected result:one day 1pm March 12, 2pm March 13?! look carefully date might also notice time zones changed. DST, March 12 23 hours, add full days worth seconds end different time.","code":"\n# How old is Hadley?\nh_age <- today() - ymd(19791014)\nh_age\n#> Time difference of 15099 days\nas.duration(h_age)\n#> [1] \"1304553600s (~41.34 years)\"\ndseconds(15)\n#> [1] \"15s\"\ndminutes(10)\n#> [1] \"600s (~10 minutes)\"\ndhours(c(12, 24))\n#> [1] \"43200s (~12 hours)\" \"86400s (~1 days)\"\nddays(0:5)\n#> [1] \"0s\"                \"86400s (~1 days)\"  \"172800s (~2 days)\"\n#> [4] \"259200s (~3 days)\" \"345600s (~4 days)\" \"432000s (~5 days)\"\ndweeks(3)\n#> [1] \"1814400s (~3 weeks)\"\ndyears(1)\n#> [1] \"31557600s (~1 years)\"\n2 * dyears(1)\n#> [1] \"63115200s (~2 years)\"\ndyears(1) + dweeks(12) + dhours(15)\n#> [1] \"38869200s (~1.23 years)\"\ntomorrow <- today() + ddays(1)\nlast_year <- today() - dyears(1)\none_pm <- ymd_hms(\"2016-03-12 13:00:00\", tz = \"America/New_York\")\n\none_pm\n#> [1] \"2016-03-12 13:00:00 EST\"\none_pm + ddays(1)\n#> [1] \"2016-03-13 14:00:00 EDT\""},{"path":"dates-and-times.html","id":"periods","chapter":"16 Dates and times","heading":"16.4.2 Periods","text":"solve problem, lubridate provides periods. Periods time spans don’t fixed length seconds, instead work “human” times, like days months. allows work intuitive way:Like durations, periods can created number friendly constructor functions.can add multiply periods:course, add dates. Compared durations, periods likely expect:Let’s use periods fix oddity related flight dates. planes appear arrived destination departed New York City.overnight flights. used date information departure arrival times, flights arrived following day. can fix adding days(1) arrival time overnight flight.Now flights obey laws physics.","code":"\none_pm\n#> [1] \"2016-03-12 13:00:00 EST\"\none_pm + days(1)\n#> [1] \"2016-03-13 13:00:00 EDT\"\nseconds(15)\n#> [1] \"15S\"\nminutes(10)\n#> [1] \"10M 0S\"\nhours(c(12, 24))\n#> [1] \"12H 0M 0S\" \"24H 0M 0S\"\ndays(7)\n#> [1] \"7d 0H 0M 0S\"\nmonths(1:6)\n#> [1] \"1m 0d 0H 0M 0S\" \"2m 0d 0H 0M 0S\" \"3m 0d 0H 0M 0S\" \"4m 0d 0H 0M 0S\"\n#> [5] \"5m 0d 0H 0M 0S\" \"6m 0d 0H 0M 0S\"\nweeks(3)\n#> [1] \"21d 0H 0M 0S\"\nyears(1)\n#> [1] \"1y 0m 0d 0H 0M 0S\"\n10 * (months(6) + days(1))\n#> [1] \"60m 10d 0H 0M 0S\"\ndays(50) + hours(25) + minutes(2)\n#> [1] \"50d 25H 2M 0S\"\n# A leap year\nymd(\"2016-01-01\") + dyears(1)\n#> [1] \"2016-12-31 06:00:00 UTC\"\nymd(\"2016-01-01\") + years(1)\n#> [1] \"2017-01-01\"\n\n# Daylight Savings Time\none_pm + ddays(1)\n#> [1] \"2016-03-13 14:00:00 EDT\"\none_pm + days(1)\n#> [1] \"2016-03-13 13:00:00 EDT\"\nflights_dt %>% \n  filter(arr_time < dep_time) \n#> # A tibble: 10,633 x 9\n#>   origin dest  dep_delay arr_delay dep_time            sched_dep_time     \n#>   <chr>  <chr>     <dbl>     <dbl> <dttm>              <dttm>             \n#> 1 EWR    BQN           9        -4 2013-01-01 19:29:00 2013-01-01 19:20:00\n#> 2 JFK    DFW          59        NA 2013-01-01 19:39:00 2013-01-01 18:40:00\n#> 3 EWR    TPA          -2         9 2013-01-01 20:58:00 2013-01-01 21:00:00\n#> 4 EWR    SJU          -6       -12 2013-01-01 21:02:00 2013-01-01 21:08:00\n#> 5 EWR    SFO          11       -14 2013-01-01 21:08:00 2013-01-01 20:57:00\n#> 6 LGA    FLL         -10        -2 2013-01-01 21:20:00 2013-01-01 21:30:00\n#> # … with 10,627 more rows, and 3 more variables: arr_time <dttm>,\n#> #   sched_arr_time <dttm>, air_time <dbl>\nflights_dt <- flights_dt %>% \n  mutate(\n    overnight = arr_time < dep_time,\n    arr_time = arr_time + days(overnight * 1),\n    sched_arr_time = sched_arr_time + days(overnight * 1)\n  )\nflights_dt %>% \n  filter(overnight, arr_time < dep_time) \n#> # A tibble: 0 x 10\n#> # … with 10 variables: origin <chr>, dest <chr>, dep_delay <dbl>,\n#> #   arr_delay <dbl>, dep_time <dttm>, sched_dep_time <dttm>, arr_time <dttm>,\n#> #   sched_arr_time <dttm>, air_time <dbl>, overnight <lgl>"},{"path":"dates-and-times.html","id":"intervals","chapter":"16 Dates and times","heading":"16.4.3 Intervals","text":"’s obvious dyears(1) / ddays(365) return: one, durations always represented number seconds, duration year defined 365 days worth seconds.years(1) / days(1) return? Well, year 2015 return 365, 2016, return 366! ’s quite enough information lubridate give single clear answer. instead give estimate, warning:want accurate measurement, ’ll use interval. interval duration starting point: makes precise can determine exactly long :find many periods fall interval, need use integer division:","code":"\nyears(1) / days(1)\n#> [1] 365.25\nnext_year <- today() + years(1)\n(today() %--% next_year) / ddays(1)\n#> [1] 365\n(today() %--% next_year) %/% days(1)\n#> [1] 365"},{"path":"dates-and-times.html","id":"summary-1","chapter":"16 Dates and times","heading":"16.4.4 Summary","text":"pick duration, periods, intervals? always, pick simplest data structure solves problem. care physical time, use duration; need add human times, use period; need figure long span human units, use interval.Figure 16.1 summarises permitted arithmetic operations different data types.\nFigure 16.1: allowed arithmetic operations pairs date/time classes.\n","code":""},{"path":"dates-and-times.html","id":"exercises-49","chapter":"16 Dates and times","heading":"16.4.5 Exercises","text":"months() dmonths()?months() dmonths()?Explain days(overnight * 1) someone just started\nlearning R. work?Explain days(overnight * 1) someone just started\nlearning R. work?Create vector dates giving first day every month 2015.\nCreate vector dates giving first day every month\ncurrent year.Create vector dates giving first day every month 2015.\nCreate vector dates giving first day every month\ncurrent year.Write function given birthday (date), returns\nold years.Write function given birthday (date), returns\nold years.can’t (today() %--% (today() + years(1))) / months(1) work?can’t (today() %--% (today() + years(1))) / months(1) work?","code":""},{"path":"dates-and-times.html","id":"time-zones","chapter":"16 Dates and times","heading":"16.5 Time zones","text":"Time zones enormously complicated topic interaction geopolitical entities. Fortunately don’t need dig details ’re important data analysis, challenges ’ll need tackle head .first challenge everyday names time zones tend ambiguous. example, ’re American ’re probably familiar EST, Eastern Standard Time. However, Australia Canada also EST! avoid confusion, R uses international standard IANA time zones. use consistent naming scheme “/”, typically form “<continent>/<city>” (exceptions every country lies continent). Examples include “America/New_York”, “Europe/Paris”, “Pacific/Auckland”.might wonder time zone uses city, typically think time zones associated country region within country. IANA database record decades worth time zone rules. course decades, countries change names (break apart) fairly frequently, city names tend stay . Another problem name needs reflect current behaviour, also complete history. example, time zones “America/New_York” “America/Detroit”. cities currently use Eastern Standard Time 1969-1972 Michigan (state Detroit located), follow DST, needs different name. ’s worth reading raw time zone database (available http://www.iana.org/time-zones) just read stories!can find R thinks current time zone Sys.timezone():(R doesn’t know, ’ll get NA.)see complete list time zone names OlsonNames():R, time zone attribute date-time controls printing. example, three objects represent instant time:can verify ’re time using subtraction:Unless otherwise specified, lubridate always uses UTC. UTC (Coordinated Universal Time) standard time zone used scientific community roughly equivalent predecessor GMT (Greenwich Mean Time). DST, makes convenient representation computation. Operations combine date-times, like c(), often drop time zone. case, date-times display local time zone:can change time zone two ways:Keep instant time , change ’s displayed.\nUse instant correct, want natural\ndisplay.\n\nx4a <- with_tz(x4, tzone = \"Australia/Lord_Howe\")\nx4a\n#> [1] \"2015-06-02 02:30:00 +1030\" \"2015-06-02 02:30:00 +1030\"\n#> [3] \"2015-06-02 02:30:00 +1030\"\nx4a - x4\n#> Time differences secs\n#> [1] 0 0 0\n(also illustrates another challenge times zones: ’re \ninteger hour offsets!)Keep instant time , change ’s displayed.\nUse instant correct, want natural\ndisplay.(also illustrates another challenge times zones: ’re \ninteger hour offsets!)Change underlying instant time. Use \ninstant labelled incorrect time zone, \nneed fix .\n\nx4b <- force_tz(x4, tzone = \"Australia/Lord_Howe\")\nx4b\n#> [1] \"2015-06-01 12:00:00 +1030\" \"2015-06-01 12:00:00 +1030\"\n#> [3] \"2015-06-01 12:00:00 +1030\"\nx4b - x4\n#> Time differences hours\n#> [1] -14.5 -14.5 -14.5Change underlying instant time. Use \ninstant labelled incorrect time zone, \nneed fix .","code":"\nSys.timezone()\n#> [1] \"Asia/Hong_Kong\"\nlength(OlsonNames())\n#> [1] 594\nhead(OlsonNames())\n#> [1] \"Africa/Abidjan\"     \"Africa/Accra\"       \"Africa/Addis_Ababa\"\n#> [4] \"Africa/Algiers\"     \"Africa/Asmara\"      \"Africa/Asmera\"\n(x1 <- ymd_hms(\"2015-06-01 12:00:00\", tz = \"America/New_York\"))\n#> [1] \"2015-06-01 12:00:00 EDT\"\n(x2 <- ymd_hms(\"2015-06-01 18:00:00\", tz = \"Europe/Copenhagen\"))\n#> [1] \"2015-06-01 18:00:00 CEST\"\n(x3 <- ymd_hms(\"2015-06-02 04:00:00\", tz = \"Pacific/Auckland\"))\n#> [1] \"2015-06-02 04:00:00 NZST\"\nx1 - x2\n#> Time difference of 0 secs\nx1 - x3\n#> Time difference of 0 secs\nx4 <- c(x1, x2, x3)\nx4\n#> [1] \"2015-06-01 12:00:00 EDT\" \"2015-06-01 12:00:00 EDT\"\n#> [3] \"2015-06-01 12:00:00 EDT\"\nx4a <- with_tz(x4, tzone = \"Australia/Lord_Howe\")\nx4a\n#> [1] \"2015-06-02 02:30:00 +1030\" \"2015-06-02 02:30:00 +1030\"\n#> [3] \"2015-06-02 02:30:00 +1030\"\nx4a - x4\n#> Time differences in secs\n#> [1] 0 0 0\nx4b <- force_tz(x4, tzone = \"Australia/Lord_Howe\")\nx4b\n#> [1] \"2015-06-01 12:00:00 +1030\" \"2015-06-01 12:00:00 +1030\"\n#> [3] \"2015-06-01 12:00:00 +1030\"\nx4b - x4\n#> Time differences in hours\n#> [1] -14.5 -14.5 -14.5"},{"path":"program-intro.html","id":"program-intro","chapter":"17 Introduction","heading":"17 Introduction","text":"part book, ’ll improve programming skills. Programming cross-cutting skill needed data science work: must use computer data science; head, pencil paper.Programming produces code, code tool communication. Obviously code tells computer want . also communicates meaning humans. Thinking code vehicle communication important every project fundamentally collaborative. Even ’re working people, ’ll definitely working future-! Writing clear code important others (like future-) can understand tackled analysis way . means getting better programming also involves getting better communicating. time, want code become just easier write, easier others read.Writing code similar many ways writing prose. One parallel find particularly useful cases rewriting key clarity. first expression ideas unlikely particularly clear, may need rewrite multiple times. solving data analysis challenge, ’s often worth looking code thinking whether ’s obvious ’ve done. spend little time rewriting code ideas fresh, can save lot time later trying recreate code . doesn’t mean rewrite every function: need balance need achieve now saving time long run. (rewrite functions likely first attempt clear.)following four chapters, ’ll learn skills allow tackle new programs solve existing problems greater clarity ease:pipes, dive deep pipe, %>%, learn \nworks, alternatives , use .pipes, dive deep pipe, %>%, learn \nworks, alternatives , use .Copy--paste powerful tool, avoid \ntwice. Repeating code dangerous can easily lead\nerrors inconsistencies. Instead, functions, ’ll learn\nwrite functions let extract repeated code \ncan easily reused.Copy--paste powerful tool, avoid \ntwice. Repeating code dangerous can easily lead\nerrors inconsistencies. Instead, functions, ’ll learn\nwrite functions let extract repeated code \ncan easily reused.start write powerful functions, ’ll need solid\ngrounding R’s data structures, provided vectors. must master\nfour common atomic vectors, three important S3 classes built \ntop , understand mysteries list data frame.start write powerful functions, ’ll need solid\ngrounding R’s data structures, provided vectors. must master\nfour common atomic vectors, three important S3 classes built \ntop , understand mysteries list data frame.Functions extract repeated code, often need repeat \nactions different inputs. need tools iteration \nlet similar things . tools include loops\nfunctional programming, ’ll learn iteration.Functions extract repeated code, often need repeat \nactions different inputs. need tools iteration \nlet similar things . tools include loops\nfunctional programming, ’ll learn iteration.","code":""},{"path":"program-intro.html","id":"learning-more-2","chapter":"17 Introduction","heading":"17.1 Learning more","text":"goal chapters teach minimum programming need practice data science, turns reasonable amount. mastered material book, strongly believe invest programming skills. Learning programming long-term investment: won’t pay immediately, long term allow solve new problems quickly, let reuse insights previous problems new scenarios.learn need study R programming language, just interactive environment data science. written two books help :Hands Programming R,\nGarrett Grolemund. introduction R programming language\ngreat place start R first programming language. \ncovers similar material chapters, different style \ndifferent motivation examples (based casino). ’s useful complement\nfind four chapters go quickly.Hands Programming R,\nGarrett Grolemund. introduction R programming language\ngreat place start R first programming language. \ncovers similar material chapters, different style \ndifferent motivation examples (based casino). ’s useful complement\nfind four chapters go quickly.Advanced R Hadley Wickham. dives \ndetails R programming language. great place start \nexisting programming experience. ’s also great next step ’ve\ninternalised ideas chapters. can read online \nhttp://adv-r..co.nz.Advanced R Hadley Wickham. dives \ndetails R programming language. great place start \nexisting programming experience. ’s also great next step ’ve\ninternalised ideas chapters. can read online \nhttp://adv-r..co.nz.","code":""},{"path":"pipes.html","id":"pipes","chapter":"18 Pipes","heading":"18 Pipes","text":"","code":""},{"path":"pipes.html","id":"introduction-11","chapter":"18 Pipes","heading":"18.1 Introduction","text":"Pipes powerful tool clearly expressing sequence multiple operations. far, ’ve using without knowing work, alternatives . Now, chapter, ’s time explore pipe detail. ’ll learn alternatives pipe, shouldn’t use pipe, useful related tools.","code":""},{"path":"pipes.html","id":"prerequisites-11","chapter":"18 Pipes","heading":"18.1.1 Prerequisites","text":"pipe, %>%, comes magrittr package Stefan Milton Bache. Packages tidyverse load %>% automatically, don’t usually load magrittr explicitly. , however, ’re focussing piping, aren’t loading packages, load explicitly.","code":"\nlibrary(magrittr)"},{"path":"pipes.html","id":"piping-alternatives","chapter":"18 Pipes","heading":"18.2 Piping alternatives","text":"point pipe help write code way easier read understand. see pipe useful, ’re going explore number ways writing code. Let’s use code tell story little bunny named Foo Foo:Little bunny Foo Foo\nWent hopping forest\nScooping field mice\nbopping headThis popular Children’s poem accompanied hand actions.’ll start defining object represent little bunny Foo Foo:’ll use function key verb: hop(), scoop(), bop(). Using object verbs, (least) four ways retell story code:Save intermediate step new object.Overwrite original object many times.Compose functions.Use pipe.’ll work approach, showing code talking advantages disadvantages.","code":"\nfoo_foo <- little_bunny()"},{"path":"pipes.html","id":"intermediate-steps","chapter":"18 Pipes","heading":"18.2.1 Intermediate steps","text":"simplest approach save step new object:main downside form forces name intermediate element. natural names, good idea, . many times, like example, aren’t natural names, add numeric suffixes make names unique. leads two problems:code cluttered unimportant namesThe code cluttered unimportant namesYou carefully increment suffix line.carefully increment suffix line.Whenever write code like , invariably use wrong number one line spend 10 minutes scratching head trying figure went wrong code.may also worry form creates many copies data takes lot memory. Surprisingly, ’s case. First, note proactively worrying memory useful way spend time: worry becomes problem (.e. run memory), . Second, R isn’t stupid, share columns across data frames, possible. Let’s take look actual data manipulation pipeline add new column ggplot2::diamonds:pryr::object_size() gives memory occupied arguments. results seem counterintuitive first:diamonds takes 3.46 MB,diamonds2 takes 3.89 MB,diamonds diamonds2 together take 3.89 MB!can work? Well, diamonds2 10 columns common diamonds: ’s need duplicate data, two data frames variables common. variables get copied modify one . following example, modify single value diamonds$carat. means carat variable can longer shared two data frames, copy must made. size data frame unchanged, collective size increases:(Note use pryr::object_size() , built-object.size(). object.size() takes single object can’t compute data shared across multiple objects.)","code":"\nfoo_foo_1 <- hop(foo_foo, through = forest)\nfoo_foo_2 <- scoop(foo_foo_1, up = field_mice)\nfoo_foo_3 <- bop(foo_foo_2, on = head)\ndiamonds <- ggplot2::diamonds\ndiamonds2 <- diamonds %>% \n  dplyr::mutate(price_per_carat = price / carat)\n\npryr::object_size(diamonds)\n#> Registered S3 method overwritten by 'pryr':\n#>   method      from\n#>   print.bytes Rcpp\n#> 3.46 MB\npryr::object_size(diamonds2)\n#> 3.89 MB\npryr::object_size(diamonds, diamonds2)\n#> 3.89 MB\ndiamonds$carat[1] <- NA\npryr::object_size(diamonds)\n#> 3.46 MB\npryr::object_size(diamonds2)\n#> 3.89 MB\npryr::object_size(diamonds, diamonds2)\n#> 4.32 MB"},{"path":"pipes.html","id":"overwrite-the-original","chapter":"18 Pipes","heading":"18.2.2 Overwrite the original","text":"Instead creating intermediate objects step, overwrite original object:less typing (less thinking), ’re less likely make mistakes. However, two problems:Debugging painful: make mistake ’ll need re-run \ncomplete pipeline beginning.Debugging painful: make mistake ’ll need re-run \ncomplete pipeline beginning.repetition object transformed (’ve written foo_foo six\ntimes!) obscures ’s changing line.repetition object transformed (’ve written foo_foo six\ntimes!) obscures ’s changing line.","code":"\nfoo_foo <- hop(foo_foo, through = forest)\nfoo_foo <- scoop(foo_foo, up = field_mice)\nfoo_foo <- bop(foo_foo, on = head)"},{"path":"pipes.html","id":"function-composition","chapter":"18 Pipes","heading":"18.2.3 Function composition","text":"Another approach abandon assignment just string function calls together:disadvantage read inside-, right--left, arguments end spread far apart (evocatively called \nDagwood sandwich problem). short, code hard human consume.","code":"\nbop(\n  scoop(\n    hop(foo_foo, through = forest),\n    up = field_mice\n  ), \n  on = head\n)"},{"path":"pipes.html","id":"use-the-pipe","chapter":"18 Pipes","heading":"18.2.4 Use the pipe","text":"Finally, can use pipe:favourite form, focusses verbs, nouns. can read series function compositions like ’s set imperative actions. Foo Foo hops, scoops, bops. downside, course, need familiar pipe. ’ve never seen %>% , ’ll idea code . Fortunately, people pick idea quickly, share code others aren’t familiar pipe, can easily teach .pipe works performing “lexical transformation”: behind scenes, magrittr reassembles code pipe form works overwriting intermediate object. run pipe like one , magrittr something like :means pipe won’t work two classes functions:Functions use current environment. example, assign()\ncreate new variable given name current environment:\n\nassign(\"x\", 10)\nx\n#> [1] 10\n\n\"x\" %>% assign(100)\nx\n#> [1] 10\nuse assign pipe work assigns \ntemporary environment used %>%. want use assign \npipe, must explicit environment:\n\nenv <- environment()\n\"x\" %>% assign(100, envir = env)\nx\n#> [1] 100\nfunctions problem include get() load().Functions use current environment. example, assign()\ncreate new variable given name current environment:use assign pipe work assigns \ntemporary environment used %>%. want use assign \npipe, must explicit environment:functions problem include get() load().Functions use lazy evaluation. R, function arguments\ncomputed function uses , prior calling \nfunction. pipe computes element turn, can’t\nrely behaviour.\nOne place problem tryCatch(), lets capture\nhandle errors:\n\ntryCatch(stop(\"!\"), error = function(e) \"error\")\n#> [1] \"error\"\n\nstop(\"!\") %>% \n  tryCatch(error = function(e) \"error\")\n#> [1] \"error\"\nrelatively wide class functions behaviour,\nincluding try(), suppressMessages(), suppressWarnings()\nbase R.Functions use lazy evaluation. R, function arguments\ncomputed function uses , prior calling \nfunction. pipe computes element turn, can’t\nrely behaviour.One place problem tryCatch(), lets capture\nhandle errors:relatively wide class functions behaviour,\nincluding try(), suppressMessages(), suppressWarnings()\nbase R.","code":"\nfoo_foo %>%\n  hop(through = forest) %>%\n  scoop(up = field_mice) %>%\n  bop(on = head)\nmy_pipe <- function(.) {\n  . <- hop(., through = forest)\n  . <- scoop(., up = field_mice)\n  bop(., on = head)\n}\nmy_pipe(foo_foo)\nassign(\"x\", 10)\nx\n#> [1] 10\n\n\"x\" %>% assign(100)\nx\n#> [1] 10\nenv <- environment()\n\"x\" %>% assign(100, envir = env)\nx\n#> [1] 100\ntryCatch(stop(\"!\"), error = function(e) \"An error\")\n#> [1] \"An error\"\n\nstop(\"!\") %>% \n  tryCatch(error = function(e) \"An error\")\n#> [1] \"An error\""},{"path":"pipes.html","id":"when-not-to-use-the-pipe","chapter":"18 Pipes","heading":"18.3 When not to use the pipe","text":"pipe powerful tool, ’s tool disposal, doesn’t solve every problem! Pipes useful rewriting fairly short linear sequence operations. think reach another tool :pipes longer (say) ten steps. case, create\nintermediate objects meaningful names. make debugging easier,\ncan easily check intermediate results, makes\neasier understand code, variable names can help\ncommunicate intent.pipes longer (say) ten steps. case, create\nintermediate objects meaningful names. make debugging easier,\ncan easily check intermediate results, makes\neasier understand code, variable names can help\ncommunicate intent.multiple inputs outputs. isn’t one primary object\ntransformed, two objects combined together,\ndon’t use pipe.multiple inputs outputs. isn’t one primary object\ntransformed, two objects combined together,\ndon’t use pipe.starting think directed graph complex\ndependency structure. Pipes fundamentally linear expressing\ncomplex relationships typically yield confusing code.starting think directed graph complex\ndependency structure. Pipes fundamentally linear expressing\ncomplex relationships typically yield confusing code.","code":""},{"path":"pipes.html","id":"other-tools-from-magrittr","chapter":"18 Pipes","heading":"18.4 Other tools from magrittr","text":"packages tidyverse automatically make %>% available , don’t normally load magrittr explicitly. However, useful tools inside magrittr might want try :working complex pipes, ’s sometimes useful call \nfunction side-effects. Maybe want print current\nobject, plot , save disk. Many times, functions don’t\nreturn anything, effectively terminating pipe.\nwork around problem, can use “tee” pipe. %T>% works like\n%>% except returns left-hand side instead right-hand\nside. ’s called “tee” ’s like literal T-shaped pipe.\n\nrnorm(100) %>%\n  matrix(ncol = 2) %>%\n  plot() %>%\n  str()\n#>  NULL\n\nrnorm(100) %>%\n  matrix(ncol = 2) %T>%\n  plot() %>%\n  str()\n#>  num [1:50, 1:2] -0.387 -0.785 -1.057 -0.796 -1.756 ...\nworking complex pipes, ’s sometimes useful call \nfunction side-effects. Maybe want print current\nobject, plot , save disk. Many times, functions don’t\nreturn anything, effectively terminating pipe.work around problem, can use “tee” pipe. %T>% works like\n%>% except returns left-hand side instead right-hand\nside. ’s called “tee” ’s like literal T-shaped pipe.’re working functions don’t data frame based API\n(.e. pass individual vectors, data frame expressions\nevaluated context data frame), might find %$%\nuseful. “explodes” variables data frame can\nrefer explicitly. useful working many functions\nbase R:\n\nmtcars %$%\n  cor(disp, mpg)\n#> [1] -0.8475514If ’re working functions don’t data frame based API\n(.e. pass individual vectors, data frame expressions\nevaluated context data frame), might find %$%\nuseful. “explodes” variables data frame can\nrefer explicitly. useful working many functions\nbase R:assignment magrittr provides %<>% operator allows \nreplace code like:\n\nmtcars <- mtcars %>% \n  transform(cyl = cyl * 2)\n\n\nmtcars %<>% transform(cyl = cyl * 2)\n’m fan operator think assignment \nspecial operation always clear ’s occurring.\nopinion, little bit duplication (.e. repeating \nname object twice) fine return making assignment\nexplicit.assignment magrittr provides %<>% operator allows \nreplace code like:withI’m fan operator think assignment \nspecial operation always clear ’s occurring.\nopinion, little bit duplication (.e. repeating \nname object twice) fine return making assignment\nexplicit.","code":"\nrnorm(100) %>%\n  matrix(ncol = 2) %>%\n  plot() %>%\n  str()\n#>  NULL\n\nrnorm(100) %>%\n  matrix(ncol = 2) %T>%\n  plot() %>%\n  str()\n#>  num [1:50, 1:2] -0.387 -0.785 -1.057 -0.796 -1.756 ...\nmtcars %$%\n  cor(disp, mpg)\n#> [1] -0.8475514\nmtcars <- mtcars %>% \n  transform(cyl = cyl * 2)\nmtcars %<>% transform(cyl = cyl * 2)"},{"path":"functions.html","id":"functions","chapter":"19 Functions","heading":"19 Functions","text":"","code":""},{"path":"functions.html","id":"introduction-12","chapter":"19 Functions","heading":"19.1 Introduction","text":"One best ways improve reach data scientist write functions. Functions allow automate common tasks powerful general way copy--pasting. Writing function three big advantages using copy--paste:can give function evocative name makes code easier \nunderstand.can give function evocative name makes code easier \nunderstand.requirements change, need update code one place, instead\nmany.requirements change, need update code one place, instead\nmany.eliminate chance making incidental mistakes copy \npaste (.e. updating variable name one place, another).eliminate chance making incidental mistakes copy \npaste (.e. updating variable name one place, another).Writing good functions lifetime journey. Even using R many years still learn new techniques better ways approaching old problems. goal chapter teach every esoteric detail functions get started pragmatic advice can apply immediately.well practical advice writing functions, chapter also gives suggestions style code. Good code style like correct punctuation. Youcanmanagewithoutit, sure makes things easier read! styles punctuation, many possible variations. present style use code, important thing consistent.","code":""},{"path":"functions.html","id":"prerequisites-12","chapter":"19 Functions","heading":"19.1.1 Prerequisites","text":"focus chapter writing functions base R, won’t need extra packages.","code":""},{"path":"functions.html","id":"when-should-you-write-a-function","chapter":"19 Functions","heading":"19.2 When should you write a function?","text":"consider writing function whenever ’ve copied pasted block code twice (.e. now three copies code). example, take look code. ?might able puzzle rescales column range 0 1. spot mistake? made error copying--pasting code df$b: forgot change b. Extracting repeated code function good idea prevents making type mistake.write function need first analyse code. many inputs ?code one input: df$. (’re surprised TRUE input, can explore exercise .) make inputs clear, ’s good idea rewrite code using temporary variables general names. code requires single numeric vector, ’ll call x:duplication code. ’re computing range data three times, makes sense one step:Pulling intermediate calculations named variables good practice makes clear code . Now ’ve simplified code, checked still works, can turn function:three key steps creating new function:need pick name function. ’ve used rescale01\nfunction rescales vector lie 0 1.need pick name function. ’ve used rescale01\nfunction rescales vector lie 0 1.list inputs, arguments, function inside function.\njust one argument. call look like\nfunction(x, y, z).list inputs, arguments, function inside function.\njust one argument. call look like\nfunction(x, y, z).place code developed body function, \n{ block immediately follows function(...).place code developed body function, \n{ block immediately follows function(...).Note overall process: made function ’d figured make work simple input. ’s easier start working code turn function; ’s harder create function try make work.point ’s good idea check function different inputs:write functions ’ll eventually want convert informal, interactive tests formal, automated tests. process called unit testing. Unfortunately, ’s beyond scope book, can learn http://r-pkgs..co.nz/tests.html.can simplify original example now function:Compared original, code easier understand ’ve eliminated one class copy--paste errors. still quite bit duplication since ’re thing multiple columns. ’ll learn eliminate duplication iteration, ’ve learned R’s data structures vectors.Another advantage functions requirements change, need make change one place. example, might discover variables include infinite values, rescale01() fails:’ve extracted code function, need make fix one place:important part “repeat ” (DRY) principle. repetition code, places need remember update things change (always !), likely create bugs time.","code":"\ndf <- tibble::tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\n\ndf$a <- (df$a - min(df$a, na.rm = TRUE)) / \n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$b <- (df$b - min(df$b, na.rm = TRUE)) / \n  (max(df$b, na.rm = TRUE) - min(df$a, na.rm = TRUE))\ndf$c <- (df$c - min(df$c, na.rm = TRUE)) / \n  (max(df$c, na.rm = TRUE) - min(df$c, na.rm = TRUE))\ndf$d <- (df$d - min(df$d, na.rm = TRUE)) / \n  (max(df$d, na.rm = TRUE) - min(df$d, na.rm = TRUE))\n(df$a - min(df$a, na.rm = TRUE)) /\n  (max(df$a, na.rm = TRUE) - min(df$a, na.rm = TRUE))\nx <- df$a\n(x - min(x, na.rm = TRUE)) / (max(x, na.rm = TRUE) - min(x, na.rm = TRUE))\n#>  [1] 0.2892677 0.7509271 0.0000000 0.6781686 0.8530656 1.0000000 0.1716402\n#>  [8] 0.6107464 0.6116181 0.6008793\nrng <- range(x, na.rm = TRUE)\n(x - rng[1]) / (rng[2] - rng[1])\n#>  [1] 0.2892677 0.7509271 0.0000000 0.6781686 0.8530656 1.0000000 0.1716402\n#>  [8] 0.6107464 0.6116181 0.6008793\nrescale01 <- function(x) {\n  rng <- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale01(c(0, 5, 10))\n#> [1] 0.0 0.5 1.0\nrescale01(c(-10, 0, 10))\n#> [1] 0.0 0.5 1.0\nrescale01(c(1, 2, 3, NA, 5))\n#> [1] 0.00 0.25 0.50   NA 1.00\ndf$a <- rescale01(df$a)\ndf$b <- rescale01(df$b)\ndf$c <- rescale01(df$c)\ndf$d <- rescale01(df$d)\nx <- c(1:10, Inf)\nrescale01(x)\n#>  [1]   0   0   0   0   0   0   0   0   0   0 NaN\nrescale01 <- function(x) {\n  rng <- range(x, na.rm = TRUE, finite = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\nrescale01(x)\n#>  [1] 0.0000000 0.1111111 0.2222222 0.3333333 0.4444444 0.5555556 0.6666667\n#>  [8] 0.7777778 0.8888889 1.0000000       Inf"},{"path":"functions.html","id":"exercises-50","chapter":"19 Functions","heading":"19.2.1 Exercises","text":"TRUE parameter rescale01()? happen \nx contained single missing value, na.rm FALSE?TRUE parameter rescale01()? happen \nx contained single missing value, na.rm FALSE?second variant rescale01(), infinite values left\nunchanged. Rewrite rescale01() -Inf mapped 0, \nInf mapped 1.second variant rescale01(), infinite values left\nunchanged. Rewrite rescale01() -Inf mapped 0, \nInf mapped 1.Practice turning following code snippets functions. Think \nfunction . call ? many arguments \nneed? Can rewrite expressive less duplicative?\n\nmean(.na(x))\n\nx / sum(x, na.rm = TRUE)\n\nsd(x, na.rm = TRUE) / mean(x, na.rm = TRUE)Practice turning following code snippets functions. Think \nfunction . call ? many arguments \nneed? Can rewrite expressive less duplicative?Write functions compute variance skewness numeric vector.\nVariance defined \n\\[\n\\mathrm{Var}(x) = \\frac{1}{n - 1} \\sum_{=1}^n (x_i - \\bar{x}) ^2 \\text{,}\n\\]\n\\(\\bar{x} = (\\sum_i^n x_i) / n\\) sample mean.\nSkewness defined \n\\[\n\\mathrm{Skew}(x) = \\frac{\\frac{1}{n-2}\\left(\\sum_{=1}^n(x_i - \\bar x)^3\\right)}{\\mathrm{Var}(x)^{3/2}} \\text{.}\n\\]Write functions compute variance skewness numeric vector.\nVariance defined \n\\[\n\\mathrm{Var}(x) = \\frac{1}{n - 1} \\sum_{=1}^n (x_i - \\bar{x}) ^2 \\text{,}\n\\]\n\\(\\bar{x} = (\\sum_i^n x_i) / n\\) sample mean.\nSkewness defined \n\\[\n\\mathrm{Skew}(x) = \\frac{\\frac{1}{n-2}\\left(\\sum_{=1}^n(x_i - \\bar x)^3\\right)}{\\mathrm{Var}(x)^{3/2}} \\text{.}\n\\]Write both_na(), function takes two vectors length\nreturns number positions NA vectors.Write both_na(), function takes two vectors length\nreturns number positions NA vectors.following functions ? useful even though \nshort?\n\nis_directory <- function(x) file.info(x)$isdir\nis_readable <- function(x) file.access(x, 4) == 0What following functions ? useful even though \nshort?Read complete lyrics\n“Little Bunny Foo Foo”. ’s lot duplication song.\nExtend initial piping example recreate complete song, use\nfunctions reduce duplication.Read complete lyrics\n“Little Bunny Foo Foo”. ’s lot duplication song.\nExtend initial piping example recreate complete song, use\nfunctions reduce duplication.","code":"\nmean(is.na(x))\n\nx / sum(x, na.rm = TRUE)\n\nsd(x, na.rm = TRUE) / mean(x, na.rm = TRUE)\nis_directory <- function(x) file.info(x)$isdir\nis_readable <- function(x) file.access(x, 4) == 0"},{"path":"functions.html","id":"functions-are-for-humans-and-computers","chapter":"19 Functions","heading":"19.3 Functions are for humans and computers","text":"’s important remember functions just computer, also humans. R doesn’t care function called, comments contains, important human readers. section discusses things bear mind writing functions humans can understand.name function important. Ideally, name function short, clearly evoke function . ’s hard! ’s better clear short, RStudio’s autocomplete makes easy type long names.Generally, function names verbs, arguments nouns. exceptions: nouns ok function computes well known noun (.e. mean() better compute_mean()), accessing property object (.e. coef() better get_coefficients()). good sign noun might better choice ’re using broad verb like “get”, “compute”, “calculate”, “determine”. Use best judgement don’t afraid rename function figure better name later.function name composed multiple words, recommend using “snake_case”, lowercase word separated underscore. camelCase popular alternative. doesn’t really matter one pick, important thing consistent: pick one stick . R consistent, ’s nothing can . Make sure don’t fall trap making code consistent possible.family functions similar things, make sure consistent names arguments. Use common prefix indicate connected. ’s better common suffix autocomplete allows type prefix see members family.good example design stringr package: don’t remember exactly function need, can type str_ jog memory.possible, avoid overriding existing functions variables. ’s impossible general many good names already taken packages, avoiding common names base R avoid confusion.Use comments, lines starting #, explain “” code. generally avoid comments explain “” “”. can’t understand code reading , think rewrite clear. need add intermediate variables useful names? need break subcomponent large function can name ? However, code can never capture reasoning behind decisions: choose approach instead alternative? else try didn’t work? ’s great idea capture sort thinking comment.Another important use comments break file easily readable chunks. Use long lines - = make easy spot breaks.RStudio provides keyboard shortcut create headers (Cmd/Ctrl + Shift + R), display code navigation drop-bottom-left editor:","code":"\n# Too short\nf()\n\n# Not a verb, or descriptive\nmy_awesome_function()\n\n# Long, but clear\nimpute_missing()\ncollapse_years()\n# Never do this!\ncol_mins <- function(x, y) {}\nrowMaxes <- function(y, x) {}\n# Good\ninput_select()\ninput_checkbox()\ninput_text()\n\n# Not so good\nselect_input()\ncheckbox_input()\ntext_input()\n# Don't do this!\nT <- FALSE\nc <- 10\nmean <- function(x) sum(x)\n# Load data --------------------------------------\n\n# Plot data --------------------------------------"},{"path":"functions.html","id":"exercises-51","chapter":"19 Functions","heading":"19.3.1 Exercises","text":"Read source code following three functions, puzzle \n, brainstorm better names.\n\nf1 <- function(string, prefix) {\n  substr(string, 1, nchar(prefix)) == prefix\n}\nf2 <- function(x) {\n  (length(x) <= 1) return(NULL)\n  x[-length(x)]\n}\nf3 <- function(x, y) {\n  rep(y, length.= length(x))\n}Read source code following three functions, puzzle \n, brainstorm better names.Take function ’ve written recently spend 5 minutes\nbrainstorming better name arguments.Take function ’ve written recently spend 5 minutes\nbrainstorming better name arguments.Compare contrast rnorm() MASS::mvrnorm(). make\nconsistent?Compare contrast rnorm() MASS::mvrnorm(). make\nconsistent?Make case norm_r(), norm_d() etc better \nrnorm(), dnorm(). Make case opposite.Make case norm_r(), norm_d() etc better \nrnorm(), dnorm(). Make case opposite.","code":"\nf1 <- function(string, prefix) {\n  substr(string, 1, nchar(prefix)) == prefix\n}\nf2 <- function(x) {\n  if (length(x) <= 1) return(NULL)\n  x[-length(x)]\n}\nf3 <- function(x, y) {\n  rep(y, length.out = length(x))\n}"},{"path":"functions.html","id":"conditional-execution","chapter":"19 Functions","heading":"19.4 Conditional execution","text":"statement allows conditionally execute code. looks like :get help need surround backticks: ?``. help isn’t particularly helpful ’re already experienced programmer, least know get !’s simple function uses statement. goal function return logical vector describing whether element vector named.function takes advantage standard return rule: function returns last value computed. either one two branches statement.","code":"\nif (condition) {\n  # code executed when condition is TRUE\n} else {\n  # code executed when condition is FALSE\n}\nhas_name <- function(x) {\n  nms <- names(x)\n  if (is.null(nms)) {\n    rep(FALSE, length(x))\n  } else {\n    !is.na(nms) & nms != \"\"\n  }\n}"},{"path":"functions.html","id":"conditions","chapter":"19 Functions","heading":"19.4.1 Conditions","text":"condition must evaluate either TRUE FALSE. ’s vector, ’ll get warning message; ’s NA, ’ll get error. Watch messages code:can use || () && () combine multiple logical expressions. operators “short-circuiting”: soon || sees first TRUE returns TRUE without computing anything else. soon && sees first FALSE returns FALSE. never use | & statement: vectorised operations apply multiple values (’s use filter()). logical vector, can use () () collapse single value.careful testing equality. == vectorised, means ’s easy get one output. Either check length already 1, collapse () (), use non-vectorised identical(). identical() strict: always returns either single TRUE single FALSE, doesn’t coerce types. means need careful comparing integers doubles:also need wary floating point numbers:Instead use dplyr::near() comparisons, described comparisons.remember, x == NA doesn’t anything useful!","code":"\nif (c(TRUE, FALSE)) {}\n#> Warning in if (c(TRUE, FALSE)) {: the condition has length > 1 and only the\n#> first element will be used\n#> NULL\n\nif (NA) {}\n#> Error in if (NA) {: missing value where TRUE/FALSE needed\nidentical(0L, 0)\n#> [1] FALSE\nx <- sqrt(2) ^ 2\nx\n#> [1] 2\nx == 2\n#> [1] FALSE\nx - 2\n#> [1] 4.440892e-16"},{"path":"functions.html","id":"multiple-conditions","chapter":"19 Functions","heading":"19.4.2 Multiple conditions","text":"can chain multiple statements together:end long series chained statements, consider rewriting. One useful technique switch() function. allows evaluate selected code based position name.Another useful function can often eliminate long chains statements cut(). ’s used discretise continuous variables.","code":"\nif (this) {\n  # do that\n} else if (that) {\n  # do something else\n} else {\n  # \n}#> function(x, y, op) {\n#>   switch(op,\n#>     plus = x + y,\n#>     minus = x - y,\n#>     times = x * y,\n#>     divide = x / y,\n#>     stop(\"Unknown op!\")\n#>   )\n#> }"},{"path":"functions.html","id":"code-style","chapter":"19 Functions","heading":"19.4.3 Code style","text":"function (almost) always followed squiggly brackets ({}), contents indented two spaces. makes easier see hierarchy code skimming left-hand margin.opening curly brace never go line always followed new line. closing curly brace always go line, unless ’s followed else. Always indent code inside curly braces.’s ok drop curly braces short statement can fit one line:recommend brief statements. Otherwise, full form easier read:","code":"# Good\nif (y < 0 && debug) {\n  message(\"Y is negative\")\n}\n\nif (y == 0) {\n  log(x)\n} else {\n  y ^ x\n}\n\n# Bad\nif (y < 0 && debug)\nmessage(\"Y is negative\")\n\nif (y == 0) {\n  log(x)\n} \nelse {\n  y ^ x\n}\ny <- 10\nx <- if (y < 20) \"Too low\" else \"Too high\"\nif (y < 20) {\n  x <- \"Too low\" \n} else {\n  x <- \"Too high\"\n}"},{"path":"functions.html","id":"exercises-52","chapter":"19 Functions","heading":"19.4.4 Exercises","text":"’s difference ifelse()? Carefully read help\nconstruct three examples illustrate key differences.’s difference ifelse()? Carefully read help\nconstruct three examples illustrate key differences.Write greeting function says “good morning”, “good afternoon”,\n“good evening”, depending time day. (Hint: use time\nargument defaults lubridate::now(). make \neasier test function.)Write greeting function says “good morning”, “good afternoon”,\n“good evening”, depending time day. (Hint: use time\nargument defaults lubridate::now(). make \neasier test function.)Implement fizzbuzz function. takes single number input. \nnumber divisible three, returns “fizz”. ’s divisible \nfive returns “buzz”. ’s divisible three five, returns\n“fizzbuzz”. Otherwise, returns number. Make sure first write\nworking code create function.Implement fizzbuzz function. takes single number input. \nnumber divisible three, returns “fizz”. ’s divisible \nfive returns “buzz”. ’s divisible three five, returns\n“fizzbuzz”. Otherwise, returns number. Make sure first write\nworking code create function.use cut() simplify set nested -else statements?\n\n(temp <= 0) {\n  \"freezing\"\n} else (temp <= 10) {\n  \"cold\"\n} else (temp <= 20) {\n  \"cool\"\n} else (temp <= 30) {\n  \"warm\"\n} else {\n  \"hot\"\n}\nchange call cut() ’d used < instead <=?\nchief advantage cut() problem? (Hint:\nhappens many values temp?)use cut() simplify set nested -else statements?change call cut() ’d used < instead <=?\nchief advantage cut() problem? (Hint:\nhappens many values temp?)happens use switch() numeric values?happens use switch() numeric values?switch() call ? happens x “e”?\n\nswitch(x, \n  = ,\n  b = \"ab\",\n  c = ,\n  d = \"cd\"\n)\nExperiment, carefully read documentation.switch() call ? happens x “e”?Experiment, carefully read documentation.","code":"\nif (temp <= 0) {\n  \"freezing\"\n} else if (temp <= 10) {\n  \"cold\"\n} else if (temp <= 20) {\n  \"cool\"\n} else if (temp <= 30) {\n  \"warm\"\n} else {\n  \"hot\"\n}\nswitch(x, \n  a = ,\n  b = \"ab\",\n  c = ,\n  d = \"cd\"\n)"},{"path":"functions.html","id":"function-arguments","chapter":"19 Functions","heading":"19.5 Function arguments","text":"arguments function typically fall two broad sets: one set supplies data compute , supplies arguments control details computation. example:log(), data x, detail base logarithm.log(), data x, detail base logarithm.mean(), data x, details much data trim\nends (trim) handle missing values (na.rm).mean(), data x, details much data trim\nends (trim) handle missing values (na.rm).t.test(), data x y, details test \nalternative, mu, paired, var.equal, conf.level.t.test(), data x y, details test \nalternative, mu, paired, var.equal, conf.level.str_c() can supply number strings ..., details\nconcatenation controlled sep collapse.str_c() can supply number strings ..., details\nconcatenation controlled sep collapse.Generally, data arguments come first. Detail arguments go end, usually default values. specify default value way call function named argument:default value almost always common value. exceptions rule safety. example, makes sense na.rm default FALSE missing values important. Even though na.rm = TRUE usually put code, ’s bad idea silently ignore missing values default.call function, typically omit names data arguments, used commonly. override default value detail argument, use full name:can refer argument unique prefix (e.g. mean(x, n = TRUE)), generally best avoided given possibilities confusion.Notice call function, place space around = function calls, always put space comma, (just like regular English). Using whitespace makes easier skim function important components.","code":"\n# Compute confidence interval around mean using normal approximation\nmean_ci <- function(x, conf = 0.95) {\n  se <- sd(x) / sqrt(length(x))\n  alpha <- 1 - conf\n  mean(x) + se * qnorm(c(alpha / 2, 1 - alpha / 2))\n}\n\nx <- runif(100)\nmean_ci(x)\n#> [1] 0.4976111 0.6099594\nmean_ci(x, conf = 0.99)\n#> [1] 0.4799599 0.6276105\n# Good\nmean(1:10, na.rm = TRUE)\n\n# Bad\nmean(x = 1:10, , FALSE)\nmean(, TRUE, x = c(1:10, NA))\n# Good\naverage <- mean(feet / 12 + inches, na.rm = TRUE)\n\n# Bad\naverage<-mean(feet/12+inches,na.rm=TRUE)"},{"path":"functions.html","id":"choosing-names","chapter":"19 Functions","heading":"19.5.1 Choosing names","text":"names arguments also important. R doesn’t care, readers code (including future-!) . Generally prefer longer, descriptive names, handful common, short names. ’s worth memorising :x, y, z: vectors.w: vector weights.df: data frame., j: numeric indices (typically rows columns).n: length, number rows.p: number columns.Otherwise, consider matching names arguments existing R functions. example, use na.rm determine missing values removed.","code":""},{"path":"functions.html","id":"checking-values","chapter":"19 Functions","heading":"19.5.2 Checking values","text":"start write functions, ’ll eventually get point don’t remember exactly function works. point ’s easy call function invalid inputs. avoid problem, ’s often useful make constraints explicit. example, imagine ’ve written functions computing weighted summary statistics:happens x w length?case, R’s vector recycling rules, don’t get error.’s good practice check important preconditions, throw error (stop()), true:careful take far. ’s tradeoff much time spend making function robust, versus long spend writing . example, also added na.rm argument, probably wouldn’t check carefully:lot extra work little additional gain. useful compromise built-stopifnot(): checks argument TRUE, produces generic error message .Note using stopifnot() assert true rather checking might wrong.","code":"\nwt_mean <- function(x, w) {\n  sum(x * w) / sum(w)\n}\nwt_var <- function(x, w) {\n  mu <- wt_mean(x, w)\n  sum(w * (x - mu) ^ 2) / sum(w)\n}\nwt_sd <- function(x, w) {\n  sqrt(wt_var(x, w))\n}\nwt_mean(1:6, 1:3)\n#> [1] 7.666667\nwt_mean <- function(x, w) {\n  if (length(x) != length(w)) {\n    stop(\"`x` and `w` must be the same length\", call. = FALSE)\n  }\n  sum(w * x) / sum(w)\n}\nwt_mean <- function(x, w, na.rm = FALSE) {\n  if (!is.logical(na.rm)) {\n    stop(\"`na.rm` must be logical\")\n  }\n  if (length(na.rm) != 1) {\n    stop(\"`na.rm` must be length 1\")\n  }\n  if (length(x) != length(w)) {\n    stop(\"`x` and `w` must be the same length\", call. = FALSE)\n  }\n  \n  if (na.rm) {\n    miss <- is.na(x) | is.na(w)\n    x <- x[!miss]\n    w <- w[!miss]\n  }\n  sum(w * x) / sum(w)\n}\nwt_mean <- function(x, w, na.rm = FALSE) {\n  stopifnot(is.logical(na.rm), length(na.rm) == 1)\n  stopifnot(length(x) == length(w))\n  \n  if (na.rm) {\n    miss <- is.na(x) | is.na(w)\n    x <- x[!miss]\n    w <- w[!miss]\n  }\n  sum(w * x) / sum(w)\n}\nwt_mean(1:6, 6:1, na.rm = \"foo\")\n#> Error in wt_mean(1:6, 6:1, na.rm = \"foo\"): is.logical(na.rm) is not TRUE"},{"path":"functions.html","id":"dot-dot-dot","chapter":"19 Functions","heading":"19.5.3 Dot-dot-dot (…)","text":"Many functions R take arbitrary number inputs:functions work? rely special argument: ... (pronounced dot-dot-dot). special argument captures number arguments aren’t otherwise matched.’s useful can send ... another function. useful catch-function primarily wraps another function. example, commonly create helper functions wrap around str_c():... lets forward arguments don’t want deal str_c(). ’s convenient technique. come price: misspelled arguments raise error. makes easy typos go unnoticed:just want capture values ..., use list(...).","code":"\nsum(1, 2, 3, 4, 5, 6, 7, 8, 9, 10)\n#> [1] 55\nstringr::str_c(\"a\", \"b\", \"c\", \"d\", \"e\", \"f\")\n#> [1] \"abcdef\"\ncommas <- function(...) stringr::str_c(..., collapse = \", \")\ncommas(letters[1:10])\n#> [1] \"a, b, c, d, e, f, g, h, i, j\"\n\nrule <- function(..., pad = \"-\") {\n  title <- paste0(...)\n  width <- getOption(\"width\") - nchar(title) - 5\n  cat(title, \" \", stringr::str_dup(pad, width), \"\\n\", sep = \"\")\n}\nrule(\"Important output\")\n#> Important output -----------------------------------------------------------\nx <- c(1, 2)\nsum(x, na.mr = TRUE)\n#> [1] 4"},{"path":"functions.html","id":"lazy-evaluation","chapter":"19 Functions","heading":"19.5.4 Lazy evaluation","text":"Arguments R lazily evaluated: ’re computed ’re needed. means ’re never used, ’re never called. important property R programming language, generally important ’re writing functions data analysis. can read lazy evaluation http://adv-r..co.nz/Functions.html#lazy-evaluation.","code":""},{"path":"functions.html","id":"exercises-53","chapter":"19 Functions","heading":"19.5.5 Exercises","text":"commas(letters, collapse = \"-\") ? ?commas(letters, collapse = \"-\") ? ?’d nice supply multiple characters pad argument,\ne.g. rule(\"Title\", pad = \"-+\"). doesn’t currently work? \nfix ?’d nice supply multiple characters pad argument,\ne.g. rule(\"Title\", pad = \"-+\"). doesn’t currently work? \nfix ?trim argument mean() ? might use ?trim argument mean() ? might use ?default value method argument cor() \nc(\"pearson\", \"kendall\", \"spearman\"). mean? \nvalue used default?default value method argument cor() \nc(\"pearson\", \"kendall\", \"spearman\"). mean? \nvalue used default?","code":""},{"path":"functions.html","id":"return-values","chapter":"19 Functions","heading":"19.6 Return values","text":"Figuring function return usually straightforward: ’s created function first place! two things consider returning value:returning early make function easier read?returning early make function easier read?Can make function pipeable?Can make function pipeable?","code":""},{"path":"functions.html","id":"explicit-return-statements","chapter":"19 Functions","heading":"19.6.1 Explicit return statements","text":"value returned function usually last statement evaluates, can choose return early using return(). think ’s best save use return() signal can return early simpler solution. common reason inputs empty:Another reason statement one complex block one simple block. example, might write statement like :first block long, time get else, ’ve forgotten condition. One way rewrite use early return simple case:tends make code easier understand, don’t need quite much context understand .","code":"\ncomplicated_function <- function(x, y, z) {\n  if (length(x) == 0 || length(y) == 0) {\n    return(0)\n  }\n    \n  # Complicated code here\n}\nf <- function() {\n  if (x) {\n    # Do \n    # something\n    # that\n    # takes\n    # many\n    # lines\n    # to\n    # express\n  } else {\n    # return something short\n  }\n}\n\nf <- function() {\n  if (!x) {\n    return(something_short)\n  }\n\n  # Do \n  # something\n  # that\n  # takes\n  # many\n  # lines\n  # to\n  # express\n}"},{"path":"functions.html","id":"writing-pipeable-functions","chapter":"19 Functions","heading":"19.6.2 Writing pipeable functions","text":"want write pipeable functions, ’s important think return value. Knowing return value’s object type mean pipeline “just work”. example, dplyr tidyr object type data frame.two basic types pipeable functions: transformations side-effects. transformations, object passed function’s first argument modified object returned. side-effects, passed object transformed. Instead, function performs action object, like drawing plot saving file. Side-effects functions “invisibly” return first argument, ’re printed can still used pipeline. example, simple function prints number missing values data frame:call interactively, invisible() means input df doesn’t get printed :’s still , ’s just printed default:can still use pipe:","code":"\nshow_missings <- function(df) {\n  n <- sum(is.na(df))\n  cat(\"Missing values: \", n, \"\\n\", sep = \"\")\n  \n  invisible(df)\n}\nshow_missings(mtcars)\n#> Missing values: 0\nx <- show_missings(mtcars) \n#> Missing values: 0\nclass(x)\n#> [1] \"data.frame\"\ndim(x)\n#> [1] 32 11\nmtcars %>% \n  show_missings() %>% \n  mutate(mpg = ifelse(mpg < 20, NA, mpg)) %>% \n  show_missings() \n#> Missing values: 0\n#> Missing values: 18"},{"path":"functions.html","id":"environment","chapter":"19 Functions","heading":"19.7 Environment","text":"last component function environment. something need understand deeply first start writing functions. However, ’s important know little bit environments crucial functions work. environment function controls R finds value associated name. example, take function:many programming languages, error, y defined inside function. R, valid code R uses rules called lexical scoping find value associated name. Since y defined inside function, R look environment function defined:behaviour seems like recipe bugs, indeed avoid creating functions like deliberately, large doesn’t cause many problems (especially regularly restart R get clean slate).advantage behaviour language standpoint allows R consistent. Every name looked using set rules. f() includes behaviour two things might expect: { +. allows devious things like:common phenomenon R. R places limits power. can many things can’t programming languages. can many things 99% time extremely ill-advised (like overriding addition works!). power flexibility makes tools like ggplot2 dplyr possible. Learning make best use flexibility beyond scope book, can read Advanced R.","code":"\nf <- function(x) {\n  x + y\n} \ny <- 100\nf(10)\n#> [1] 110\n\ny <- 1000\nf(10)\n#> [1] 1010\n`+` <- function(x, y) {\n  if (runif(1) < 0.1) {\n    sum(x, y)\n  } else {\n    sum(x, y) * 1.1\n  }\n}\ntable(replicate(1000, 1 + 2))\n#> \n#>   3 3.3 \n#> 100 900\nrm(`+`)"},{"path":"vectors.html","id":"vectors","chapter":"20 Vectors","heading":"20 Vectors","text":"","code":""},{"path":"vectors.html","id":"introduction-13","chapter":"20 Vectors","heading":"20.1 Introduction","text":"far book focussed tibbles packages work . start write functions, dig deeper R, need learn vectors, objects underlie tibbles. ’ve learned R traditional way, ’re probably already familiar vectors, R resources start vectors work way tibbles. think ’s better start tibbles ’re immediately useful, work way underlying components.Vectors particularly important functions write work vectors. possible write functions work tibbles (like ggplot2, dplyr, tidyr), tools need write functions currently idiosyncratic immature. working better approach, https://github.com/hadley/lazyeval, ready time publication book. Even complete, ’ll still need understand vectors, ’ll just make easier write user-friendly layer top.","code":""},{"path":"vectors.html","id":"prerequisites-13","chapter":"20 Vectors","heading":"20.1.1 Prerequisites","text":"focus chapter base R data structures, isn’t essential load packages. , however, use handful functions purrr package avoid inconsistencies base R.","code":"\nlibrary(tidyverse)"},{"path":"vectors.html","id":"vector-basics","chapter":"20 Vectors","heading":"20.2 Vector basics","text":"two types vectors:Atomic vectors, six types:\nlogical, integer, double, character, complex, \nraw. Integer double vectors collectively known \nnumeric vectors.Atomic vectors, six types:\nlogical, integer, double, character, complex, \nraw. Integer double vectors collectively known \nnumeric vectors.Lists, sometimes called recursive vectors lists can\ncontain lists.Lists, sometimes called recursive vectors lists can\ncontain lists.chief difference atomic vectors lists atomic vectors homogeneous, lists can heterogeneous. ’s one related object: NULL. NULL often used represent absence vector (opposed NA used represent absence value vector). NULL typically behaves like vector length 0. Figure 20.1 summarises interrelationships.\nFigure 20.1: hierarchy R’s vector types\nEvery vector two key properties:type, can determine typeof().\n\ntypeof(letters)\n#> [1] \"character\"\ntypeof(1:10)\n#> [1] \"integer\"type, can determine typeof().length, can determine length().\n\nx <- list(\"\", \"b\", 1:10)\nlength(x)\n#> [1] 3Its length, can determine length().Vectors can also contain arbitrary additional metadata form attributes. attributes used create augmented vectors build additional behaviour. three important types augmented vector:Factors built top integer vectors.Dates date-times built top numeric vectors.Data frames tibbles built top lists.chapter introduce important vectors simplest complicated. ’ll start atomic vectors, build lists, finish augmented vectors.","code":"\ntypeof(letters)\n#> [1] \"character\"\ntypeof(1:10)\n#> [1] \"integer\"\nx <- list(\"a\", \"b\", 1:10)\nlength(x)\n#> [1] 3"},{"path":"vectors.html","id":"important-types-of-atomic-vector","chapter":"20 Vectors","heading":"20.3 Important types of atomic vector","text":"four important types atomic vector logical, integer, double, character. Raw complex rarely used data analysis, won’t discuss .","code":""},{"path":"vectors.html","id":"logical","chapter":"20 Vectors","heading":"20.3.1 Logical","text":"Logical vectors simplest type atomic vector can take three possible values: FALSE, TRUE, NA. Logical vectors usually constructed comparison operators, described comparisons. can also create hand c():","code":"\n1:10 %% 3 == 0\n#>  [1] FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE FALSE  TRUE FALSE\n\nc(TRUE, TRUE, FALSE, NA)\n#> [1]  TRUE  TRUE FALSE    NA"},{"path":"vectors.html","id":"numeric","chapter":"20 Vectors","heading":"20.3.2 Numeric","text":"Integer double vectors known collectively numeric vectors. R, numbers doubles default. make integer, place L number:distinction integers doubles usually important, two important differences aware :Doubles approximations. Doubles represent floating point numbers \ncan always precisely represented fixed amount memory.\nmeans consider doubles approximations.\nexample, square square root two?\n\nx <- sqrt(2) ^ 2\nx\n#> [1] 2\nx - 2\n#> [1] 4.440892e-16\nbehaviour common working floating point numbers: \ncalculations include approximation error. Instead comparing floating\npoint numbers using ==, use dplyr::near() allows \nnumerical tolerance.Doubles approximations. Doubles represent floating point numbers \ncan always precisely represented fixed amount memory.\nmeans consider doubles approximations.\nexample, square square root two?behaviour common working floating point numbers: \ncalculations include approximation error. Instead comparing floating\npoint numbers using ==, use dplyr::near() allows \nnumerical tolerance.Integers one special value: NA, doubles four:\nNA, NaN, Inf -Inf. three special values NaN, Inf -Inf can arise division:\n\nc(-1, 0, 1) / 0\n#> [1] -Inf  NaN  Inf\nAvoid using == check special values. Instead use \nhelper functions .finite(), .infinite(), .nan():\n\n0\nInf\nNA\nNaN\n.finite()\nx\n\n\n\n.infinite()\n\nx\n\n\n.na()\n\n\nx\nx\n.nan()\n\n\n\nx\nIntegers one special value: NA, doubles four:\nNA, NaN, Inf -Inf. three special values NaN, Inf -Inf can arise division:Avoid using == check special values. Instead use \nhelper functions .finite(), .infinite(), .nan():","code":"\ntypeof(1)\n#> [1] \"double\"\ntypeof(1L)\n#> [1] \"integer\"\n1.5L\n#> [1] 1.5\nx <- sqrt(2) ^ 2\nx\n#> [1] 2\nx - 2\n#> [1] 4.440892e-16\nc(-1, 0, 1) / 0\n#> [1] -Inf  NaN  Inf"},{"path":"vectors.html","id":"character","chapter":"20 Vectors","heading":"20.3.3 Character","text":"Character vectors complex type atomic vector, element character vector string, string can contain arbitrary amount data.’ve already learned lot working strings strings. wanted mention one important feature underlying string implementation: R uses global string pool. means unique string stored memory , every use string points representation. reduces amount memory needed duplicated strings. can see behaviour practice pryr::object_size():y doesn’t take 1,000x much memory x, element y just pointer string. pointer 8 bytes, 1000 pointers 152 B string 8 * 1000 + 152 = 8.14 kB.","code":"\nx <- \"This is a reasonably long string.\"\npryr::object_size(x)\n#> Registered S3 method overwritten by 'pryr':\n#>   method      from\n#>   print.bytes Rcpp\n#> 152 B\n\ny <- rep(x, 1000)\npryr::object_size(y)\n#> 8.14 kB"},{"path":"vectors.html","id":"missing-values-4","chapter":"20 Vectors","heading":"20.3.4 Missing values","text":"Note type atomic vector missing value:Normally don’t need know different types can always use NA converted correct type using implicit coercion rules described next. However, functions strict inputs, ’s useful knowledge sitting back pocket can specific needed.","code":"\nNA            # logical\n#> [1] NA\nNA_integer_   # integer\n#> [1] NA\nNA_real_      # double\n#> [1] NA\nNA_character_ # character\n#> [1] NA"},{"path":"vectors.html","id":"exercises-54","chapter":"20 Vectors","heading":"20.3.5 Exercises","text":"Describe difference .finite(x) !.infinite(x).Describe difference .finite(x) !.infinite(x).Read source code dplyr::near() (Hint: see source code,\ndrop ()). work?Read source code dplyr::near() (Hint: see source code,\ndrop ()). work?logical vector can take 3 possible values. many possible\nvalues can integer vector take? many possible values can\ndouble take? Use google research.logical vector can take 3 possible values. many possible\nvalues can integer vector take? many possible values can\ndouble take? Use google research.Brainstorm least four functions allow convert double \ninteger. differ? precise.Brainstorm least four functions allow convert double \ninteger. differ? precise.functions readr package allow turn string\nlogical, integer, double vector?functions readr package allow turn string\nlogical, integer, double vector?","code":""},{"path":"vectors.html","id":"using-atomic-vectors","chapter":"20 Vectors","heading":"20.4 Using atomic vectors","text":"Now understand different types atomic vector, ’s useful review important tools working . include:convert one type another, happens\nautomatically.convert one type another, happens\nautomatically.tell object specific type vector.tell object specific type vector.happens work vectors different lengths.happens work vectors different lengths.name elements vector.name elements vector.pull elements interest.pull elements interest.","code":""},{"path":"vectors.html","id":"coercion","chapter":"20 Vectors","heading":"20.4.1 Coercion","text":"two ways convert, coerce, one type vector another:Explicit coercion happens call function like .logical(),\n.integer(), .double(), .character(). Whenever find\nusing explicit coercion, always check whether can\nmake fix upstream, vector never wrong type \nfirst place. example, may need tweak readr\ncol_types specification.Explicit coercion happens call function like .logical(),\n.integer(), .double(), .character(). Whenever find\nusing explicit coercion, always check whether can\nmake fix upstream, vector never wrong type \nfirst place. example, may need tweak readr\ncol_types specification.Implicit coercion happens use vector specific context\nexpects certain type vector. example, use logical\nvector numeric summary function, use double vector\ninteger vector expected.Implicit coercion happens use vector specific context\nexpects certain type vector. example, use logical\nvector numeric summary function, use double vector\ninteger vector expected.explicit coercion used relatively rarely, largely easy understand, ’ll focus implicit coercion .’ve already seen important type implicit coercion: using logical vector numeric context. case TRUE converted 1 FALSE converted 0. means sum logical vector number trues, mean logical vector proportion trues:may see code (typically older) relies implicit coercion opposite direction, integer logical:case, 0 converted FALSE everything else converted TRUE. think makes harder understand code, don’t recommend . Instead explicit: length(x) > 0.’s also important understand happens try create vector containing multiple types c(): complex type always wins.atomic vector can mix different types type property complete vector, individual elements. need mix multiple types vector, use list, ’ll learn shortly.","code":"\nx <- sample(20, 100, replace = TRUE)\ny <- x > 10\nsum(y)  # how many are greater than 10?\n#> [1] 38\nmean(y) # what proportion are greater than 10?\n#> [1] 0.38\nif (length(x)) {\n  # do something\n}\ntypeof(c(TRUE, 1L))\n#> [1] \"integer\"\ntypeof(c(1L, 1.5))\n#> [1] \"double\"\ntypeof(c(1.5, \"a\"))\n#> [1] \"character\""},{"path":"vectors.html","id":"test-functions","chapter":"20 Vectors","heading":"20.4.2 Test functions","text":"Sometimes want different things based type vector. One option use typeof(). Another use test function returns TRUE FALSE. Base R provides many functions like .vector() .atomic(), often return surprising results. Instead, ’s safer use is_* functions provided purrr, summarised table .","code":""},{"path":"vectors.html","id":"scalars-and-recycling-rules","chapter":"20 Vectors","heading":"20.4.3 Scalars and recycling rules","text":"well implicitly coercing types vectors compatible, R also implicitly coerce length vectors. called vector recycling, shorter vector repeated, recycled, length longer vector.generally useful mixing vectors “scalars”. put scalars quotes R doesn’t actually scalars: instead, single number vector length 1. scalars, built-functions vectorised, meaning operate vector numbers. ’s , example, code works:R, basic mathematical operations work vectors. means never need perform explicit iteration performing simple mathematical computations.’s intuitive happen add two vectors length, vector “scalar”, happens add two vectors different lengths?, R expand shortest vector length longest, called recycling. silent except length longer integer multiple length shorter:vector recycling can used create succinct, clever code, can also silently conceal problems. reason, vectorised functions tidyverse throw errors recycle anything scalar. want recycle, ’ll need rep():","code":"\nsample(10) + 100\n#>  [1] 107 104 103 109 102 101 106 110 105 108\nrunif(10) > 0.5\n#>  [1] FALSE  TRUE FALSE FALSE  TRUE  TRUE  TRUE  TRUE  TRUE  TRUE\n1:10 + 1:2\n#>  [1]  2  4  4  6  6  8  8 10 10 12\n1:10 + 1:3\n#> Warning in 1:10 + 1:3: longer object length is not a multiple of shorter object\n#> length\n#>  [1]  2  4  6  5  7  9  8 10 12 11\ntibble(x = 1:4, y = 1:2)\n#> Error: Tibble columns must have compatible sizes.\n#> * Size 4: Existing data.\n#> * Size 2: Column `y`.\n#> ℹ Only values of size one are recycled.\n\ntibble(x = 1:4, y = rep(1:2, 2))\n#> # A tibble: 4 x 2\n#>       x     y\n#>   <int> <int>\n#> 1     1     1\n#> 2     2     2\n#> 3     3     1\n#> 4     4     2\n\ntibble(x = 1:4, y = rep(1:2, each = 2))\n#> # A tibble: 4 x 2\n#>       x     y\n#>   <int> <int>\n#> 1     1     1\n#> 2     2     1\n#> 3     3     2\n#> 4     4     2"},{"path":"vectors.html","id":"naming-vectors","chapter":"20 Vectors","heading":"20.4.4 Naming vectors","text":"types vectors can named. can name creation c():fact purrr::set_names():Named vectors useful subsetting, described next.","code":"\nc(x = 1, y = 2, z = 4)\n#> x y z \n#> 1 2 4\nset_names(1:3, c(\"a\", \"b\", \"c\"))\n#> a b c \n#> 1 2 3"},{"path":"vectors.html","id":"vector-subsetting","chapter":"20 Vectors","heading":"20.4.5 Subsetting","text":"far ’ve used dplyr::filter() filter rows tibble. filter() works tibble, ’ll need new tool vectors: [. [ subsetting function, called like x[]. four types things can subset vector :numeric vector containing integers. integers must either \npositive, negative, zero.\nSubsetting positive integers keeps elements positions:\n\nx <- c(\"one\", \"two\", \"three\", \"four\", \"five\")\nx[c(3, 2, 5)]\n#> [1] \"three\" \"two\"   \"five\"\nrepeating position, can actually make longer output \ninput:\n\nx[c(1, 1, 5, 5, 5, 2)]\n#> [1] \"one\"  \"one\"  \"five\" \"five\" \"five\" \"two\"\nNegative values drop elements specified positions:\n\nx[c(-1, -3, -5)]\n#> [1] \"two\"  \"four\"\n’s error mix positive negative values:\n\nx[c(1, -1)]\n#> Error x[c(1, -1)]: 0's may mixed negative subscripts\nerror message mentions subsetting zero, returns values:\n\nx[0]\n#> character(0)\nuseful often, can helpful want create\nunusual data structures test functions .numeric vector containing integers. integers must either \npositive, negative, zero.Subsetting positive integers keeps elements positions:repeating position, can actually make longer output \ninput:Negative values drop elements specified positions:’s error mix positive negative values:error message mentions subsetting zero, returns values:useful often, can helpful want create\nunusual data structures test functions .Subsetting logical vector keeps values corresponding \nTRUE value. often useful conjunction \ncomparison functions.\n\nx <- c(10, 3, NA, 5, 8, 1, NA)\n\n# non-missing values x\nx[!.na(x)]\n#> [1] 10  3  5  8  1\n\n# even (missing!) values x\nx[x %% 2 == 0]\n#> [1] 10 NA  8 NASubsetting logical vector keeps values corresponding \nTRUE value. often useful conjunction \ncomparison functions.named vector, can subset character vector:\n\nx <- c(abc = 1, def = 2, xyz = 5)\nx[c(\"xyz\", \"def\")]\n#> xyz def \n#>   5   2\nLike positive integers, can also use character vector \nduplicate individual entries.named vector, can subset character vector:Like positive integers, can also use character vector \nduplicate individual entries.simplest type subsetting nothing, x[], returns \ncomplete x. useful subsetting vectors, useful\nsubsetting matrices (high dimensional structures) \nlets select rows columns, leaving \nindex blank. example, x 2d, x[1, ] selects first row \ncolumns, x[, -1] selects rows columns except\nfirst.simplest type subsetting nothing, x[], returns \ncomplete x. useful subsetting vectors, useful\nsubsetting matrices (high dimensional structures) \nlets select rows columns, leaving \nindex blank. example, x 2d, x[1, ] selects first row \ncolumns, x[, -1] selects rows columns except\nfirst.learn applications subsetting, reading “Subsetting” chapter Advanced R: http://adv-r..co.nz/Subsetting.html#applications.important variation [ called [[. [[ ever extracts single element, always drops names. ’s good idea use whenever want make clear ’re extracting single item, loop. distinction [ [[ important lists, ’ll see shortly.","code":"\nx <- c(\"one\", \"two\", \"three\", \"four\", \"five\")\nx[c(3, 2, 5)]\n#> [1] \"three\" \"two\"   \"five\"\nx[c(1, 1, 5, 5, 5, 2)]\n#> [1] \"one\"  \"one\"  \"five\" \"five\" \"five\" \"two\"\nx[c(-1, -3, -5)]\n#> [1] \"two\"  \"four\"\nx[c(1, -1)]\n#> Error in x[c(1, -1)]: only 0's may be mixed with negative subscripts\nx[0]\n#> character(0)\nx <- c(10, 3, NA, 5, 8, 1, NA)\n\n# All non-missing values of x\nx[!is.na(x)]\n#> [1] 10  3  5  8  1\n\n# All even (or missing!) values of x\nx[x %% 2 == 0]\n#> [1] 10 NA  8 NA\nx <- c(abc = 1, def = 2, xyz = 5)\nx[c(\"xyz\", \"def\")]\n#> xyz def \n#>   5   2"},{"path":"vectors.html","id":"exercises-55","chapter":"20 Vectors","heading":"20.4.6 Exercises","text":"mean(.na(x)) tell vector x? \nsum(!.finite(x))?mean(.na(x)) tell vector x? \nsum(!.finite(x))?Carefully read documentation .vector(). actually\ntest ? .atomic() agree definition \natomic vectors ?Carefully read documentation .vector(). actually\ntest ? .atomic() agree definition \natomic vectors ?Compare contrast setNames() purrr::set_names().Compare contrast setNames() purrr::set_names().Create functions take vector input returns:\nlast value. use [ [[?\nelements even numbered positions.\nEvery element except last value.\neven numbers (missing values).\nCreate functions take vector input returns:last value. use [ [[?last value. use [ [[?elements even numbered positions.elements even numbered positions.Every element except last value.Every element except last value.even numbers (missing values).even numbers (missing values).x[-(x > 0)] x[x <= 0]?x[-(x > 0)] x[x <= 0]?happens subset positive integer ’s bigger\nlength vector? happens subset \nname doesn’t exist?happens subset positive integer ’s bigger\nlength vector? happens subset \nname doesn’t exist?","code":""},{"path":"vectors.html","id":"lists","chapter":"20 Vectors","heading":"20.5 Recursive vectors (lists)","text":"Lists step complexity atomic vectors, lists can contain lists. makes suitable representing hierarchical tree-like structures. create list list():useful tool working lists str() focusses structure, contents.Unlike atomic vectors, list() can contain mix objects:Lists can even contain lists!","code":"\nx <- list(1, 2, 3)\nx\n#> [[1]]\n#> [1] 1\n#> \n#> [[2]]\n#> [1] 2\n#> \n#> [[3]]\n#> [1] 3\nstr(x)\n#> List of 3\n#>  $ : num 1\n#>  $ : num 2\n#>  $ : num 3\n\nx_named <- list(a = 1, b = 2, c = 3)\nstr(x_named)\n#> List of 3\n#>  $ a: num 1\n#>  $ b: num 2\n#>  $ c: num 3\ny <- list(\"a\", 1L, 1.5, TRUE)\nstr(y)\n#> List of 4\n#>  $ : chr \"a\"\n#>  $ : int 1\n#>  $ : num 1.5\n#>  $ : logi TRUE\nz <- list(list(1, 2), list(3, 4))\nstr(z)\n#> List of 2\n#>  $ :List of 2\n#>   ..$ : num 1\n#>   ..$ : num 2\n#>  $ :List of 2\n#>   ..$ : num 3\n#>   ..$ : num 4"},{"path":"vectors.html","id":"visualising-lists","chapter":"20 Vectors","heading":"20.5.1 Visualising lists","text":"explain complicated list manipulation functions, ’s helpful visual representation lists. example, take three lists:’ll draw follows:three principles:Lists rounded corners. Atomic vectors square corners.Lists rounded corners. Atomic vectors square corners.Children drawn inside parent, slightly darker\nbackground make easier see hierarchy.Children drawn inside parent, slightly darker\nbackground make easier see hierarchy.orientation children (.e. rows columns) isn’t important,\n’ll pick row column orientation either save space illustrate\nimportant property example.orientation children (.e. rows columns) isn’t important,\n’ll pick row column orientation either save space illustrate\nimportant property example.","code":"\nx1 <- list(c(1, 2), c(3, 4))\nx2 <- list(list(1, 2), list(3, 4))\nx3 <- list(1, list(2, list(3)))"},{"path":"vectors.html","id":"subsetting-1","chapter":"20 Vectors","heading":"20.5.2 Subsetting","text":"three ways subset list, ’ll illustrate list named :[ extracts sub-list. result always list.\n\nstr([1:2])\n#> List 2\n#>  $ : int [1:3] 1 2 3\n#>  $ b: chr \"string\"\nstr([4])\n#> List 1\n#>  $ d:List 2\n#>   ..$ : num -1\n#>   ..$ : num -5\nLike vectors, can subset logical, integer, character\nvector.[ extracts sub-list. result always list.Like vectors, can subset logical, integer, character\nvector.[[ extracts single component list. removes level \nhierarchy list.\n\nstr([[1]])\n#>  int [1:3] 1 2 3\nstr([[4]])\n#> List 2\n#>  $ : num -1\n#>  $ : num -5[[ extracts single component list. removes level \nhierarchy list.$ shorthand extracting named elements list. works\nsimilarly [[ except don’t need use quotes.\n\n$\n#> [1] 1 2 3\n[[\"\"]]\n#> [1] 1 2 3$ shorthand extracting named elements list. works\nsimilarly [[ except don’t need use quotes.distinction [ [[ really important lists, [[ drills list [ returns new, smaller list. Compare code output visual representation Figure 20.2.\nFigure 20.2: Subsetting list, visually.\n","code":"\na <- list(a = 1:3, b = \"a string\", c = pi, d = list(-1, -5))\nstr(a[1:2])\n#> List of 2\n#>  $ a: int [1:3] 1 2 3\n#>  $ b: chr \"a string\"\nstr(a[4])\n#> List of 1\n#>  $ d:List of 2\n#>   ..$ : num -1\n#>   ..$ : num -5\nstr(a[[1]])\n#>  int [1:3] 1 2 3\nstr(a[[4]])\n#> List of 2\n#>  $ : num -1\n#>  $ : num -5\na$a\n#> [1] 1 2 3\na[[\"a\"]]\n#> [1] 1 2 3"},{"path":"vectors.html","id":"lists-of-condiments","chapter":"20 Vectors","heading":"20.5.3 Lists of condiments","text":"difference [ [[ important, ’s easy get confused. help remember, let show unusual pepper shaker.pepper shaker list x, , x[1] pepper shaker containing single pepper packet:x[2] look , contain second packet. x[1:2] pepper shaker containing two pepper packets.x[[1]] :wanted get content pepper package, ’d need x[[1]][[1]]:","code":""},{"path":"vectors.html","id":"exercises-56","chapter":"20 Vectors","heading":"20.5.4 Exercises","text":"Draw following lists nested sets:\nlist(, b, list(c, d), list(e, f))\nlist(list(list(list(list(list())))))\nDraw following lists nested sets:list(, b, list(c, d), list(e, f))list(list(list(list(list(list())))))happens subset tibble ’re subsetting list?\nkey differences list tibble?happens subset tibble ’re subsetting list?\nkey differences list tibble?","code":""},{"path":"vectors.html","id":"attributes","chapter":"20 Vectors","heading":"20.6 Attributes","text":"vector can contain arbitrary additional metadata attributes. can think attributes named list vectors can attached object.\ncan get set individual attribute values attr() see attributes().three important attributes used implement fundamental parts R:Names used name elements vector.Dimensions (dims, short) make vector behave like matrix array.Class used implement S3 object oriented system.’ve seen names , won’t cover dimensions don’t use matrices book. remains describe class, controls generic functions work. Generic functions key object oriented programming R, make functions behave differently different classes input. detailed discussion object oriented programming beyond scope book, can read Advanced R http://adv-r..co.nz/OO-essentials.html#s3.’s typical generic function looks like:call “UseMethod” means generic function, call specific method, function, based class first argument. (methods functions; functions methods). can list methods generic methods():example, x character vector, .Date() call .Date.character(); ’s factor, ’ll call .Date.factor().can see specific implementation method getS3method():important S3 generic print(): controls object printed type name console. important generics subsetting functions [, [[, $.","code":"\nx <- 1:10\nattr(x, \"greeting\")\n#> NULL\nattr(x, \"greeting\") <- \"Hi!\"\nattr(x, \"farewell\") <- \"Bye!\"\nattributes(x)\n#> $greeting\n#> [1] \"Hi!\"\n#> \n#> $farewell\n#> [1] \"Bye!\"\nas.Date\n#> function (x, ...) \n#> UseMethod(\"as.Date\")\n#> <bytecode: 0x55fedea4d4b8>\n#> <environment: namespace:base>\nmethods(\"as.Date\")\n#> [1] as.Date.character   as.Date.default     as.Date.factor     \n#> [4] as.Date.numeric     as.Date.POSIXct     as.Date.POSIXlt    \n#> [7] as.Date.vctrs_sclr* as.Date.vctrs_vctr*\n#> see '?methods' for accessing help and source code\ngetS3method(\"as.Date\", \"default\")\n#> function (x, ...) \n#> {\n#>     if (inherits(x, \"Date\")) \n#>         x\n#>     else if (is.null(x)) \n#>         .Date(numeric())\n#>     else if (is.logical(x) && all(is.na(x))) \n#>         .Date(as.numeric(x))\n#>     else stop(gettextf(\"do not know how to convert '%s' to class %s\", \n#>         deparse1(substitute(x)), dQuote(\"Date\")), domain = NA)\n#> }\n#> <bytecode: 0x55fee0522590>\n#> <environment: namespace:base>\ngetS3method(\"as.Date\", \"numeric\")\n#> function (x, origin, ...) \n#> {\n#>     if (missing(origin)) {\n#>         if (!length(x)) \n#>             return(.Date(numeric()))\n#>         if (!any(is.finite(x))) \n#>             return(.Date(x))\n#>         stop(\"'origin' must be supplied\")\n#>     }\n#>     as.Date(origin, ...) + x\n#> }\n#> <bytecode: 0x55fede1efd60>\n#> <environment: namespace:base>"},{"path":"vectors.html","id":"augmented-vectors","chapter":"20 Vectors","heading":"20.7 Augmented vectors","text":"Atomic vectors lists building blocks important vector types like factors dates. call augmented vectors, vectors additional attributes, including class. augmented vectors class, behave differently atomic vector built. book, make use four important augmented vectors:FactorsDatesDate-timesTibblesThese described .","code":""},{"path":"vectors.html","id":"factors-1","chapter":"20 Vectors","heading":"20.7.1 Factors","text":"Factors designed represent categorical data can take fixed set possible values. Factors built top integers, levels attribute:","code":"\nx <- factor(c(\"ab\", \"cd\", \"ab\"), levels = c(\"ab\", \"cd\", \"ef\"))\ntypeof(x)\n#> [1] \"integer\"\nattributes(x)\n#> $levels\n#> [1] \"ab\" \"cd\" \"ef\"\n#> \n#> $class\n#> [1] \"factor\""},{"path":"vectors.html","id":"dates-and-date-times","chapter":"20 Vectors","heading":"20.7.2 Dates and date-times","text":"Dates R numeric vectors represent number days since 1 January 1970.Date-times numeric vectors class POSIXct represent number seconds since 1 January 1970. (case wondering, “POSIXct” stands “Portable Operating System Interface”, calendar time.)tzone attribute optional. controls time printed, absolute time refers .another type date-times called POSIXlt. built top named lists:POSIXlts rare inside tidyverse. crop base R, needed extract specific components date, like year month. Since lubridate provides helpers instead, don’t need . POSIXct’s always easier work , find POSIXlt, always convert regular date time lubridate::as_date_time().","code":"\nx <- as.Date(\"1971-01-01\")\nunclass(x)\n#> [1] 365\n\ntypeof(x)\n#> [1] \"double\"\nattributes(x)\n#> $class\n#> [1] \"Date\"\nx <- lubridate::ymd_hm(\"1970-01-01 01:00\")\nunclass(x)\n#> [1] 3600\n#> attr(,\"tzone\")\n#> [1] \"UTC\"\n\ntypeof(x)\n#> [1] \"double\"\nattributes(x)\n#> $class\n#> [1] \"POSIXct\" \"POSIXt\" \n#> \n#> $tzone\n#> [1] \"UTC\"\nattr(x, \"tzone\") <- \"US/Pacific\"\nx\n#> [1] \"1969-12-31 17:00:00 PST\"\n\nattr(x, \"tzone\") <- \"US/Eastern\"\nx\n#> [1] \"1969-12-31 20:00:00 EST\"\ny <- as.POSIXlt(x)\ntypeof(y)\n#> [1] \"list\"\nattributes(y)\n#> $names\n#>  [1] \"sec\"    \"min\"    \"hour\"   \"mday\"   \"mon\"    \"year\"   \"wday\"   \"yday\"  \n#>  [9] \"isdst\"  \"zone\"   \"gmtoff\"\n#> \n#> $class\n#> [1] \"POSIXlt\" \"POSIXt\" \n#> \n#> $tzone\n#> [1] \"US/Eastern\" \"EST\"        \"EDT\""},{"path":"vectors.html","id":"tibbles-1","chapter":"20 Vectors","heading":"20.7.3 Tibbles","text":"Tibbles augmented lists: class “tbl_df” + “tbl” + “data.frame”, names (column) row.names attributes:difference tibble list elements data frame must vectors length. functions work tibbles enforce constraint.Traditional data.frames similar structure:main difference class. class tibble includes “data.frame” means tibbles inherit regular data frame behaviour default.","code":"\ntb <- tibble::tibble(x = 1:5, y = 5:1)\ntypeof(tb)\n#> [1] \"list\"\nattributes(tb)\n#> $names\n#> [1] \"x\" \"y\"\n#> \n#> $row.names\n#> [1] 1 2 3 4 5\n#> \n#> $class\n#> [1] \"tbl_df\"     \"tbl\"        \"data.frame\"\ndf <- data.frame(x = 1:5, y = 5:1)\ntypeof(df)\n#> [1] \"list\"\nattributes(df)\n#> $names\n#> [1] \"x\" \"y\"\n#> \n#> $class\n#> [1] \"data.frame\"\n#> \n#> $row.names\n#> [1] 1 2 3 4 5"},{"path":"vectors.html","id":"exercises-57","chapter":"20 Vectors","heading":"20.7.4 Exercises","text":"hms::hms(3600) return? print? primitive\ntype augmented vector built top ? attributes \nuse?hms::hms(3600) return? print? primitive\ntype augmented vector built top ? attributes \nuse?Try make tibble columns different lengths. \nhappens?Try make tibble columns different lengths. \nhappens?Based definition , ok list \ncolumn tibble?Based definition , ok list \ncolumn tibble?","code":""},{"path":"iteration.html","id":"iteration","chapter":"21 Iteration","heading":"21 Iteration","text":"","code":""},{"path":"iteration.html","id":"introduction-14","chapter":"21 Iteration","heading":"21.1 Introduction","text":"functions, talked important reduce duplication code creating functions instead copying--pasting. Reducing code duplication three main benefits:’s easier see intent code, eyes \ndrawn ’s different, stays .’s easier see intent code, eyes \ndrawn ’s different, stays .’s easier respond changes requirements. needs\nchange, need make changes one place, rather \nremembering change every place copied--pasted \ncode.’s easier respond changes requirements. needs\nchange, need make changes one place, rather \nremembering change every place copied--pasted \ncode.’re likely fewer bugs line code \nused places.’re likely fewer bugs line code \nused places.One tool reducing duplication functions, reduce duplication identifying repeated patterns code extract independent pieces can easily reused updated. Another tool reducing duplication iteration, helps need thing multiple inputs: repeating operation different columns, different datasets.\nchapter ’ll learn two important iteration paradigms: imperative programming functional programming. imperative side tools like loops loops, great place start make iteration explicit, ’s obvious ’s happening. However, loops quite verbose, require quite bit bookkeeping code duplicated every loop. Functional programming (FP) offers tools extract duplicated code, common loop pattern gets function. master vocabulary FP, can solve many common iteration problems less code, ease, fewer errors.","code":""},{"path":"iteration.html","id":"prerequisites-14","chapter":"21 Iteration","heading":"21.1.1 Prerequisites","text":"’ve mastered loops provided base R, ’ll learn powerful programming tools provided purrr, one tidyverse core packages.","code":"\nlibrary(tidyverse)"},{"path":"iteration.html","id":"for-loops","chapter":"21 Iteration","heading":"21.2 For loops","text":"Imagine simple tibble:want compute median column. copy--paste:breaks rule thumb: never copy paste twice. Instead, use loop:Every loop three components:output: output <- vector(\"double\", length(x)).\nstart loop, must always allocate sufficient space\noutput. important efficiency: grow\nloop iteration using c() (example), loop\nslow.\ngeneral way creating empty vector given length vector()\nfunction. two arguments: type vector (“logical”,\n“integer”, “double”, “character”, etc) length vector.output: output <- vector(\"double\", length(x)).\nstart loop, must always allocate sufficient space\noutput. important efficiency: grow\nloop iteration using c() (example), loop\nslow.general way creating empty vector given length vector()\nfunction. two arguments: type vector (“logical”,\n“integer”, “double”, “character”, etc) length vector.sequence: seq_along(df). determines loop :\nrun loop assign different value \nseq_along(df). ’s useful think pronoun, like “”.\nmight seen seq_along() . ’s safe version \nfamiliar 1:length(l), important difference: \nzero-length vector, seq_along() right thing:\n\ny <- vector(\"double\", 0)\nseq_along(y)\n#> integer(0)\n1:length(y)\n#> [1] 1 0\nprobably won’t create zero-length vector deliberately, \n’s easy create accidentally. use 1:length(x) instead\nseq_along(x), ’re likely get confusing error message.sequence: seq_along(df). determines loop :\nrun loop assign different value \nseq_along(df). ’s useful think pronoun, like “”.might seen seq_along() . ’s safe version \nfamiliar 1:length(l), important difference: \nzero-length vector, seq_along() right thing:probably won’t create zero-length vector deliberately, \n’s easy create accidentally. use 1:length(x) instead\nseq_along(x), ’re likely get confusing error message.body: output[[]] <- median(df[[]]). code \nwork. ’s run repeatedly, time different value .\nfirst iteration run output[[1]] <- median(df[[1]]),\nsecond run output[[2]] <- median(df[[2]]), .body: output[[]] <- median(df[[]]). code \nwork. ’s run repeatedly, time different value .\nfirst iteration run output[[1]] <- median(df[[1]]),\nsecond run output[[2]] <- median(df[[2]]), .’s loop! Now good time practice creating basic (basic) loops using exercises . ’ll move variations loop help solve problems crop practice.","code":"\ndf <- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\nmedian(df$a)\n#> [1] -0.2457625\nmedian(df$b)\n#> [1] -0.2873072\nmedian(df$c)\n#> [1] -0.05669771\nmedian(df$d)\n#> [1] 0.1442633\noutput <- vector(\"double\", ncol(df))  # 1. output\nfor (i in seq_along(df)) {            # 2. sequence\n  output[[i]] <- median(df[[i]])      # 3. body\n}\noutput\n#> [1] -0.24576245 -0.28730721 -0.05669771  0.14426335\ny <- vector(\"double\", 0)\nseq_along(y)\n#> integer(0)\n1:length(y)\n#> [1] 1 0"},{"path":"iteration.html","id":"exercises-58","chapter":"21 Iteration","heading":"21.2.1 Exercises","text":"Write loops :\nCompute mean every column mtcars.\nDetermine type column nycflights13::flights.\nCompute number unique values column iris.\nGenerate 10 random normals distributions means -10, 0, 10, 100.\nThink output, sequence, body start writing\nloop.Write loops :Compute mean every column mtcars.Determine type column nycflights13::flights.Compute number unique values column iris.Generate 10 random normals distributions means -10, 0, 10, 100.Think output, sequence, body start writing\nloop.Eliminate loop following examples taking\nadvantage existing function works vectors:\n\n<- \"\"\n(x letters) {\n  <- stringr::str_c(, x)\n}\n\nx <- sample(100)\nsd <- 0\n(seq_along(x)) {\n  sd <- sd + (x[] - mean(x)) ^ 2\n}\nsd <- sqrt(sd / (length(x) - 1))\n\nx <- runif(100)\n<- vector(\"numeric\", length(x))\n[1] <- x[1]\n(2:length(x)) {\n  [] <- [- 1] + x[]\n}Eliminate loop following examples taking\nadvantage existing function works vectors:Combine function writing loop skills:\nWrite loop prints() lyrics children’s song\n“Alice camel”.\nConvert nursery rhyme “ten bed” function. Generalise\nnumber people sleeping structure.\nConvert song “99 bottles beer wall” function.\nGeneralise number vessel containing liquid \nsurface.\nCombine function writing loop skills:Write loop prints() lyrics children’s song\n“Alice camel”.Write loop prints() lyrics children’s song\n“Alice camel”.Convert nursery rhyme “ten bed” function. Generalise\nnumber people sleeping structure.Convert nursery rhyme “ten bed” function. Generalise\nnumber people sleeping structure.Convert song “99 bottles beer wall” function.\nGeneralise number vessel containing liquid \nsurface.Convert song “99 bottles beer wall” function.\nGeneralise number vessel containing liquid \nsurface.’s common see loops don’t preallocate output instead\nincrease length vector step:\n\noutput <- vector(\"integer\", 0)\n(seq_along(x)) {\n  output <- c(output, lengths(x[[]]))\n}\noutput\naffect performance? Design execute experiment.’s common see loops don’t preallocate output instead\nincrease length vector step:affect performance? Design execute experiment.","code":"\nout <- \"\"\nfor (x in letters) {\n  out <- stringr::str_c(out, x)\n}\n\nx <- sample(100)\nsd <- 0\nfor (i in seq_along(x)) {\n  sd <- sd + (x[i] - mean(x)) ^ 2\n}\nsd <- sqrt(sd / (length(x) - 1))\n\nx <- runif(100)\nout <- vector(\"numeric\", length(x))\nout[1] <- x[1]\nfor (i in 2:length(x)) {\n  out[i] <- out[i - 1] + x[i]\n}\noutput <- vector(\"integer\", 0)\nfor (i in seq_along(x)) {\n  output <- c(output, lengths(x[[i]]))\n}\noutput"},{"path":"iteration.html","id":"for-loop-variations","chapter":"21 Iteration","heading":"21.3 For loop variations","text":"basic loop belt, variations aware . variations important regardless iteration, don’t forget ’ve mastered FP techniques ’ll learn next section.four variations basic theme loop:Modifying existing object, instead creating new object.Looping names values, instead indices.Handling outputs unknown length.Handling sequences unknown length.","code":""},{"path":"iteration.html","id":"modifying-an-existing-object","chapter":"21 Iteration","heading":"21.3.1 Modifying an existing object","text":"Sometimes want use loop modify existing object. example, remember challenge functions. wanted rescale every column data frame:solve loop think three components:Output: already output — ’s input!Output: already output — ’s input!Sequence: can think data frame list columns, \ncan iterate column seq_along(df).Sequence: can think data frame list columns, \ncan iterate column seq_along(df).Body: apply rescale01().Body: apply rescale01().gives us:Typically ’ll modifying list data frame sort loop, remember use [[, [. might spotted used [[ loops: think ’s better use [[ even atomic vectors makes clear want work single element.","code":"\ndf <- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\nrescale01 <- function(x) {\n  rng <- range(x, na.rm = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\n\ndf$a <- rescale01(df$a)\ndf$b <- rescale01(df$b)\ndf$c <- rescale01(df$c)\ndf$d <- rescale01(df$d)\nfor (i in seq_along(df)) {\n  df[[i]] <- rescale01(df[[i]])\n}"},{"path":"iteration.html","id":"looping-patterns","chapter":"21 Iteration","heading":"21.3.2 Looping patterns","text":"three basic ways loop vector. far ’ve shown general: looping numeric indices (seq_along(xs)), extracting value x[[]]. two forms:Loop elements: (x xs). useful \ncare side-effects, like plotting saving file, ’s\ndifficult save output efficiently.Loop elements: (x xs). useful \ncare side-effects, like plotting saving file, ’s\ndifficult save output efficiently.Loop names: (nm names(xs)). gives name, \ncan use access value x[[nm]]. useful want\nuse name plot title file name. ’re creating\nnamed output, make sure name results vector like :\n\nresults <- vector(\"list\", length(x))\nnames(results) <- names(x)Loop names: (nm names(xs)). gives name, \ncan use access value x[[nm]]. useful want\nuse name plot title file name. ’re creating\nnamed output, make sure name results vector like :Iteration numeric indices general form, given position can extract name value:","code":"\nresults <- vector(\"list\", length(x))\nnames(results) <- names(x)\nfor (i in seq_along(x)) {\n  name <- names(x)[[i]]\n  value <- x[[i]]\n}"},{"path":"iteration.html","id":"unknown-output-length","chapter":"21 Iteration","heading":"21.3.3 Unknown output length","text":"Sometimes might know long output . example, imagine want simulate random vectors random lengths. might tempted solve problem progressively growing vector:efficient iteration, R copy data previous iterations. technical terms get “quadratic” (\\(O(n^2)\\)) behaviour means loop three times many elements take nine (\\(3^2\\)) times long run.better solution save results list, combine single vector loop done:’ve used unlist() flatten list vectors single vector. stricter option use purrr::flatten_dbl() — throw error input isn’t list doubles.pattern occurs places :might generating long string. Instead paste()ing together\niteration previous, save output character vector \ncombine vector single string \npaste(output, collapse = \"\").might generating long string. Instead paste()ing together\niteration previous, save output character vector \ncombine vector single string \npaste(output, collapse = \"\").might generating big data frame. Instead sequentially\nrbind()ing iteration, save output list, use\ndplyr::bind_rows(output) combine output single\ndata frame.might generating big data frame. Instead sequentially\nrbind()ing iteration, save output list, use\ndplyr::bind_rows(output) combine output single\ndata frame.Watch pattern. Whenever see , switch complex result object, combine one step end.","code":"\nmeans <- c(0, 1, 2)\n\noutput <- double()\nfor (i in seq_along(means)) {\n  n <- sample(100, 1)\n  output <- c(output, rnorm(n, means[[i]]))\n}\nstr(output)\n#>  num [1:138] 0.912 0.205 2.584 -0.789 0.588 ...\nout <- vector(\"list\", length(means))\nfor (i in seq_along(means)) {\n  n <- sample(100, 1)\n  out[[i]] <- rnorm(n, means[[i]])\n}\nstr(out)\n#> List of 3\n#>  $ : num [1:76] -0.3389 -0.0756 0.0402 0.1243 -0.9984 ...\n#>  $ : num [1:17] -0.11 1.149 0.614 0.77 1.392 ...\n#>  $ : num [1:41] 1.88 2.46 2.62 1.82 1.88 ...\nstr(unlist(out))\n#>  num [1:134] -0.3389 -0.0756 0.0402 0.1243 -0.9984 ..."},{"path":"iteration.html","id":"unknown-sequence-length","chapter":"21 Iteration","heading":"21.3.4 Unknown sequence length","text":"Sometimes don’t even know long input sequence run . common simulations. example, might want loop get three heads row. can’t sort iteration loop. Instead, can use loop. loop simpler loop two components, condition body:loop also general loop, can rewrite loop loop, can’t rewrite every loop loop:’s use loop find many tries takes get three heads row:mention loops briefly, hardly ever use . ’re often used simulation, outside scope book. However, good know exist ’re prepared problems number iterations known advance.","code":"\nwhile (condition) {\n  # body\n}\nfor (i in seq_along(x)) {\n  # body\n}\n\n# Equivalent to\ni <- 1\nwhile (i <= length(x)) {\n  # body\n  i <- i + 1 \n}\nflip <- function() sample(c(\"T\", \"H\"), 1)\n\nflips <- 0\nnheads <- 0\n\nwhile (nheads < 3) {\n  if (flip() == \"H\") {\n    nheads <- nheads + 1\n  } else {\n    nheads <- 0\n  }\n  flips <- flips + 1\n}\nflips\n#> [1] 21"},{"path":"iteration.html","id":"exercises-59","chapter":"21 Iteration","heading":"21.3.5 Exercises","text":"Imagine directory full CSV files want read .\npaths vector,\nfiles <- dir(\"data/\", pattern = \"\\\\.csv$\", full.names = TRUE), now\nwant read one read_csv(). Write loop \nload single data frame.Imagine directory full CSV files want read .\npaths vector,\nfiles <- dir(\"data/\", pattern = \"\\\\.csv$\", full.names = TRUE), now\nwant read one read_csv(). Write loop \nload single data frame.happens use (nm names(x)) x names?\nelements named? names \nunique?happens use (nm names(x)) x names?\nelements named? names \nunique?Write function prints mean numeric column data\nframe, along name. example, show_mean(iris) print:\n\nshow_mean(iris)\n#> Sepal.Length: 5.84\n#> Sepal.Width:  3.06\n#> Petal.Length: 3.76\n#> Petal.Width:  1.20\n(Extra challenge: function use make sure numbers\nlined nicely, even though variable names different lengths?)Write function prints mean numeric column data\nframe, along name. example, show_mean(iris) print:(Extra challenge: function use make sure numbers\nlined nicely, even though variable names different lengths?)code ? work?\n\ntrans <- list( \n  disp = function(x) x * 0.0163871,\n  = function(x) {\n    factor(x, labels = c(\"auto\", \"manual\"))\n  }\n)\n(var names(trans)) {\n  mtcars[[var]] <- trans[[var]](mtcars[[var]])\n}code ? work?","code":"\nshow_mean(iris)\n#> Sepal.Length: 5.84\n#> Sepal.Width:  3.06\n#> Petal.Length: 3.76\n#> Petal.Width:  1.20\ntrans <- list( \n  disp = function(x) x * 0.0163871,\n  am = function(x) {\n    factor(x, labels = c(\"auto\", \"manual\"))\n  }\n)\nfor (var in names(trans)) {\n  mtcars[[var]] <- trans[[var]](mtcars[[var]])\n}"},{"path":"iteration.html","id":"for-loops-vs.-functionals","chapter":"21 Iteration","heading":"21.4 For loops vs. functionals","text":"loops important R languages R functional programming language. means ’s possible wrap loops function, call function instead using loop directly.see important, consider () simple data frame:Imagine want compute mean every column. loop:realise ’re going want compute means every column pretty frequently, extract function:think ’d also helpful able compute median, standard deviation, copy paste col_mean() function replace mean() median() sd():Uh oh! ’ve copied--pasted code twice, ’s time think generalise . Notice code -loop boilerplate ’s hard see one thing (mean(), median(), sd()) different functions.saw set functions like :Hopefully, ’d notice ’s lot duplication, extract additional argument:’ve reduced chance bugs (now 1/3 original code), made easy generalise new situations.can exactly thing col_mean(), col_median() col_sd() adding argument supplies function apply column:idea passing function another function extremely powerful idea, ’s one behaviours makes R functional programming language. might take wrap head around idea, ’s worth investment. rest chapter, ’ll learn use purrr package, provides functions eliminate need many common loops. apply family functions base R (apply(), lapply(), tapply(), etc) solve similar problem, purrr consistent thus easier learn.goal using purrr functions instead loops allow break common list manipulation challenges independent pieces:can solve problem single element list? \n’ve solved problem, purrr takes care generalising \nsolution every element list.can solve problem single element list? \n’ve solved problem, purrr takes care generalising \nsolution every element list.’re solving complex problem, can break \nbite-sized pieces allow advance one small step towards \nsolution? purrr, get lots small pieces can\ncompose together pipe.’re solving complex problem, can break \nbite-sized pieces allow advance one small step towards \nsolution? purrr, get lots small pieces can\ncompose together pipe.structure makes easier solve new problems. also makes easier understand solutions old problems re-read old code.","code":"\ndf <- tibble(\n  a = rnorm(10),\n  b = rnorm(10),\n  c = rnorm(10),\n  d = rnorm(10)\n)\noutput <- vector(\"double\", length(df))\nfor (i in seq_along(df)) {\n  output[[i]] <- mean(df[[i]])\n}\noutput\n#> [1] -0.3260369  0.1356639  0.4291403 -0.2498034\ncol_mean <- function(df) {\n  output <- vector(\"double\", length(df))\n  for (i in seq_along(df)) {\n    output[i] <- mean(df[[i]])\n  }\n  output\n}\ncol_median <- function(df) {\n  output <- vector(\"double\", length(df))\n  for (i in seq_along(df)) {\n    output[i] <- median(df[[i]])\n  }\n  output\n}\ncol_sd <- function(df) {\n  output <- vector(\"double\", length(df))\n  for (i in seq_along(df)) {\n    output[i] <- sd(df[[i]])\n  }\n  output\n}\nf1 <- function(x) abs(x - mean(x)) ^ 1\nf2 <- function(x) abs(x - mean(x)) ^ 2\nf3 <- function(x) abs(x - mean(x)) ^ 3\nf <- function(x, i) abs(x - mean(x)) ^ i\ncol_summary <- function(df, fun) {\n  out <- vector(\"double\", length(df))\n  for (i in seq_along(df)) {\n    out[i] <- fun(df[[i]])\n  }\n  out\n}\ncol_summary(df, median)\n#> [1] -0.51850298  0.02779864  0.17295591 -0.61163819\ncol_summary(df, mean)\n#> [1] -0.3260369  0.1356639  0.4291403 -0.2498034"},{"path":"iteration.html","id":"exercises-60","chapter":"21 Iteration","heading":"21.4.1 Exercises","text":"Read documentation apply(). 2d case, two loops\ngeneralise?Read documentation apply(). 2d case, two loops\ngeneralise?Adapt col_summary() applies numeric columns\nmight want start is_numeric() function returns\nlogical vector TRUE corresponding numeric column.Adapt col_summary() applies numeric columns\nmight want start is_numeric() function returns\nlogical vector TRUE corresponding numeric column.","code":""},{"path":"iteration.html","id":"the-map-functions","chapter":"21 Iteration","heading":"21.5 The map functions","text":"pattern looping vector, something element saving results common purrr package provides family functions . one function type output:map() makes list.map_lgl() makes logical vector.map_int() makes integer vector.map_dbl() makes double vector.map_chr() makes character vector.function takes vector input, applies function piece, returns new vector ’s length (names) input. type vector determined suffix map function.master functions, ’ll find takes much less time solve iteration problems. never feel bad using loop instead map function. map functions step tower abstraction, can take long time get head around work. important thing solve problem ’re working , write concise elegant code (although ’s definitely something want strive towards!).people tell avoid loops slow. ’re wrong! (Well least ’re rather date, loops haven’t slow many years.) chief benefits using functions like map() speed, clarity: make code easier write read.can use functions perform computations last loop. summary functions returned doubles, need use map_dbl():Compared using loop, focus operation performed (.e. mean(), median(), sd()), bookkeeping required loop every element store output. even apparent use pipe:differences map_*() col_summary():purrr functions implemented C. makes little faster\nexpense readability.purrr functions implemented C. makes little faster\nexpense readability.second argument, .f, function apply, can formula, \ncharacter vector, integer vector. ’ll learn handy\nshortcuts next section.second argument, .f, function apply, can formula, \ncharacter vector, integer vector. ’ll learn handy\nshortcuts next section.map_*() uses … ([dot dot dot]) pass along additional arguments\n.f time ’s called:\n\nmap_dbl(df, mean, trim = 0.5)\n#>                     b           c           d \n#> -0.51850298  0.02779864  0.17295591 -0.61163819map_*() uses … ([dot dot dot]) pass along additional arguments\n.f time ’s called:map functions also preserve names:\n\nz <- list(x = 1:3, y = 4:5)\nmap_int(z, length)\n#> x y \n#> 3 2The map functions also preserve names:","code":"\nmap_dbl(df, mean)\n#>          a          b          c          d \n#> -0.3260369  0.1356639  0.4291403 -0.2498034\nmap_dbl(df, median)\n#>           a           b           c           d \n#> -0.51850298  0.02779864  0.17295591 -0.61163819\nmap_dbl(df, sd)\n#>         a         b         c         d \n#> 0.9214834 0.4848945 0.9816016 1.1563324\ndf %>% map_dbl(mean)\n#>          a          b          c          d \n#> -0.3260369  0.1356639  0.4291403 -0.2498034\ndf %>% map_dbl(median)\n#>           a           b           c           d \n#> -0.51850298  0.02779864  0.17295591 -0.61163819\ndf %>% map_dbl(sd)\n#>         a         b         c         d \n#> 0.9214834 0.4848945 0.9816016 1.1563324\nmap_dbl(df, mean, trim = 0.5)\n#>           a           b           c           d \n#> -0.51850298  0.02779864  0.17295591 -0.61163819\nz <- list(x = 1:3, y = 4:5)\nmap_int(z, length)\n#> x y \n#> 3 2"},{"path":"iteration.html","id":"shortcuts","chapter":"21 Iteration","heading":"21.5.1 Shortcuts","text":"shortcuts can use .f order save little typing. Imagine want fit linear model group dataset. following toy example splits mtcars dataset three pieces (one value cylinder) fits linear model piece:syntax creating anonymous function R quite verbose purrr provides convenient shortcut: one-sided formula.’ve used .x pronoun: refers current list element (way referred current index loop). .x one-sided formula corresponds argument anonymous function.’re looking many models, might want extract summary statistic like \\(R^2\\). need first run summary() extract component called r.squared. using shorthand anonymous functions:extracting named components common operation, purrr provides even shorter shortcut: can use string.can also use integer select elements position:","code":"\nmodels <- mtcars %>% \n  split(.$cyl) %>% \n  map(function(df) lm(mpg ~ wt, data = df))\nmodels <- mtcars %>% \n  split(.$cyl) %>% \n  map(~lm(mpg ~ wt, data = .x))\nmodels %>% \n  map(summary) %>% \n  map_dbl(~.x$r.squared)\n#>         4         6         8 \n#> 0.5086326 0.4645102 0.4229655\nmodels %>% \n  map(summary) %>% \n  map_dbl(\"r.squared\")\n#>         4         6         8 \n#> 0.5086326 0.4645102 0.4229655\nx <- list(list(1, 2, 3), list(4, 5, 6), list(7, 8, 9))\nx %>% map_dbl(2)\n#> [1] 2 5 8"},{"path":"iteration.html","id":"base-r","chapter":"21 Iteration","heading":"21.5.2 Base R","text":"’re familiar apply family functions base R, might noticed similarities purrr functions:lapply() basically identical map(), except map() \nconsistent functions purrr, can use \nshortcuts .f.lapply() basically identical map(), except map() \nconsistent functions purrr, can use \nshortcuts .f.Base sapply() wrapper around lapply() automatically\nsimplifies output. useful interactive work \nproblematic function never know sort output\n’ll get:\n\nx1 <- list(\n  c(0.27, 0.37, 0.57, 0.91, 0.20),\n  c(0.90, 0.94, 0.66, 0.63, 0.06), \n  c(0.21, 0.18, 0.69, 0.38, 0.77)\n)\nx2 <- list(\n  c(0.50, 0.72, 0.99, 0.38, 0.78), \n  c(0.93, 0.21, 0.65, 0.13, 0.27), \n  c(0.39, 0.01, 0.38, 0.87, 0.34)\n)\n\nthreshold <- function(x, cutoff = 0.8) x[x > cutoff]\nx1 %>% sapply(threshold) %>% str()\n#> List 3\n#>  $ : num 0.91\n#>  $ : num [1:2] 0.9 0.94\n#>  $ : num(0)\nx2 %>% sapply(threshold) %>% str()\n#>  num [1:3] 0.99 0.93 0.87Base sapply() wrapper around lapply() automatically\nsimplifies output. useful interactive work \nproblematic function never know sort output\n’ll get:vapply() safe alternative sapply() supply \nadditional argument defines type. problem \nvapply() ’s lot typing:\nvapply(df, .numeric, logical(1)) equivalent \nmap_lgl(df, .numeric). One advantage vapply() purrr’s map\nfunctions can also produce matrices — map functions \never produce vectors.vapply() safe alternative sapply() supply \nadditional argument defines type. problem \nvapply() ’s lot typing:\nvapply(df, .numeric, logical(1)) equivalent \nmap_lgl(df, .numeric). One advantage vapply() purrr’s map\nfunctions can also produce matrices — map functions \never produce vectors.focus purrr functions consistent names arguments, helpful shortcuts, future provide easy parallelism progress bars.","code":"\nx1 <- list(\n  c(0.27, 0.37, 0.57, 0.91, 0.20),\n  c(0.90, 0.94, 0.66, 0.63, 0.06), \n  c(0.21, 0.18, 0.69, 0.38, 0.77)\n)\nx2 <- list(\n  c(0.50, 0.72, 0.99, 0.38, 0.78), \n  c(0.93, 0.21, 0.65, 0.13, 0.27), \n  c(0.39, 0.01, 0.38, 0.87, 0.34)\n)\n\nthreshold <- function(x, cutoff = 0.8) x[x > cutoff]\nx1 %>% sapply(threshold) %>% str()\n#> List of 3\n#>  $ : num 0.91\n#>  $ : num [1:2] 0.9 0.94\n#>  $ : num(0)\nx2 %>% sapply(threshold) %>% str()\n#>  num [1:3] 0.99 0.93 0.87"},{"path":"iteration.html","id":"exercises-61","chapter":"21 Iteration","heading":"21.5.3 Exercises","text":"Write code uses one map functions :\nCompute mean every column mtcars.\nDetermine type column nycflights13::flights.\nCompute number unique values column iris.\nGenerate 10 random normals distributions means -10, 0, 10, 100.\nWrite code uses one map functions :Compute mean every column mtcars.Determine type column nycflights13::flights.Compute number unique values column iris.Generate 10 random normals distributions means -10, 0, 10, 100.can create single vector column data frame\nindicates whether ’s factor?can create single vector column data frame\nindicates whether ’s factor?happens use map functions vectors aren’t lists?\nmap(1:5, runif) ? ?happens use map functions vectors aren’t lists?\nmap(1:5, runif) ? ?map(-2:2, rnorm, n = 5) ? ?\nmap_dbl(-2:2, rnorm, n = 5) ? ?map(-2:2, rnorm, n = 5) ? ?\nmap_dbl(-2:2, rnorm, n = 5) ? ?Rewrite map(x, function(df) lm(mpg ~ wt, data = df)) eliminate \nanonymous function.Rewrite map(x, function(df) lm(mpg ~ wt, data = df)) eliminate \nanonymous function.","code":""},{"path":"iteration.html","id":"dealing-with-failure","chapter":"21 Iteration","heading":"21.6 Dealing with failure","text":"use map functions repeat many operations, chances much higher one operations fail. happens, ’ll get error message, output. annoying: one failure prevent accessing successes? ensure one bad apple doesn’t ruin whole barrel?section ’ll learn deal situation new function: safely(). safely() adverb: takes function (verb) returns modified version. case, modified function never throw error. Instead, always returns list two elements:result original result. error, NULL.result original result. error, NULL.error error object. operation successful, \nNULL.error error object. operation successful, \nNULL.(might familiar try() function base R. ’s similar, sometimes returns original result sometimes returns error object ’s difficult work .)Let’s illustrate simple example: log():function succeeds, result element contains result error element NULL. function fails, result element NULL error element contains error object.safely() designed work map:easier work two lists: one errors one output. ’s easy get purrr::transpose():’s deal errors, typically ’ll either look values x y error, work values y ok:Purrr provides two useful adverbs:Like safely(), possibly() always succeeds. ’s simpler safely(),\ngive default value return error.\n\nx <- list(1, 10, \"\")\nx %>% map_dbl(possibly(log, NA_real_))\n#> [1] 0.000000 2.302585       NALike safely(), possibly() always succeeds. ’s simpler safely(),\ngive default value return error.quietly() performs similar role safely(), instead capturing\nerrors, captures printed output, messages, warnings:\n\nx <- list(1, -1)\nx %>% map(quietly(log)) %>% str()\n#> List 2\n#>  $ :List 4\n#>   ..$ result  : num 0\n#>   ..$ output  : chr \"\"\n#>   ..$ warnings: chr(0) \n#>   ..$ messages: chr(0) \n#>  $ :List 4\n#>   ..$ result  : num NaN\n#>   ..$ output  : chr \"\"\n#>   ..$ warnings: chr \"NaNs produced\"\n#>   ..$ messages: chr(0)quietly() performs similar role safely(), instead capturing\nerrors, captures printed output, messages, warnings:","code":"\nsafe_log <- safely(log)\nstr(safe_log(10))\n#> List of 2\n#>  $ result: num 2.3\n#>  $ error : NULL\nstr(safe_log(\"a\"))\n#> List of 2\n#>  $ result: NULL\n#>  $ error :List of 2\n#>   ..$ message: chr \"non-numeric argument to mathematical function\"\n#>   ..$ call   : language .Primitive(\"log\")(x, base)\n#>   ..- attr(*, \"class\")= chr [1:3] \"simpleError\" \"error\" \"condition\"\nx <- list(1, 10, \"a\")\ny <- x %>% map(safely(log))\nstr(y)\n#> List of 3\n#>  $ :List of 2\n#>   ..$ result: num 0\n#>   ..$ error : NULL\n#>  $ :List of 2\n#>   ..$ result: num 2.3\n#>   ..$ error : NULL\n#>  $ :List of 2\n#>   ..$ result: NULL\n#>   ..$ error :List of 2\n#>   .. ..$ message: chr \"non-numeric argument to mathematical function\"\n#>   .. ..$ call   : language .Primitive(\"log\")(x, base)\n#>   .. ..- attr(*, \"class\")= chr [1:3] \"simpleError\" \"error\" \"condition\"\ny <- y %>% transpose()\nstr(y)\n#> List of 2\n#>  $ result:List of 3\n#>   ..$ : num 0\n#>   ..$ : num 2.3\n#>   ..$ : NULL\n#>  $ error :List of 3\n#>   ..$ : NULL\n#>   ..$ : NULL\n#>   ..$ :List of 2\n#>   .. ..$ message: chr \"non-numeric argument to mathematical function\"\n#>   .. ..$ call   : language .Primitive(\"log\")(x, base)\n#>   .. ..- attr(*, \"class\")= chr [1:3] \"simpleError\" \"error\" \"condition\"\nis_ok <- y$error %>% map_lgl(is_null)\nx[!is_ok]\n#> [[1]]\n#> [1] \"a\"\ny$result[is_ok] %>% flatten_dbl()\n#> [1] 0.000000 2.302585\nx <- list(1, 10, \"a\")\nx %>% map_dbl(possibly(log, NA_real_))\n#> [1] 0.000000 2.302585       NA\nx <- list(1, -1)\nx %>% map(quietly(log)) %>% str()\n#> List of 2\n#>  $ :List of 4\n#>   ..$ result  : num 0\n#>   ..$ output  : chr \"\"\n#>   ..$ warnings: chr(0) \n#>   ..$ messages: chr(0) \n#>  $ :List of 4\n#>   ..$ result  : num NaN\n#>   ..$ output  : chr \"\"\n#>   ..$ warnings: chr \"NaNs produced\"\n#>   ..$ messages: chr(0)"},{"path":"iteration.html","id":"mapping-over-multiple-arguments","chapter":"21 Iteration","heading":"21.7 Mapping over multiple arguments","text":"far ’ve mapped along single input. often multiple related inputs need iterate along parallel. ’s job map2() pmap() functions. example, imagine want simulate random normals different means. know map():also want vary standard deviation? One way iterate indices index vectors means sds:obfuscates intent code. Instead use map2() iterates two vectors parallel:map2() generates series function calls:Note arguments vary call come function; arguments every call come .Like map(), map2() just wrapper around loop:also imagine map3(), map4(), map5(), map6() etc, get tedious quickly. Instead, purrr provides pmap() takes list arguments. might use wanted vary mean, standard deviation, number samples:looks like:don’t name list’s elements, pmap() use positional matching calling function. ’s little fragile, makes code harder read, ’s better name arguments:generates longer, safer, calls:Since arguments length, makes sense store data frame:soon code gets complicated, think data frame good approach ensures column name length columns.","code":"\nmu <- list(5, 10, -3)\nmu %>% \n  map(rnorm, n = 5) %>% \n  str()\n#> List of 3\n#>  $ : num [1:5] 5.63 7.1 4.39 3.37 4.99\n#>  $ : num [1:5] 9.34 9.33 9.52 11.32 10.64\n#>  $ : num [1:5] -2.49 -4.75 -2.11 -2.78 -2.42\nsigma <- list(1, 5, 10)\nseq_along(mu) %>% \n  map(~rnorm(5, mu[[.x]], sigma[[.x]])) %>% \n  str()\n#> List of 3\n#>  $ : num [1:5] 4.82 5.74 4 2.06 5.72\n#>  $ : num [1:5] 6.51 0.529 10.381 14.377 12.269\n#>  $ : num [1:5] -11.51 2.66 8.52 -10.56 -7.89\nmap2(mu, sigma, rnorm, n = 5) %>% str()\n#> List of 3\n#>  $ : num [1:5] 3.83 4.52 5.12 3.23 3.59\n#>  $ : num [1:5] 13.55 3.8 8.16 12.31 8.39\n#>  $ : num [1:5] -15.872 -13.3 12.141 0.469 14.794\nmap2 <- function(x, y, f, ...) {\n  out <- vector(\"list\", length(x))\n  for (i in seq_along(x)) {\n    out[[i]] <- f(x[[i]], y[[i]], ...)\n  }\n  out\n}\nn <- list(1, 3, 5)\nargs1 <- list(n, mu, sigma)\nargs1 %>%\n  pmap(rnorm) %>% \n  str()\n#> List of 3\n#>  $ : num 5.39\n#>  $ : num [1:3] 5.41 2.08 9.58\n#>  $ : num [1:5] -23.85 -2.96 -6.56 8.46 -5.21\nargs2 <- list(mean = mu, sd = sigma, n = n)\nargs2 %>% \n  pmap(rnorm) %>% \n  str()\nparams <- tribble(\n  ~mean, ~sd, ~n,\n    5,     1,  1,\n   10,     5,  3,\n   -3,    10,  5\n)\nparams %>% \n  pmap(rnorm)\n#> [[1]]\n#> [1] 6.018179\n#> \n#> [[2]]\n#> [1]  8.681404 18.292712  6.129566\n#> \n#> [[3]]\n#> [1] -12.239379  -5.755334  -8.933997  -4.222859   8.797842"},{"path":"iteration.html","id":"invoking-different-functions","chapter":"21 Iteration","heading":"21.7.1 Invoking different functions","text":"’s one step complexity - well varying arguments function might also vary function :handle case, can use invoke_map():first argument list functions character vector function names. second argument list lists giving arguments vary function. subsequent arguments passed every function., can use tribble() make creating matching pairs little easier:","code":"\nf <- c(\"runif\", \"rnorm\", \"rpois\")\nparam <- list(\n  list(min = -1, max = 1), \n  list(sd = 5), \n  list(lambda = 10)\n)\ninvoke_map(f, param, n = 5) %>% str()\n#> List of 3\n#>  $ : num [1:5] 0.479 0.439 -0.471 0.348 -0.581\n#>  $ : num [1:5] 2.48 3.9 7.54 -9.12 3.94\n#>  $ : int [1:5] 6 11 5 8 9\nsim <- tribble(\n  ~f,      ~params,\n  \"runif\", list(min = -1, max = 1),\n  \"rnorm\", list(sd = 5),\n  \"rpois\", list(lambda = 10)\n)\nsim %>% \n  mutate(sim = invoke_map(f, params, n = 10))"},{"path":"iteration.html","id":"walk","chapter":"21 Iteration","heading":"21.8 Walk","text":"Walk alternative map use want call function side effects, rather return value. typically want render output screen save files disk - important thing action, return value. ’s simple example:walk() generally useful compared walk2() pwalk(). example, list plots vector file names, use pwalk() save file corresponding location disk:walk(), walk2() pwalk() invisibly return ., first argument. makes suitable use middle pipelines.","code":"\nx <- list(1, \"a\", 3)\n\nx %>% \n  walk(print)\n#> [1] 1\n#> [1] \"a\"\n#> [1] 3\nlibrary(ggplot2)\nplots <- mtcars %>% \n  split(.$cyl) %>% \n  map(~ggplot(.x, aes(mpg, wt)) + geom_point())\npaths <- stringr::str_c(names(plots), \".pdf\")\n\npwalk(list(paths, plots), ggsave, path = tempdir())"},{"path":"iteration.html","id":"other-patterns-of-for-loops","chapter":"21 Iteration","heading":"21.9 Other patterns of for loops","text":"Purrr provides number functions abstract types loops. ’ll use less frequently map functions, ’re useful know . goal briefly illustrate function, hopefully come mind see similar problem future. can go look documentation details.","code":""},{"path":"iteration.html","id":"predicate-functions","chapter":"21 Iteration","heading":"21.9.1 Predicate functions","text":"number functions work predicate functions return either single TRUE FALSE.keep() discard() keep elements input predicate TRUE FALSE respectively:() every() determine predicate true \nelements.detect() finds first element predicate true; detect_index() returns position.head_while() tail_while() take elements start end vector predicate true:","code":"\niris %>% \n  keep(is.factor) %>% \n  str()\n#> 'data.frame':    150 obs. of  1 variable:\n#>  $ Species: Factor w/ 3 levels \"setosa\",\"versicolor\",..: 1 1 1 1 1 1 1 1 1 1 ...\n\niris %>% \n  discard(is.factor) %>% \n  str()\n#> 'data.frame':    150 obs. of  4 variables:\n#>  $ Sepal.Length: num  5.1 4.9 4.7 4.6 5 5.4 4.6 5 4.4 4.9 ...\n#>  $ Sepal.Width : num  3.5 3 3.2 3.1 3.6 3.9 3.4 3.4 2.9 3.1 ...\n#>  $ Petal.Length: num  1.4 1.4 1.3 1.5 1.4 1.7 1.4 1.5 1.4 1.5 ...\n#>  $ Petal.Width : num  0.2 0.2 0.2 0.2 0.2 0.4 0.3 0.2 0.2 0.1 ...\nx <- list(1:5, letters, list(10))\n\nx %>% \n  some(is_character)\n#> [1] TRUE\n\nx %>% \n  every(is_vector)\n#> [1] TRUE\nx <- sample(10)\nx\n#>  [1] 10  6  1  3  2  4  5  8  9  7\n\nx %>% \n  detect(~ .x > 5)\n#> [1] 10\n\nx %>% \n  detect_index(~ .x > 5)\n#> [1] 1\nx %>% \n  head_while(~ .x > 5)\n#> [1] 10  6\n\nx %>% \n  tail_while(~ .x > 5)\n#> [1] 8 9 7"},{"path":"iteration.html","id":"reduce-and-accumulate","chapter":"21 Iteration","heading":"21.9.2 Reduce and accumulate","text":"Sometimes complex list want reduce simple list repeatedly applying function reduces pair singleton. useful want apply two-table dplyr verb multiple tables. example, might list data frames, want reduce single data frame joining elements together:maybe list vectors, want find intersection:reduce() takes “binary” function (.e. function two primary inputs), applies repeatedly list single element left.accumulate() similar keeps interim results. use implement cumulative sum:","code":"\ndfs <- list(\n  age = tibble(name = \"John\", age = 30),\n  sex = tibble(name = c(\"John\", \"Mary\"), sex = c(\"M\", \"F\")),\n  trt = tibble(name = \"Mary\", treatment = \"A\")\n)\n\ndfs %>% reduce(full_join)\n#> Joining, by = \"name\"\n#> Joining, by = \"name\"\n#> # A tibble: 2 x 4\n#>   name    age sex   treatment\n#>   <chr> <dbl> <chr> <chr>    \n#> 1 John     30 M     <NA>     \n#> 2 Mary     NA F     A\nvs <- list(\n  c(1, 3, 5, 6, 10),\n  c(1, 2, 3, 7, 8, 10),\n  c(1, 2, 3, 4, 8, 9, 10)\n)\n\nvs %>% reduce(intersect)\n#> [1]  1  3 10\nx <- sample(10)\nx\n#>  [1]  7  5 10  9  8  3  1  4  2  6\nx %>% accumulate(`+`)\n#>  [1]  7 12 22 31 39 42 43 47 49 55"},{"path":"iteration.html","id":"exercises-62","chapter":"21 Iteration","heading":"21.9.3 Exercises","text":"Implement version every() using loop. Compare \npurrr::every(). purrr’s version version doesn’t?Implement version every() using loop. Compare \npurrr::every(). purrr’s version version doesn’t?Create enhanced col_summary() applies summary function every\nnumeric column data frame.Create enhanced col_summary() applies summary function every\nnumeric column data frame.possible base R equivalent col_summary() :\n\ncol_sum3 <- function(df, f) {\n  is_num <- sapply(df, .numeric)\n  df_num <- df[, is_num]\n\n  sapply(df_num, f)\n}\nnumber bugs illustrated following inputs:\n\ndf <- tibble(\n  x = 1:3, \n  y = 3:1,\n  z = c(\"\", \"b\", \"c\")\n)\n# OK\ncol_sum3(df, mean)\n# problems: always return numeric vector\ncol_sum3(df[1:2], mean)\ncol_sum3(df[1], mean)\ncol_sum3(df[0], mean)\ncauses bugs?possible base R equivalent col_summary() :number bugs illustrated following inputs:causes bugs?","code":"\ncol_sum3 <- function(df, f) {\n  is_num <- sapply(df, is.numeric)\n  df_num <- df[, is_num]\n\n  sapply(df_num, f)\n}\ndf <- tibble(\n  x = 1:3, \n  y = 3:1,\n  z = c(\"a\", \"b\", \"c\")\n)\n# OK\ncol_sum3(df, mean)\n# Has problems: don't always return numeric vector\ncol_sum3(df[1:2], mean)\ncol_sum3(df[1], mean)\ncol_sum3(df[0], mean)"},{"path":"model-intro.html","id":"model-intro","chapter":"22 Introduction","heading":"22 Introduction","text":"Now equipped powerful programming tools can finally return modelling. ’ll use new tools data wrangling programming, fit many models understand work. focus book exploration, confirmation formal inference. ’ll learn basic tools help understand variation within models.goal model provide simple low-dimensional summary dataset. Ideally, model capture true “signals” (.e. patterns generated phenomenon interest), ignore “noise” (.e. random variation ’re interested ). cover “predictive” models, , name suggests, generate predictions. another type model ’re going discuss: “data discovery” models. models don’t make predictions, instead help discover interesting relationships within data. (two categories models sometimes called supervised unsupervised, don’t think terminology particularly illuminating.)book going give deep understanding mathematical theory underlies models. , however, build intuition statistical models work, give family useful tools allow use models better understand data:model basics, ’ll learn models work mechanistically, focussing \nimportant family linear models. ’ll learn general tools gaining\ninsight predictive model tells data, focussing \nsimple simulated datasets.model basics, ’ll learn models work mechanistically, focussing \nimportant family linear models. ’ll learn general tools gaining\ninsight predictive model tells data, focussing \nsimple simulated datasets.model building, ’ll learn use models pull known\npatterns real data. recognised important pattern\n’s useful make explicit model, can\neasily see subtler signals remain.model building, ’ll learn use models pull known\npatterns real data. recognised important pattern\n’s useful make explicit model, can\neasily see subtler signals remain.many models, ’ll learn use many simple models help\nunderstand complex datasets. powerful technique, access\n’ll need combine modelling programming tools.many models, ’ll learn use many simple models help\nunderstand complex datasets. powerful technique, access\n’ll need combine modelling programming tools.topics notable don’t include: tools quantitatively assessing models. deliberate: precisely quantifying model requires couple big ideas just don’t space cover . now, ’ll rely qualitative assessment natural scepticism. Learning models, ’ll point resources can learn .","code":""},{"path":"model-intro.html","id":"hypothesis-generation-vs.-hypothesis-confirmation","chapter":"22 Introduction","heading":"22.1 Hypothesis generation vs. hypothesis confirmation","text":"book, going use models tool exploration, completing trifecta tools EDA introduced Part 1. models usually taught, see, models important tool exploration. Traditionally, focus modelling inference, confirming hypothesis true. correctly complicated, hard. pair ideas must understand order inference correctly:observation can either used exploration confirmation,\n.observation can either used exploration confirmation,\n.can use observation many times like exploration,\ncan use confirmation. soon use \nobservation twice, ’ve switched confirmation exploration.can use observation many times like exploration,\ncan use confirmation. soon use \nobservation twice, ’ve switched confirmation exploration.necessary confirm hypothesis must use data independent data used generate hypothesis. Otherwise optimistic. absolutely nothing wrong exploration, never sell exploratory analysis confirmatory analysis fundamentally misleading.serious confirmatory analysis, one approach split data three pieces begin analysis:60% data goes training (exploration) set. ’re\nallowed anything like data: visualise fit tons\nmodels .60% data goes training (exploration) set. ’re\nallowed anything like data: visualise fit tons\nmodels .20% goes query set. can use data compare models\nvisualisations hand, ’re allowed use part \nautomated process.20% goes query set. can use data compare models\nvisualisations hand, ’re allowed use part \nautomated process.20% held back test set. can use data , \ntest final model.20% held back test set. can use data , \ntest final model.partitioning allows explore training data, occasionally generating candidate hypotheses check query set. confident right model, can check test data.(Note even confirmatory modelling, still need EDA. don’t EDA remain blind quality problems data.)","code":""},{"path":"model-basics.html","id":"model-basics","chapter":"23 Model basics","heading":"23 Model basics","text":"","code":""},{"path":"model-basics.html","id":"introduction-15","chapter":"23 Model basics","heading":"23.1 Introduction","text":"goal model provide simple low-dimensional summary dataset. context book ’re going use models partition data patterns residuals. Strong patterns hide subtler trends, ’ll use models help peel back layers structure explore dataset.However, can start using models interesting, real, datasets, need understand basics models work. reason, chapter book unique uses simulated datasets. datasets simple, interesting, help understand essence modelling apply techniques real data next chapter.two parts model:First, define family models express precise, \ngeneric, pattern want capture. example, pattern\nmight straight line, quadratic curve. express\nmodel family equation like y = a_1 * x + a_2 \ny = a_1 * x ^ a_2. , x y known variables \ndata, a_1 a_2 parameters can vary capture\ndifferent patterns.First, define family models express precise, \ngeneric, pattern want capture. example, pattern\nmight straight line, quadratic curve. express\nmodel family equation like y = a_1 * x + a_2 \ny = a_1 * x ^ a_2. , x y known variables \ndata, a_1 a_2 parameters can vary capture\ndifferent patterns.Next, generate fitted model finding model \nfamily closest data. takes generic model\nfamily makes specific, like y = 3 * x + 7 y = 9 * x ^ 2.Next, generate fitted model finding model \nfamily closest data. takes generic model\nfamily makes specific, like y = 3 * x + 7 y = 9 * x ^ 2.’s important understand fitted model just closest model family models. implies “best” model (according criteria); doesn’t imply good model certainly doesn’t imply model “true”. George Box puts well famous aphorism:models wrong, useful.’s worth reading fuller context quote:Now remarkable system existing real world\nexactly represented simple model. However, cunningly chosen\nparsimonious models often provide remarkably useful approximations. \nexample, law PV = RT relating pressure P, volume V temperature T \n“ideal” gas via constant R exactly true real gas, \nfrequently provides useful approximation furthermore structure \ninformative since springs physical view behavior gas\nmolecules.model need ask question “model true?”.\n“truth” “whole truth” answer must “”. \nquestion interest “model illuminating useful?”.goal model uncover truth, discover simple approximation still useful.","code":""},{"path":"model-basics.html","id":"prerequisites-15","chapter":"23 Model basics","heading":"23.1.1 Prerequisites","text":"chapter ’ll use modelr package wraps around base R’s modelling functions make work naturally pipe.","code":"\nlibrary(tidyverse)\n\nlibrary(modelr)\noptions(na.action = na.warn)"},{"path":"model-basics.html","id":"a-simple-model","chapter":"23 Model basics","heading":"23.2 A simple model","text":"Lets take look simulated dataset sim1, included modelr package. contains two continuous variables, x y. Let’s plot see ’re related:can see strong pattern data. Let’s use model capture pattern make explicit. ’s job supply basic form model. case, relationship looks linear, .e. y = a_0 + a_1 * x. Let’s start getting feel models family look like randomly generating overlaying data. simple case, can use geom_abline() takes slope intercept parameters. Later ’ll learn general techniques work model.250 models plot, lot really bad! need find good models making precise intuition good model “close” data. need way quantify distance data model. can fit model finding value a_0 a_1 generate model smallest distance data.One easy place start find vertical distance point model, following diagram. (Note ’ve shifted x values slightly can see individual distances.)distance just difference y value given model (prediction), actual y value data (response).compute distance, first turn model family R function. takes model parameters data inputs, gives values predicted model output:Next, need way compute overall distance predicted actual values. words, plot shows 30 distances: collapse single number?One common way statistics use “root-mean-squared deviation”. compute difference actual predicted, square , average , take square root. distance lots appealing mathematical properties, ’re going talk . ’ll just take word !Now can use purrr compute distance models defined . need helper function distance function expects model numeric vector length 2.Next, let’s overlay 10 best models data. ’ve coloured models -dist: easy way make sure best models (.e. ones smallest distance) get brighest colours.can also think models observations, visualising scatterplot a1 vs a2, coloured -dist. can longer directly see model compares data, can see many models . , ’ve highlighted 10 best models, time drawing red circles underneath .Instead trying lots random models, systematic generate evenly spaced grid points (called grid search). picked parameters grid roughly looking best models plot .overlay best 10 models back original data, look pretty good:imagine iteratively making grid finer finer narrowed best model. ’s better way tackle problem: numerical minimisation tool called Newton-Raphson search. intuition Newton-Raphson pretty simple: pick starting point look around steepest slope. ski slope little way, repeat , can’t go lower. R, can optim():Don’t worry much details optim() works. ’s intuition ’s important . function defines distance model dataset, algorithm can minimise distance modifying parameters model can find best model. neat thing approach work family models can write equation .’s one approach can use model, ’s special case broader family: linear models. linear model general form y = a_1 + a_2 * x_1 + a_3 * x_2 + ... + a_n * x_(n - 1). simple model equivalent general linear model n 2 x_1 x. R tool specifically designed fitting linear models called lm(). lm() special way specify model family: formulas. Formulas look like y ~ x, lm() translate function like y = a_1 + a_2 * x. can fit model look output:exactly values got optim()! Behind scenes lm() doesn’t use optim() instead takes advantage mathematical structure linear models. Using connections geometry, calculus, linear algebra, lm() actually finds closest model single step, using sophisticated algorithm. approach faster, guarantees global minimum.","code":"\nggplot(sim1, aes(x, y)) + \n  geom_point()\nmodels <- tibble(\n  a1 = runif(250, -20, 40),\n  a2 = runif(250, -5, 5)\n)\n\nggplot(sim1, aes(x, y)) + \n  geom_abline(aes(intercept = a1, slope = a2), data = models, alpha = 1/4) +\n  geom_point() \nmodel1 <- function(a, data) {\n  a[1] + data$x * a[2]\n}\nmodel1(c(7, 1.5), sim1)\n#>  [1]  8.5  8.5  8.5 10.0 10.0 10.0 11.5 11.5 11.5 13.0 13.0 13.0 14.5 14.5 14.5\n#> [16] 16.0 16.0 16.0 17.5 17.5 17.5 19.0 19.0 19.0 20.5 20.5 20.5 22.0 22.0 22.0\nmeasure_distance <- function(mod, data) {\n  diff <- data$y - model1(mod, data)\n  sqrt(mean(diff ^ 2))\n}\nmeasure_distance(c(7, 1.5), sim1)\n#> [1] 2.665212\nsim1_dist <- function(a1, a2) {\n  measure_distance(c(a1, a2), sim1)\n}\n\nmodels <- models %>% \n  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))\nmodels\n#> # A tibble: 250 x 3\n#>       a1      a2  dist\n#>    <dbl>   <dbl> <dbl>\n#> 1 -15.2   0.0889  30.8\n#> 2  30.1  -0.827   13.2\n#> 3  16.0   2.27    13.2\n#> 4 -10.6   1.38    18.7\n#> 5 -19.6  -1.04    41.8\n#> 6   7.98  4.59    19.3\n#> # … with 244 more rows\nggplot(sim1, aes(x, y)) + \n  geom_point(size = 2, colour = \"grey30\") + \n  geom_abline(\n    aes(intercept = a1, slope = a2, colour = -dist), \n    data = filter(models, rank(dist) <= 10)\n  )\nggplot(models, aes(a1, a2)) +\n  geom_point(data = filter(models, rank(dist) <= 10), size = 4, colour = \"red\") +\n  geom_point(aes(colour = -dist))\ngrid <- expand.grid(\n  a1 = seq(-5, 20, length = 25),\n  a2 = seq(1, 3, length = 25)\n  ) %>% \n  mutate(dist = purrr::map2_dbl(a1, a2, sim1_dist))\n\ngrid %>% \n  ggplot(aes(a1, a2)) +\n  geom_point(data = filter(grid, rank(dist) <= 10), size = 4, colour = \"red\") +\n  geom_point(aes(colour = -dist)) \nggplot(sim1, aes(x, y)) + \n  geom_point(size = 2, colour = \"grey30\") + \n  geom_abline(\n    aes(intercept = a1, slope = a2, colour = -dist), \n    data = filter(grid, rank(dist) <= 10)\n  )\nbest <- optim(c(0, 0), measure_distance, data = sim1)\nbest$par\n#> [1] 4.222248 2.051204\n\nggplot(sim1, aes(x, y)) + \n  geom_point(size = 2, colour = \"grey30\") + \n  geom_abline(intercept = best$par[1], slope = best$par[2])\nsim1_mod <- lm(y ~ x, data = sim1)\ncoef(sim1_mod)\n#> (Intercept)           x \n#>    4.220822    2.051533"},{"path":"model-basics.html","id":"exercises-63","chapter":"23 Model basics","heading":"23.2.1 Exercises","text":"One downside linear model sensitive unusual values\ndistance incorporates squared term. Fit linear model \nsimulated data , visualise results. Rerun times \ngenerate different simulated datasets. notice model?\n\nsim1a <- tibble(\n  x = rep(1:10, = 3),\n  y = x * 1.5 + 6 + rt(length(x), df = 2)\n)One downside linear model sensitive unusual values\ndistance incorporates squared term. Fit linear model \nsimulated data , visualise results. Rerun times \ngenerate different simulated datasets. notice model?One way make linear models robust use different distance\nmeasure. example, instead root-mean-squared distance, use\nmean-absolute distance:\n\nmeasure_distance <- function(mod, data) {\n  diff <- data$y - model1(mod, data)\n  mean(abs(diff))\n}\nUse optim() fit model simulated data compare \nlinear model.One way make linear models robust use different distance\nmeasure. example, instead root-mean-squared distance, use\nmean-absolute distance:Use optim() fit model simulated data compare \nlinear model.One challenge performing numerical optimisation ’s \nguaranteed find one local optimum. ’s problem optimising\nthree parameter model like ?\n\nmodel1 <- function(, data) {\n  [1] + data$x * [2] + [3]\n}One challenge performing numerical optimisation ’s \nguaranteed find one local optimum. ’s problem optimising\nthree parameter model like ?","code":"\nsim1a <- tibble(\n  x = rep(1:10, each = 3),\n  y = x * 1.5 + 6 + rt(length(x), df = 2)\n)\nmeasure_distance <- function(mod, data) {\n  diff <- data$y - model1(mod, data)\n  mean(abs(diff))\n}\nmodel1 <- function(a, data) {\n  a[1] + data$x * a[2] + a[3]\n}"},{"path":"model-basics.html","id":"visualising-models","chapter":"23 Model basics","heading":"23.3 Visualising models","text":"simple models, like one , can figure pattern model captures carefully studying model family fitted coefficients. ever take statistics course modelling, ’re likely spend lot time just . , however, ’re going take different tack. ’re going focus understanding model looking predictions. big advantage: every type predictive model makes predictions (otherwise use ?) can use set techniques understand type predictive model.’s also useful see model doesn’t capture, -called residuals left subtracting predictions data. Residuals powerful allow us use models remove striking patterns can study subtler trends remain.","code":""},{"path":"model-basics.html","id":"predictions","chapter":"23 Model basics","heading":"23.3.1 Predictions","text":"visualise predictions model, start generating evenly spaced grid values covers region data lies. easiest way use modelr::data_grid(). first argument data frame, subsequent argument finds unique variables generates combinations:(get interesting start add variables model.)Next add predictions. ’ll use modelr::add_predictions() takes data frame model. adds predictions model new column data frame:(can also use function add predictions original dataset.)Next, plot predictions. might wonder extra work compared just using geom_abline(). advantage approach work model R, simplest complex. ’re limited visualisation skills. ideas visualise complex model types, might try http://vita..co.nz/papers/model-vis.html.","code":"\ngrid <- sim1 %>% \n  data_grid(x) \ngrid\n#> # A tibble: 10 x 1\n#>       x\n#>   <int>\n#> 1     1\n#> 2     2\n#> 3     3\n#> 4     4\n#> 5     5\n#> 6     6\n#> # … with 4 more rows\ngrid <- grid %>% \n  add_predictions(sim1_mod) \ngrid\n#> # A tibble: 10 x 2\n#>       x  pred\n#>   <int> <dbl>\n#> 1     1  6.27\n#> 2     2  8.32\n#> 3     3 10.4 \n#> 4     4 12.4 \n#> 5     5 14.5 \n#> 6     6 16.5 \n#> # … with 4 more rows\nggplot(sim1, aes(x)) +\n  geom_point(aes(y = y)) +\n  geom_line(aes(y = pred), data = grid, colour = \"red\", size = 1)"},{"path":"model-basics.html","id":"residuals","chapter":"23 Model basics","heading":"23.3.2 Residuals","text":"flip-side predictions residuals. predictions tells pattern model captured, residuals tell model missed. residuals just distances observed predicted values computed .add residuals data add_residuals(), works much like add_predictions(). Note, however, use original dataset, manufactured grid. compute residuals need actual y values.different ways understand residuals tell us model. One way simply draw frequency polygon help us understand spread residuals:helps calibrate quality model: far away predictions observed values? Note average residual always 0.’ll often want recreate plots using residuals instead original predictor. ’ll see lot next chapter.looks like random noise, suggesting model done good job capturing patterns dataset.","code":"\nsim1 <- sim1 %>% \n  add_residuals(sim1_mod)\nsim1\n#> # A tibble: 30 x 3\n#>       x     y  resid\n#>   <int> <dbl>  <dbl>\n#> 1     1  4.20 -2.07 \n#> 2     1  7.51  1.24 \n#> 3     1  2.13 -4.15 \n#> 4     2  8.99  0.665\n#> 5     2 10.2   1.92 \n#> 6     2 11.3   2.97 \n#> # … with 24 more rows\nggplot(sim1, aes(resid)) + \n  geom_freqpoly(binwidth = 0.5)\nggplot(sim1, aes(x, resid)) + \n  geom_ref_line(h = 0) +\n  geom_point() "},{"path":"model-basics.html","id":"exercises-64","chapter":"23 Model basics","heading":"23.3.3 Exercises","text":"Instead using lm() fit straight line, can use loess()\nfit smooth curve. Repeat process model fitting,\ngrid generation, predictions, visualisation sim1 using\nloess() instead lm(). result compare \ngeom_smooth()?Instead using lm() fit straight line, can use loess()\nfit smooth curve. Repeat process model fitting,\ngrid generation, predictions, visualisation sim1 using\nloess() instead lm(). result compare \ngeom_smooth()?add_predictions() paired gather_predictions() \nspread_predictions(). three functions differ?add_predictions() paired gather_predictions() \nspread_predictions(). three functions differ?geom_ref_line() ? package come ?\ndisplaying reference line plots showing residuals\nuseful important?geom_ref_line() ? package come ?\ndisplaying reference line plots showing residuals\nuseful important?might want look frequency polygon absolute residuals?\npros cons compared looking raw residuals?might want look frequency polygon absolute residuals?\npros cons compared looking raw residuals?","code":""},{"path":"model-basics.html","id":"formulas-and-model-families","chapter":"23 Model basics","heading":"23.4 Formulas and model families","text":"’ve seen formulas using facet_wrap() facet_grid(). R, formulas provide general way getting “special behaviour”. Rather evaluating values variables right away, capture can interpreted function.majority modelling functions R use standard conversion formulas functions. ’ve seen one simple conversion already: y ~ x translated y = a_1 + a_2 * x. want see R actually , can use model_matrix() function. takes data frame formula returns tibble defines model equation: column output associated one coefficient model, function always y = a_1 * out_1 + a_2 * out_2. simplest case y ~ x1 shows us something interesting:way R adds intercept model just column full ones. default, R always add column. don’t want, need explicitly drop -1:model matrix grows unsurprising way add variables model:formula notation sometimes called “Wilkinson-Rogers notation”, initially described Symbolic Description Factorial Models Analysis Variance, G. N. Wilkinson C. E. Rogers https://www.jstor.org/stable/2346786. ’s worth digging reading original paper ’d like understand full details modelling algebra.following sections expand formula notation works categorical variables, interactions, transformation.","code":"\ndf <- tribble(\n  ~y, ~x1, ~x2,\n  4, 2, 5,\n  5, 1, 6\n)\nmodel_matrix(df, y ~ x1)\n#> # A tibble: 2 x 2\n#>   `(Intercept)`    x1\n#>           <dbl> <dbl>\n#> 1             1     2\n#> 2             1     1\nmodel_matrix(df, y ~ x1 - 1)\n#> # A tibble: 2 x 1\n#>      x1\n#>   <dbl>\n#> 1     2\n#> 2     1\nmodel_matrix(df, y ~ x1 + x2)\n#> # A tibble: 2 x 3\n#>   `(Intercept)`    x1    x2\n#>           <dbl> <dbl> <dbl>\n#> 1             1     2     5\n#> 2             1     1     6"},{"path":"model-basics.html","id":"categorical-variables","chapter":"23 Model basics","heading":"23.4.1 Categorical variables","text":"Generating function formula straight forward predictor continuous, things get bit complicated predictor categorical. Imagine formula like y ~ sex, sex either male female. doesn’t make sense convert formula like y = a_0 + a_1 * sex sex isn’t number - can’t multiply ! Instead R convert y = a_0 + a_1 * sex_male sex_male one sex male zero otherwise:might wonder R also doesn’t create sexfemale column. problem create column perfectly predictable based columns (.e. sexfemale = 1 - sexmale). Unfortunately exact details problem beyond scope book, basically creates model family flexible, infinitely many models equally close data.Fortunately, however, focus visualising predictions don’t need worry exact parameterisation. Let’s look data models make concrete. ’s sim2 dataset modelr:can fit model , generate predictions:Effectively, model categorical x predict mean value category. (? mean minimises root-mean-squared distance.) ’s easy see overlay predictions top original data:can’t make predictions levels didn’t observe. Sometimes ’ll accident ’s good recognise error message:","code":"\ndf <- tribble(\n  ~ sex, ~ response,\n  \"male\", 1,\n  \"female\", 2,\n  \"male\", 1\n)\nmodel_matrix(df, response ~ sex)\n#> # A tibble: 3 x 2\n#>   `(Intercept)` sexmale\n#>           <dbl>   <dbl>\n#> 1             1       1\n#> 2             1       0\n#> 3             1       1\nggplot(sim2) + \n  geom_point(aes(x, y))\nmod2 <- lm(y ~ x, data = sim2)\n\ngrid <- sim2 %>% \n  data_grid(x) %>% \n  add_predictions(mod2)\ngrid\n#> # A tibble: 4 x 2\n#>   x      pred\n#>   <chr> <dbl>\n#> 1 a      1.15\n#> 2 b      8.12\n#> 3 c      6.13\n#> 4 d      1.91\nggplot(sim2, aes(x)) + \n  geom_point(aes(y = y)) +\n  geom_point(data = grid, aes(y = pred), colour = \"red\", size = 4)\ntibble(x = \"e\") %>% \n  add_predictions(mod2)\n#> Error in model.frame.default(Terms, newdata, na.action = na.action, xlev = object$xlevels): factor x has new level e"},{"path":"model-basics.html","id":"interactions-continuous-and-categorical","chapter":"23 Model basics","heading":"23.4.2 Interactions (continuous and categorical)","text":"happens combine continuous categorical variable? sim3 contains categorical predictor continuous predictor. can visualise simple plot:two possible models fit data:add variables +, model estimate effect independent others. ’s possible fit -called interaction using *. example, y ~ x1 * x2 translated y = a_0 + a_1 * x1 + a_2 * x2 + a_12 * x1 * x2. Note whenever use *, interaction individual components included model.visualise models need two new tricks:two predictors, need give data_grid() variables.\nfinds unique values x1 x2 generates \ncombinations.two predictors, need give data_grid() variables.\nfinds unique values x1 x2 generates \ncombinations.generate predictions models simultaneously, can use\ngather_predictions() adds prediction row. \ncomplement gather_predictions() spread_predictions() adds\nprediction new column.generate predictions models simultaneously, can use\ngather_predictions() adds prediction row. \ncomplement gather_predictions() spread_predictions() adds\nprediction new column.Together gives us:can visualise results models one plot using facetting:Note model uses + slope line, different intercepts. model uses * different slope intercept line.model better data? can take look residuals. ’ve facetted model x2 makes easier see pattern within group.little obvious pattern residuals mod2. residuals mod1 show model clearly missed pattern b, less , still present pattern c, d. might wonder ’s precise way tell mod1 mod2 better. , requires lot mathematical background, don’t really care. , ’re interested qualitative assessment whether model captured pattern ’re interested .","code":"\nggplot(sim3, aes(x1, y)) + \n  geom_point(aes(colour = x2))\nmod1 <- lm(y ~ x1 + x2, data = sim3)\nmod2 <- lm(y ~ x1 * x2, data = sim3)\ngrid <- sim3 %>% \n  data_grid(x1, x2) %>% \n  gather_predictions(mod1, mod2)\ngrid\n#> # A tibble: 80 x 4\n#>   model    x1 x2     pred\n#>   <chr> <int> <fct> <dbl>\n#> 1 mod1      1 a      1.67\n#> 2 mod1      1 b      4.56\n#> 3 mod1      1 c      6.48\n#> 4 mod1      1 d      4.03\n#> 5 mod1      2 a      1.48\n#> 6 mod1      2 b      4.37\n#> # … with 74 more rows\nggplot(sim3, aes(x1, y, colour = x2)) + \n  geom_point() + \n  geom_line(data = grid, aes(y = pred)) + \n  facet_wrap(~ model)\nsim3 <- sim3 %>% \n  gather_residuals(mod1, mod2)\n\nggplot(sim3, aes(x1, resid, colour = x2)) + \n  geom_point() + \n  facet_grid(model ~ x2)"},{"path":"model-basics.html","id":"interactions-two-continuous","chapter":"23 Model basics","heading":"23.4.3 Interactions (two continuous)","text":"Let’s take look equivalent model two continuous variables. Initially things proceed almost identically previous example:Note use seq_range() inside data_grid(). Instead using every unique value x, ’m going use regularly spaced grid five values minimum maximum numbers. ’s probably super important , ’s useful technique general. two useful arguments seq_range():pretty = TRUE generate “pretty” sequence, .e. something looks\nnice human eye. useful want produce tables \noutput:\n\nseq_range(c(0.0123, 0.923423), n = 5)\n#> [1] 0.0123000 0.2400808 0.4678615 0.6956423 0.9234230\nseq_range(c(0.0123, 0.923423), n = 5, pretty = TRUE)\n#> [1] 0.0 0.2 0.4 0.6 0.8 1.0pretty = TRUE generate “pretty” sequence, .e. something looks\nnice human eye. useful want produce tables \noutput:trim = 0.1 trim 10% tail values. useful \nvariables long tailed distribution want focus generating\nvalues near center:\n\nx1 <- rcauchy(100)\nseq_range(x1, n = 5)\n#> [1] -115.86934  -83.52130  -51.17325  -18.82520   13.52284\nseq_range(x1, n = 5, trim = 0.10)\n#> [1] -13.841101  -8.709812  -3.578522   1.552767   6.684057\nseq_range(x1, n = 5, trim = 0.25)\n#> [1] -2.17345439 -1.05938856  0.05467728  1.16874312  2.28280896\nseq_range(x1, n = 5, trim = 0.50)\n#> [1] -0.7249565 -0.2677888  0.1893788  0.6465465  1.1037141trim = 0.1 trim 10% tail values. useful \nvariables long tailed distribution want focus generating\nvalues near center:expand = 0.1 sense opposite trim() expands \nrange 10%.\n\nx2 <- c(0, 1)\nseq_range(x2, n = 5)\n#> [1] 0.00 0.25 0.50 0.75 1.00\nseq_range(x2, n = 5, expand = 0.10)\n#> [1] -0.050  0.225  0.500  0.775  1.050\nseq_range(x2, n = 5, expand = 0.25)\n#> [1] -0.1250  0.1875  0.5000  0.8125  1.1250\nseq_range(x2, n = 5, expand = 0.50)\n#> [1] -0.250  0.125  0.500  0.875  1.250expand = 0.1 sense opposite trim() expands \nrange 10%.Next let’s try visualise model. two continuous predictors, can imagine model like 3d surface. display using geom_tile():doesn’t suggest models different! ’s partly illusion: eyes brains good accurately comparing shades colour. Instead looking surface top, look either side, showing multiple slices:shows interaction two continuous variables works basically way categorical continuous variable. interaction says ’s fixed offset: need consider values x1 x2 simultaneously order predict y.can see even just two continuous variables, coming good visualisations hard. ’s reasonable: shouldn’t expect easy understand three variables simultaneously interact! , ’re saved little ’re using models exploration, can gradually build model time. model doesn’t perfect, just help reveal little data.spent time looking residuals see figure mod2 better mod1. think , ’s pretty subtle. ’ll chance work exercises.","code":"\nmod1 <- lm(y ~ x1 + x2, data = sim4)\nmod2 <- lm(y ~ x1 * x2, data = sim4)\n\ngrid <- sim4 %>% \n  data_grid(\n    x1 = seq_range(x1, 5), \n    x2 = seq_range(x2, 5) \n  ) %>% \n  gather_predictions(mod1, mod2)\ngrid\n#> # A tibble: 50 x 4\n#>   model    x1    x2   pred\n#>   <chr> <dbl> <dbl>  <dbl>\n#> 1 mod1   -1    -1    0.996\n#> 2 mod1   -1    -0.5 -0.395\n#> 3 mod1   -1     0   -1.79 \n#> 4 mod1   -1     0.5 -3.18 \n#> 5 mod1   -1     1   -4.57 \n#> 6 mod1   -0.5  -1    1.91 \n#> # … with 44 more rows\nseq_range(c(0.0123, 0.923423), n = 5)\n#> [1] 0.0123000 0.2400808 0.4678615 0.6956423 0.9234230\nseq_range(c(0.0123, 0.923423), n = 5, pretty = TRUE)\n#> [1] 0.0 0.2 0.4 0.6 0.8 1.0\nx1 <- rcauchy(100)\nseq_range(x1, n = 5)\n#> [1] -115.86934  -83.52130  -51.17325  -18.82520   13.52284\nseq_range(x1, n = 5, trim = 0.10)\n#> [1] -13.841101  -8.709812  -3.578522   1.552767   6.684057\nseq_range(x1, n = 5, trim = 0.25)\n#> [1] -2.17345439 -1.05938856  0.05467728  1.16874312  2.28280896\nseq_range(x1, n = 5, trim = 0.50)\n#> [1] -0.7249565 -0.2677888  0.1893788  0.6465465  1.1037141\nx2 <- c(0, 1)\nseq_range(x2, n = 5)\n#> [1] 0.00 0.25 0.50 0.75 1.00\nseq_range(x2, n = 5, expand = 0.10)\n#> [1] -0.050  0.225  0.500  0.775  1.050\nseq_range(x2, n = 5, expand = 0.25)\n#> [1] -0.1250  0.1875  0.5000  0.8125  1.1250\nseq_range(x2, n = 5, expand = 0.50)\n#> [1] -0.250  0.125  0.500  0.875  1.250\nggplot(grid, aes(x1, x2)) + \n  geom_tile(aes(fill = pred)) + \n  facet_wrap(~ model)\nggplot(grid, aes(x1, pred, colour = x2, group = x2)) + \n  geom_line() +\n  facet_wrap(~ model)\nggplot(grid, aes(x2, pred, colour = x1, group = x1)) + \n  geom_line() +\n  facet_wrap(~ model)"},{"path":"model-basics.html","id":"transformations","chapter":"23 Model basics","heading":"23.4.4 Transformations","text":"can also perform transformations inside model formula. example, log(y) ~ sqrt(x1) + x2 transformed log(y) = a_1 + a_2 * sqrt(x1) + a_3 * x2. transformation involves +, *, ^, -, ’ll need wrap () R doesn’t treat like part model specification. example, y ~ x + (x ^ 2) translated y = a_1 + a_2 * x + a_3 * x^2. forget () specify y ~ x ^ 2 + x, R compute y ~ x * x + x. x * x means interaction x , x. R automatically drops redundant variables x + x become x, meaning y ~ x ^ 2 + x specifies function y = a_1 + a_2 * x. ’s probably intended!, get confused model , can always use model_matrix() see exactly equation lm() fitting:Transformations useful can use approximate non-linear functions. ’ve taken calculus class, may heard Taylor’s theorem says can approximate smooth function infinite sum polynomials. means can use polynomial function get arbitrarily close smooth function fitting equation like y = a_1 + a_2 * x + a_3 * x^2 + a_4 * x ^ 3. Typing sequence hand tedious, R provides helper function: poly():However ’s one major problem using poly(): outside range data, polynomials rapidly shoot positive negative infinity. One safer alternative use natural spline, splines::ns().Let’s see looks like try approximate non-linear function:’m going fit five models data.Notice extrapolation outside range data clearly bad. downside approximating function polynomial. real problem every model: model can never tell behaviour true start extrapolating outside range data seen. must rely theory science.","code":"\ndf <- tribble(\n  ~y, ~x,\n   1,  1,\n   2,  2, \n   3,  3\n)\nmodel_matrix(df, y ~ x^2 + x)\n#> # A tibble: 3 x 2\n#>   `(Intercept)`     x\n#>           <dbl> <dbl>\n#> 1             1     1\n#> 2             1     2\n#> 3             1     3\nmodel_matrix(df, y ~ I(x^2) + x)\n#> # A tibble: 3 x 3\n#>   `(Intercept)` `I(x^2)`     x\n#>           <dbl>    <dbl> <dbl>\n#> 1             1        1     1\n#> 2             1        4     2\n#> 3             1        9     3\nmodel_matrix(df, y ~ poly(x, 2))\n#> # A tibble: 3 x 3\n#>   `(Intercept)` `poly(x, 2)1` `poly(x, 2)2`\n#>           <dbl>         <dbl>         <dbl>\n#> 1             1     -7.07e- 1         0.408\n#> 2             1     -7.85e-17        -0.816\n#> 3             1      7.07e- 1         0.408\nlibrary(splines)\nmodel_matrix(df, y ~ ns(x, 2))\n#> # A tibble: 3 x 3\n#>   `(Intercept)` `ns(x, 2)1` `ns(x, 2)2`\n#>           <dbl>       <dbl>       <dbl>\n#> 1             1       0           0    \n#> 2             1       0.566      -0.211\n#> 3             1       0.344       0.771\nsim5 <- tibble(\n  x = seq(0, 3.5 * pi, length = 50),\n  y = 4 * sin(x) + rnorm(length(x))\n)\n\nggplot(sim5, aes(x, y)) +\n  geom_point()\nmod1 <- lm(y ~ ns(x, 1), data = sim5)\nmod2 <- lm(y ~ ns(x, 2), data = sim5)\nmod3 <- lm(y ~ ns(x, 3), data = sim5)\nmod4 <- lm(y ~ ns(x, 4), data = sim5)\nmod5 <- lm(y ~ ns(x, 5), data = sim5)\n\ngrid <- sim5 %>% \n  data_grid(x = seq_range(x, n = 50, expand = 0.1)) %>% \n  gather_predictions(mod1, mod2, mod3, mod4, mod5, .pred = \"y\")\n\nggplot(sim5, aes(x, y)) + \n  geom_point() +\n  geom_line(data = grid, colour = \"red\") +\n  facet_wrap(~ model)"},{"path":"model-basics.html","id":"exercises-65","chapter":"23 Model basics","heading":"23.4.5 Exercises","text":"happens repeat analysis sim2 using model without\nintercept. happens model equation? happens \npredictions?happens repeat analysis sim2 using model without\nintercept. happens model equation? happens \npredictions?Use model_matrix() explore equations generated models\nfit sim3 sim4. * good shorthand interaction?Use model_matrix() explore equations generated models\nfit sim3 sim4. * good shorthand interaction?Using basic principles, convert formulas following two\nmodels functions. (Hint: start converting categorical variable\n0-1 variables.)\n\nmod1 <- lm(y ~ x1 + x2, data = sim3)\nmod2 <- lm(y ~ x1 * x2, data = sim3)Using basic principles, convert formulas following two\nmodels functions. (Hint: start converting categorical variable\n0-1 variables.)sim4, mod1 mod2 better? think mod2 \nslightly better job removing patterns, ’s pretty subtle. Can \ncome plot support claim?sim4, mod1 mod2 better? think mod2 \nslightly better job removing patterns, ’s pretty subtle. Can \ncome plot support claim?","code":"\nmod1 <- lm(y ~ x1 + x2, data = sim3)\nmod2 <- lm(y ~ x1 * x2, data = sim3)"},{"path":"model-basics.html","id":"missing-values-5","chapter":"23 Model basics","heading":"23.5 Missing values","text":"Missing values obviously can convey information relationship variables, modelling functions drop rows contain missing values. R’s default behaviour silently drop , options(na.action = na.warn) (run prerequisites), makes sure get warning.suppress warning, set na.action = na.exclude:can always see exactly many observations used nobs():","code":"\ndf <- tribble(\n  ~x, ~y,\n  1, 2.2,\n  2, NA,\n  3, 3.5,\n  4, 8.3,\n  NA, 10\n)\n\nmod <- lm(y ~ x, data = df)\n#> Warning: Dropping 2 rows with missing values\nmod <- lm(y ~ x, data = df, na.action = na.exclude)\nnobs(mod)\n#> [1] 3"},{"path":"model-basics.html","id":"other-model-families","chapter":"23 Model basics","heading":"23.6 Other model families","text":"chapter focussed exclusively class linear models, assume relationship form y = a_1 * x1 + a_2 * x2 + ... + a_n * xn. Linear models additionally assume residuals normal distribution, haven’t talked . large set model classes extend linear model various interesting ways. :Generalised linear models, e.g. stats::glm(). Linear models assume \nresponse continuous error normal distribution.\nGeneralised linear models extend linear models include non-continuous\nresponses (e.g. binary data counts). work defining distance\nmetric based statistical idea likelihood.Generalised linear models, e.g. stats::glm(). Linear models assume \nresponse continuous error normal distribution.\nGeneralised linear models extend linear models include non-continuous\nresponses (e.g. binary data counts). work defining distance\nmetric based statistical idea likelihood.Generalised additive models, e.g. mgcv::gam(), extend generalised\nlinear models incorporate arbitrary smooth functions. means can\nwrite formula like y ~ s(x) becomes equation like\ny = f(x) let gam() estimate function (subject \nsmoothness constraints make problem tractable).Generalised additive models, e.g. mgcv::gam(), extend generalised\nlinear models incorporate arbitrary smooth functions. means can\nwrite formula like y ~ s(x) becomes equation like\ny = f(x) let gam() estimate function (subject \nsmoothness constraints make problem tractable).Penalised linear models, e.g. glmnet::glmnet(), add penalty term \ndistance penalises complex models (defined distance\nparameter vector origin). tends make\nmodels generalise better new datasets population.Penalised linear models, e.g. glmnet::glmnet(), add penalty term \ndistance penalises complex models (defined distance\nparameter vector origin). tends make\nmodels generalise better new datasets population.Robust linear models, e.g. MASS::rlm(), tweak distance downweight\npoints far away. makes less sensitive presence\noutliers, cost quite good \noutliers.Robust linear models, e.g. MASS::rlm(), tweak distance downweight\npoints far away. makes less sensitive presence\noutliers, cost quite good \noutliers.Trees, e.g. rpart::rpart(), attack problem completely different\nway linear models. fit piece-wise constant model, splitting \ndata progressively smaller smaller pieces. Trees aren’t terribly\neffective , powerful used aggregate\nmodels like random forests (e.g. randomForest::randomForest()) \ngradient boosting machines (e.g. xgboost::xgboost.)Trees, e.g. rpart::rpart(), attack problem completely different\nway linear models. fit piece-wise constant model, splitting \ndata progressively smaller smaller pieces. Trees aren’t terribly\neffective , powerful used aggregate\nmodels like random forests (e.g. randomForest::randomForest()) \ngradient boosting machines (e.g. xgboost::xgboost.)models work similarly programming perspective. ’ve mastered linear models, find easy master mechanics model classes. skilled modeller mixture good general principles big toolbox techniques. Now ’ve learned general tools one useful class models, can go learn classes sources.","code":""},{"path":"model-building.html","id":"model-building","chapter":"24 Model building","heading":"24 Model building","text":"","code":""},{"path":"model-building.html","id":"introduction-16","chapter":"24 Model building","heading":"24.1 Introduction","text":"previous chapter learned linear models work, learned basic tools understanding model telling data. previous chapter focussed simulated datasets. chapter focus real data, showing can progressively build model aid understanding data.take advantage fact can think model partitioning data pattern residuals. ’ll find patterns visualisation, make concrete precise model. ’ll repeat process, replace old response variable residuals model. goal transition implicit knowledge data head explicit knowledge quantitative model. makes easier apply new domains, easier others use.large complex datasets lot work. certainly alternative approaches - machine learning approach simply focus predictive ability model. approaches tend produce black boxes: model really good job generating predictions, don’t know . totally reasonable approach, make hard apply real world knowledge model. , turn, makes difficult assess whether model continue work long-term, fundamentals change. real models, ’d expect use combination approach classic automated approach.’s challenge know stop. need figure model good enough, additional investment unlikely pay . particularly like quote reddit user Broseidon241:long time ago art class, teacher told “artist needs know\npiece done. can’t tweak something perfection - wrap .\ndon’t like , . Otherwise begin something new”. Later\nlife, heard “poor seamstress makes many mistakes. good seamstress\nworks hard correct mistakes. great seamstress isn’t afraid \nthrow garment start .”– Broseidon241, https://www.reddit.com/r/datascience/comments/4irajq","code":""},{"path":"model-building.html","id":"prerequisites-16","chapter":"24 Model building","heading":"24.1.1 Prerequisites","text":"’ll use tools previous chapter, add real datasets: diamonds ggplot2, flights nycflights13. ’ll also need lubridate order work date/times flights.","code":"\nlibrary(tidyverse)\nlibrary(modelr)\noptions(na.action = na.warn)\n\nlibrary(nycflights13)\nlibrary(lubridate)"},{"path":"model-building.html","id":"diamond-prices","chapter":"24 Model building","heading":"24.2 Why are low quality diamonds more expensive?","text":"previous chapters ’ve seen surprising relationship quality diamonds price: low quality diamonds (poor cuts, bad colours, inferior clarity) higher prices.Note worst diamond color J (slightly yellow), worst clarity I1 (inclusions visible naked eye).","code":"\nggplot(diamonds, aes(cut, price)) + geom_boxplot()\nggplot(diamonds, aes(color, price)) + geom_boxplot()\nggplot(diamonds, aes(clarity, price)) + geom_boxplot()"},{"path":"model-building.html","id":"price-and-carat","chapter":"24 Model building","heading":"24.2.1 Price and carat","text":"looks like lower quality diamonds higher prices important confounding variable: weight (carat) diamond. weight diamond single important factor determining price diamond, lower quality diamonds tend larger.can make easier see attributes diamond affect relative price fitting model separate effect carat. first, lets make couple tweaks diamonds dataset make easier work :Focus diamonds smaller 2.5 carats (99.7% data)Log-transform carat price variables.Together, changes make easier see relationship carat price:log-transformation particularly useful makes pattern linear, linear patterns easiest work . Let’s take next step remove strong linear pattern. first make pattern explicit fitting model:look model tells us data. Note back transform predictions, undoing log transformation, can overlay predictions raw data:tells us something interesting data. believe model, large diamonds much cheaper expected. probably diamond dataset costs $19,000.Now can look residuals, verifies ’ve successfully removed strong linear pattern:Importantly, can now re-motivating plots using residuals instead price.Now see relationship expect: quality diamond increases, relative price. interpret y axis, need think residuals telling us, scale . residual -1 indicates lprice 1 unit lower prediction based solely weight. \\(2^{-1}\\) 1/2, points value -1 half expected price, residuals value 1 twice predicted price.","code":"\nggplot(diamonds, aes(carat, price)) + \n  geom_hex(bins = 50)\ndiamonds2 <- diamonds %>% \n  filter(carat <= 2.5) %>% \n  mutate(lprice = log2(price), lcarat = log2(carat))\nggplot(diamonds2, aes(lcarat, lprice)) + \n  geom_hex(bins = 50)\nmod_diamond <- lm(lprice ~ lcarat, data = diamonds2)\ngrid <- diamonds2 %>% \n  data_grid(carat = seq_range(carat, 20)) %>% \n  mutate(lcarat = log2(carat)) %>% \n  add_predictions(mod_diamond, \"lprice\") %>% \n  mutate(price = 2 ^ lprice)\n\nggplot(diamonds2, aes(carat, price)) + \n  geom_hex(bins = 50) + \n  geom_line(data = grid, colour = \"red\", size = 1)\ndiamonds2 <- diamonds2 %>% \n  add_residuals(mod_diamond, \"lresid\")\n\nggplot(diamonds2, aes(lcarat, lresid)) + \n  geom_hex(bins = 50)\nggplot(diamonds2, aes(cut, lresid)) + geom_boxplot()\nggplot(diamonds2, aes(color, lresid)) + geom_boxplot()\nggplot(diamonds2, aes(clarity, lresid)) + geom_boxplot()"},{"path":"model-building.html","id":"a-more-complicated-model","chapter":"24 Model building","heading":"24.2.2 A more complicated model","text":"wanted , continue build model, moving effects ’ve observed model make explicit. example, include color, cut, clarity model also make explicit effect three categorical variables:model now includes four predictors, ’s getting harder visualise. Fortunately, ’re currently independent means can plot individually four plots. make process little easier, ’re going use .model argument data_grid:model needs variables haven’t explicitly supplied, data_grid() automatically fill “typical” value. continuous variables, uses median, categorical variables uses common value (values, ’s tie).plot indicates diamonds quite large residuals - remember residual 2 indicates diamond 4x price expected. ’s often useful look unusual values individually:Nothing really jumps , ’s probably worth spending time considering indicates problem model, errors data. mistakes data, opportunity buy diamonds priced low incorrectly.","code":"\nmod_diamond2 <- lm(lprice ~ lcarat + color + cut + clarity, data = diamonds2)\ngrid <- diamonds2 %>% \n  data_grid(cut, .model = mod_diamond2) %>% \n  add_predictions(mod_diamond2)\ngrid\n#> # A tibble: 5 x 5\n#>   cut       lcarat color clarity  pred\n#>   <ord>      <dbl> <chr> <chr>   <dbl>\n#> 1 Fair      -0.515 G     VS2      11.2\n#> 2 Good      -0.515 G     VS2      11.3\n#> 3 Very Good -0.515 G     VS2      11.4\n#> 4 Premium   -0.515 G     VS2      11.4\n#> 5 Ideal     -0.515 G     VS2      11.4\n\nggplot(grid, aes(cut, pred)) + \n  geom_point()\ndiamonds2 <- diamonds2 %>% \n  add_residuals(mod_diamond2, \"lresid2\")\n\nggplot(diamonds2, aes(lcarat, lresid2)) + \n  geom_hex(bins = 50)\ndiamonds2 %>% \n  filter(abs(lresid2) > 1) %>% \n  add_predictions(mod_diamond2) %>% \n  mutate(pred = round(2 ^ pred)) %>% \n  select(price, pred, carat:table, x:z) %>% \n  arrange(price)\n#> # A tibble: 16 x 11\n#>   price  pred carat cut     color clarity depth table     x     y     z\n#>   <int> <dbl> <dbl> <ord>   <ord> <ord>   <dbl> <dbl> <dbl> <dbl> <dbl>\n#> 1  1013   264  0.25 Fair    F     SI2      54.4    64  4.3   4.23  2.32\n#> 2  1186   284  0.25 Premium G     SI2      59      60  5.33  5.28  3.12\n#> 3  1186   284  0.25 Premium G     SI2      58.8    60  5.33  5.28  3.12\n#> 4  1262  2644  1.03 Fair    E     I1       78.2    54  5.72  5.59  4.42\n#> 5  1415   639  0.35 Fair    G     VS2      65.9    54  5.57  5.53  3.66\n#> 6  1415   639  0.35 Fair    G     VS2      65.9    54  5.57  5.53  3.66\n#> # … with 10 more rows"},{"path":"model-building.html","id":"exercises-66","chapter":"24 Model building","heading":"24.2.3 Exercises","text":"plot lcarat vs. lprice, bright vertical\nstrips. represent?plot lcarat vs. lprice, bright vertical\nstrips. represent?log(price) = a_0 + a_1 * log(carat), say \nrelationship price carat?log(price) = a_0 + a_1 * log(carat), say \nrelationship price carat?Extract diamonds high low residuals.\nanything unusual diamonds? particularly bad\ngood, think pricing errors?Extract diamonds high low residuals.\nanything unusual diamonds? particularly bad\ngood, think pricing errors?final model, mod_diamond2, good job predicting\ndiamond prices? trust tell much spend\nbuying diamond?final model, mod_diamond2, good job predicting\ndiamond prices? trust tell much spend\nbuying diamond?","code":""},{"path":"model-building.html","id":"what-affects-the-number-of-daily-flights","chapter":"24 Model building","heading":"24.3 What affects the number of daily flights?","text":"Let’s work similar process dataset seems even simpler first glance: number flights leave NYC per day. really small dataset — 365 rows 2 columns — ’re going end fully realised model, ’ll see, steps along way help us better understand data. Let’s get started counting number flights per day visualising ggplot2.","code":"\ndaily <- flights %>% \n  mutate(date = make_date(year, month, day)) %>% \n  group_by(date) %>% \n  summarise(n = n())\ndaily\n#> # A tibble: 365 x 2\n#>   date           n\n#> * <date>     <int>\n#> 1 2013-01-01   842\n#> 2 2013-01-02   943\n#> 3 2013-01-03   914\n#> 4 2013-01-04   915\n#> 5 2013-01-05   720\n#> 6 2013-01-06   832\n#> # … with 359 more rows\n\nggplot(daily, aes(date, n)) + \n  geom_line()"},{"path":"model-building.html","id":"day-of-week","chapter":"24 Model building","heading":"24.3.1 Day of week","text":"Understanding long-term trend challenging ’s strong day--week effect dominates subtler patterns. Let’s start looking distribution flight numbers day--week:fewer flights weekends travel business. effect particularly pronounced Saturday: might sometimes leave Sunday Monday morning meeting, ’s rare ’d leave Saturday ’d much rather home family.One way remove strong pattern use model. First, fit model, display predictions overlaid original data:Next compute visualise residuals:Note change y-axis: now seeing deviation expected number flights, given day week. plot useful now ’ve removed much large day--week effect, can see subtler patterns remain:model seems fail starting June: can still see strong\nregular pattern model hasn’t captured. Drawing plot one\nline day week makes cause easier see:\n\nggplot(daily, aes(date, resid, colour = wday)) + \n  geom_ref_line(h = 0) + \n  geom_line()\n\nmodel fails accurately predict number flights Saturday:\nsummer flights expect, fall \nfewer. ’ll see can better capture pattern \nnext section.model seems fail starting June: can still see strong\nregular pattern model hasn’t captured. Drawing plot one\nline day week makes cause easier see:model fails accurately predict number flights Saturday:\nsummer flights expect, fall \nfewer. ’ll see can better capture pattern \nnext section.days far fewer flights expected:\n\ndaily %>% \n  filter(resid < -100)\n#> # tibble: 11 x 4\n#>   date           n wday  resid\n#>   <date>     <int> <ord> <dbl>\n#> 1 2013-01-01   842 Tue   -109.\n#> 2 2013-01-20   786 Sun   -105.\n#> 3 2013-05-26   729 Sun   -162.\n#> 4 2013-07-04   737 Thu   -229.\n#> 5 2013-07-05   822 Fri   -145.\n#> 6 2013-09-01   718 Sun   -173.\n#> # … 5 rows\n’re familiar American public holidays, might spot New Year’s\nday, July 4th, Thanksgiving Christmas. others don’t\nseem correspond public holidays. ’ll work one\nexercises.days far fewer flights expected:’re familiar American public holidays, might spot New Year’s\nday, July 4th, Thanksgiving Christmas. others don’t\nseem correspond public holidays. ’ll work one\nexercises.seems smoother long term trend course year.\ncan highlight trend geom_smooth():\n\ndaily %>% \n  ggplot(aes(date, resid)) + \n  geom_ref_line(h = 0) + \n  geom_line(colour = \"grey50\") + \n  geom_smooth(se = FALSE, span = 0.20)\n#> `geom_smooth()` using method = 'loess' formula 'y ~ x'\n\nfewer flights January (December), summer\n(May-Sep). can’t much pattern quantitatively, \nsingle year data. can use domain knowledge \nbrainstorm potential explanations.seems smoother long term trend course year.\ncan highlight trend geom_smooth():fewer flights January (December), summer\n(May-Sep). can’t much pattern quantitatively, \nsingle year data. can use domain knowledge \nbrainstorm potential explanations.","code":"\ndaily <- daily %>% \n  mutate(wday = wday(date, label = TRUE))\nggplot(daily, aes(wday, n)) + \n  geom_boxplot()\nmod <- lm(n ~ wday, data = daily)\n\ngrid <- daily %>% \n  data_grid(wday) %>% \n  add_predictions(mod, \"n\")\n\nggplot(daily, aes(wday, n)) + \n  geom_boxplot() +\n  geom_point(data = grid, colour = \"red\", size = 4)\ndaily <- daily %>% \n  add_residuals(mod)\ndaily %>% \n  ggplot(aes(date, resid)) + \n  geom_ref_line(h = 0) + \n  geom_line()\nggplot(daily, aes(date, resid, colour = wday)) + \n  geom_ref_line(h = 0) + \n  geom_line()\ndaily %>% \n  filter(resid < -100)\n#> # A tibble: 11 x 4\n#>   date           n wday  resid\n#>   <date>     <int> <ord> <dbl>\n#> 1 2013-01-01   842 Tue   -109.\n#> 2 2013-01-20   786 Sun   -105.\n#> 3 2013-05-26   729 Sun   -162.\n#> 4 2013-07-04   737 Thu   -229.\n#> 5 2013-07-05   822 Fri   -145.\n#> 6 2013-09-01   718 Sun   -173.\n#> # … with 5 more rows\ndaily %>% \n  ggplot(aes(date, resid)) + \n  geom_ref_line(h = 0) + \n  geom_line(colour = \"grey50\") + \n  geom_smooth(se = FALSE, span = 0.20)\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'"},{"path":"model-building.html","id":"seasonal-saturday-effect","chapter":"24 Model building","heading":"24.3.2 Seasonal Saturday effect","text":"Let’s first tackle failure accurately predict number flights Saturday. good place start go back raw numbers, focussing Saturdays:(’ve used points lines make clear data interpolation.)suspect pattern caused summer holidays: many people go holiday summer, people don’t mind travelling Saturdays vacation. Looking plot, might guess summer holidays early June late August. seems line fairly well state’s school terms: summer break 2013 Jun 26–Sep 9.Saturday flights spring fall? asked American friends suggested ’s less common plan family vacations fall big Thanksgiving Christmas holidays. don’t data know sure, seems like plausible working hypothesis.Lets create “term” variable roughly captures three school terms, check work plot:(manually tweaked dates get nice breaks plot. Using visualisation help understand function really powerful general technique.)’s useful see new variable affects days week:looks like significant variation across terms, fitting separate day week effect term reasonable. improves model, much might hope:can see problem overlaying predictions model raw data:model finding mean effect, lot big outliers, mean tends far away typical value. can alleviate problem using model robust effect outliers: MASS::rlm(). greatly reduces impact outliers estimates, gives model good job removing day week pattern:’s now much easier see long-term trend, positive negative outliers.","code":"\ndaily %>% \n  filter(wday == \"Sat\") %>% \n  ggplot(aes(date, n)) + \n    geom_point() + \n    geom_line() +\n    scale_x_date(NULL, date_breaks = \"1 month\", date_labels = \"%b\")\nterm <- function(date) {\n  cut(date, \n    breaks = ymd(20130101, 20130605, 20130825, 20140101),\n    labels = c(\"spring\", \"summer\", \"fall\") \n  )\n}\n\ndaily <- daily %>% \n  mutate(term = term(date)) \n\ndaily %>% \n  filter(wday == \"Sat\") %>% \n  ggplot(aes(date, n, colour = term)) +\n  geom_point(alpha = 1/3) + \n  geom_line() +\n  scale_x_date(NULL, date_breaks = \"1 month\", date_labels = \"%b\")\ndaily %>% \n  ggplot(aes(wday, n, colour = term)) +\n    geom_boxplot()\nmod1 <- lm(n ~ wday, data = daily)\nmod2 <- lm(n ~ wday * term, data = daily)\n\ndaily %>% \n  gather_residuals(without_term = mod1, with_term = mod2) %>% \n  ggplot(aes(date, resid, colour = model)) +\n    geom_line(alpha = 0.75)\ngrid <- daily %>% \n  data_grid(wday, term) %>% \n  add_predictions(mod2, \"n\")\n\nggplot(daily, aes(wday, n)) +\n  geom_boxplot() + \n  geom_point(data = grid, colour = \"red\") + \n  facet_wrap(~ term)\nmod3 <- MASS::rlm(n ~ wday * term, data = daily)\n\ndaily %>% \n  add_residuals(mod3, \"resid\") %>% \n  ggplot(aes(date, resid)) + \n  geom_hline(yintercept = 0, size = 2, colour = \"white\") + \n  geom_line()"},{"path":"model-building.html","id":"computed-variables","chapter":"24 Model building","heading":"24.3.3 Computed variables","text":"’re experimenting many models many visualisations, ’s good idea bundle creation variables function ’s chance accidentally applying different transformation different places. example, write:Another option put transformations directly model formula:Either approach reasonable. Making transformed variable explicit useful want check work, use visualisation. can’t easily use transformations (like splines) return multiple columns. Including transformations model function makes life little easier ’re working many different datasets model self contained.","code":"\ncompute_vars <- function(data) {\n  data %>% \n    mutate(\n      term = term(date), \n      wday = wday(date, label = TRUE)\n    )\n}\nwday2 <- function(x) wday(x, label = TRUE)\nmod3 <- lm(n ~ wday2(date) * term(date), data = daily)"},{"path":"model-building.html","id":"time-of-year-an-alternative-approach","chapter":"24 Model building","heading":"24.3.4 Time of year: an alternative approach","text":"previous section used domain knowledge (US school term affects travel) improve model. alternative using knowledge explicitly model give data room speak. use flexible model allow capture pattern ’re interested . simple linear trend isn’t adequate, try using natural spline fit smooth curve across year:see strong pattern numbers Saturday flights. reassuring, also saw pattern raw data. ’s good sign get signal different approaches.","code":"\nlibrary(splines)\nmod <- MASS::rlm(n ~ wday * ns(date, 5), data = daily)\n\ndaily %>% \n  data_grid(wday, date = seq_range(date, n = 13)) %>% \n  add_predictions(mod) %>% \n  ggplot(aes(date, pred, colour = wday)) + \n    geom_line() +\n    geom_point()"},{"path":"model-building.html","id":"exercises-67","chapter":"24 Model building","heading":"24.3.5 Exercises","text":"Use Google sleuthing skills brainstorm fewer \nexpected flights Jan 20, May 26, Sep 1. (Hint: \nexplanation.) days generalise another year?Use Google sleuthing skills brainstorm fewer \nexpected flights Jan 20, May 26, Sep 1. (Hint: \nexplanation.) days generalise another year?three days high positive residuals represent?\ndays generalise another year?\n\ndaily %>% \n  slice_max(n = 3, resid)\n#> # tibble: 3 x 5\n#>   date           n wday  resid term \n#>   <date>     <int> <ord> <dbl> <fct>\n#> 1 2013-11-30   857 Sat   112.  fall \n#> 2 2013-12-01   987 Sun    95.5 fall \n#> 3 2013-12-28   814 Sat    69.4 fallWhat three days high positive residuals represent?\ndays generalise another year?Create new variable splits wday variable terms, \nSaturdays, .e. Thurs, Fri, Sat-summer,\nSat-spring, Sat-fall. model compare model \nevery combination wday term?Create new variable splits wday variable terms, \nSaturdays, .e. Thurs, Fri, Sat-summer,\nSat-spring, Sat-fall. model compare model \nevery combination wday term?Create new wday variable combines day week, term\n(Saturdays), public holidays. residuals \nmodel look like?Create new wday variable combines day week, term\n(Saturdays), public holidays. residuals \nmodel look like?happens fit day week effect varies month\n(.e. n ~ wday * month)? helpful?happens fit day week effect varies month\n(.e. n ~ wday * month)? helpful?expect model n ~ wday + ns(date, 5) look like?\nKnowing know data, expect \nparticularly effective?expect model n ~ wday + ns(date, 5) look like?\nKnowing know data, expect \nparticularly effective?hypothesised people leaving Sundays likely \nbusiness travellers need somewhere Monday. Explore \nhypothesis seeing breaks based distance time: \n’s true, ’d expect see Sunday evening flights places \nfar away.hypothesised people leaving Sundays likely \nbusiness travellers need somewhere Monday. Explore \nhypothesis seeing breaks based distance time: \n’s true, ’d expect see Sunday evening flights places \nfar away.’s little frustrating Sunday Saturday separate ends\nplot. Write small function set levels \nfactor week starts Monday.’s little frustrating Sunday Saturday separate ends\nplot. Write small function set levels \nfactor week starts Monday.","code":"\ndaily %>% \n  slice_max(n = 3, resid)\n#> # A tibble: 3 x 5\n#>   date           n wday  resid term \n#>   <date>     <int> <ord> <dbl> <fct>\n#> 1 2013-11-30   857 Sat   112.  fall \n#> 2 2013-12-01   987 Sun    95.5 fall \n#> 3 2013-12-28   814 Sat    69.4 fall"},{"path":"model-building.html","id":"learning-more-about-models","chapter":"24 Model building","heading":"24.4 Learning more about models","text":"scratched absolute surface modelling, hopefully gained simple, general-purpose tools can use improve data analyses. ’s OK start simple! ’ve seen, even simple models can make dramatic difference ability tease interactions variables.modelling chapters even opinionated rest book. approach modelling somewhat different perspective others, relatively little space devoted . Modelling really deserves book , ’d highly recommend read least one three books:Statistical Modeling: Fresh Approach Danny Kaplan,\nhttp://project-mosaic-books.com/?page_id=13. book provides\ngentle introduction modelling, build intuition,\nmathematical tools, R skills parallel. book replaces traditional\n“introduction statistics” course, providing curriculum --date\nrelevant data science.Statistical Modeling: Fresh Approach Danny Kaplan,\nhttp://project-mosaic-books.com/?page_id=13. book provides\ngentle introduction modelling, build intuition,\nmathematical tools, R skills parallel. book replaces traditional\n“introduction statistics” course, providing curriculum --date\nrelevant data science.Introduction Statistical Learning Gareth James, Daniela Witten,\nTrevor Hastie, Robert Tibshirani, http://www-bcf.usc.edu/~gareth/ISL/\n(available online free). book presents family modern modelling\ntechniques collectively known statistical learning. even deeper\nunderstanding math behind models, read classic\nElements Statistical Learning Trevor Hastie, Robert Tibshirani, \nJerome Friedman, https://web.stanford.edu/~hastie/Papers/ESLII.pdf (also\navailable online free).Introduction Statistical Learning Gareth James, Daniela Witten,\nTrevor Hastie, Robert Tibshirani, http://www-bcf.usc.edu/~gareth/ISL/\n(available online free). book presents family modern modelling\ntechniques collectively known statistical learning. even deeper\nunderstanding math behind models, read classic\nElements Statistical Learning Trevor Hastie, Robert Tibshirani, \nJerome Friedman, https://web.stanford.edu/~hastie/Papers/ESLII.pdf (also\navailable online free).Applied Predictive Modeling Max Kuhn Kjell Johnson,\nhttp://appliedpredictivemodeling.com. book companion \ncaret package provides practical tools dealing real-life\npredictive modelling challenges.Applied Predictive Modeling Max Kuhn Kjell Johnson,\nhttp://appliedpredictivemodeling.com. book companion \ncaret package provides practical tools dealing real-life\npredictive modelling challenges.","code":""},{"path":"many-models.html","id":"many-models","chapter":"25 Many models","heading":"25 Many models","text":"","code":""},{"path":"many-models.html","id":"introduction-17","chapter":"25 Many models","heading":"25.1 Introduction","text":"chapter ’re going learn three powerful ideas help work large numbers models ease:Using many simple models better understand complex datasets.Using many simple models better understand complex datasets.Using list-columns store arbitrary data structures data frame.\nexample, allow column contains linear\nmodels.Using list-columns store arbitrary data structures data frame.\nexample, allow column contains linear\nmodels.Using broom package, David Robinson, turn models tidy\ndata. powerful technique working large numbers models\ntidy data, can apply techniques \n’ve learned earlier book.Using broom package, David Robinson, turn models tidy\ndata. powerful technique working large numbers models\ntidy data, can apply techniques \n’ve learned earlier book.’ll start diving motivating example using data life expectancy around world. ’s small dataset illustrates important modelling can improving visualisations. ’ll use large number simple models partition strongest signals can see subtler signals remain. ’ll also see model summaries can help us pick outliers unusual trends.following sections dive detail individual techniques:list-columns, ’ll learn list-column data structure,\n’s valid put lists data frames.list-columns, ’ll learn list-column data structure,\n’s valid put lists data frames.creating list-columns, ’ll learn three main ways ’ll\ncreate list-columns.creating list-columns, ’ll learn three main ways ’ll\ncreate list-columns.simplifying list-columns ’ll learn convert list-columns back\nregular atomic vectors (sets atomic vectors) can work\neasily.simplifying list-columns ’ll learn convert list-columns back\nregular atomic vectors (sets atomic vectors) can work\neasily.making tidy data broom, ’ll learn full set tools\nprovided broom, see can applied types \ndata structure.making tidy data broom, ’ll learn full set tools\nprovided broom, see can applied types \ndata structure.chapter somewhat aspirational: book first introduction R, chapter likely struggle. requires deeply internalised ideas modelling, data structures, iteration. don’t worry don’t get — just put chapter aside months, come back want stretch brain.","code":""},{"path":"many-models.html","id":"prerequisites-17","chapter":"25 Many models","heading":"25.1.1 Prerequisites","text":"Working many models requires many packages tidyverse (data exploration, wrangling, programming) modelr facilitate modelling.","code":"\nlibrary(modelr)\nlibrary(tidyverse)"},{"path":"many-models.html","id":"gapminder","chapter":"25 Many models","heading":"25.2 gapminder","text":"motivate power many simple models, ’re going look “gapminder” data. data popularised Hans Rosling, Swedish doctor statistician. ’ve never heard , stop reading chapter right now go watch one videos! fantastic data presenter illustrates can use data present compelling story. good place start short video filmed conjunction BBC: https://www.youtube.com/watch?v=jbkSRLYSojo.gapminder data summarises progression countries time, looking statistics like life expectancy GDP. data easy access R, thanks Jenny Bryan created gapminder package:case study, ’re going focus just three variables answer question “life expectancy (lifeExp) change time (year) country (country)?”. good place start plot:small dataset: ~1,700 observations 3 variables. ’s still hard see ’s going ! Overall, looks like life expectancy steadily improving. However, look closely, might notice countries don’t follow pattern. can make countries easier see?One way use approach last chapter: ’s strong signal (overall linear growth) makes hard see subtler trends. ’ll tease factors apart fitting model linear trend. model captures steady growth time, residuals show ’s left.already know single country:can easily fit model every country?","code":"\nlibrary(gapminder)\ngapminder\n#> # A tibble: 1,704 x 6\n#>   country     continent  year lifeExp      pop gdpPercap\n#>   <fct>       <fct>     <int>   <dbl>    <int>     <dbl>\n#> 1 Afghanistan Asia       1952    28.8  8425333      779.\n#> 2 Afghanistan Asia       1957    30.3  9240934      821.\n#> 3 Afghanistan Asia       1962    32.0 10267083      853.\n#> 4 Afghanistan Asia       1967    34.0 11537966      836.\n#> 5 Afghanistan Asia       1972    36.1 13079460      740.\n#> 6 Afghanistan Asia       1977    38.4 14880372      786.\n#> # … with 1,698 more rows\ngapminder %>% \n  ggplot(aes(year, lifeExp, group = country)) +\n    geom_line(alpha = 1/3)\nnz <- filter(gapminder, country == \"New Zealand\")\nnz %>% \n  ggplot(aes(year, lifeExp)) + \n  geom_line() + \n  ggtitle(\"Full data = \")\n\nnz_mod <- lm(lifeExp ~ year, data = nz)\nnz %>% \n  add_predictions(nz_mod) %>%\n  ggplot(aes(year, pred)) + \n  geom_line() + \n  ggtitle(\"Linear trend + \")\n\nnz %>% \n  add_residuals(nz_mod) %>% \n  ggplot(aes(year, resid)) + \n  geom_hline(yintercept = 0, colour = \"white\", size = 3) + \n  geom_line() + \n  ggtitle(\"Remaining pattern\")"},{"path":"many-models.html","id":"nested-data","chapter":"25 Many models","heading":"25.2.1 Nested data","text":"imagine copy pasting code multiple times; ’ve already learned better way! Extract common code function repeat using map function purrr. problem structured little differently ’ve seen . Instead repeating action variable, want repeat action country, subset rows. , need new data structure: nested data frame. create nested data frame start grouped data frame, “nest” :(’m cheating little grouping continent country. Given country, continent fixed, doesn’t add groups, ’s easy way carry extra variable along ride.)creates data frame one row per group (per country), rather unusual column: data. data list data frames (tibbles, precise). seems like crazy idea: data frame column list data frames! ’ll explain shortly think good idea.data column little tricky look ’s moderately complicated list, ’re still working good tools explore objects. Unfortunately using str() recommended often produce long output. pluck single element data column ’ll see contains data country (case, Afghanistan).Note difference standard grouped data frame nested data frame: grouped data frame, row observation; nested data frame, row group. Another way think nested dataset now meta-observation: row represents complete time course country, rather single point time.","code":"\nby_country <- gapminder %>% \n  group_by(country, continent) %>% \n  nest()\n\nby_country\n#> # A tibble: 142 x 3\n#> # Groups:   country, continent [142]\n#>   country     continent data             \n#>   <fct>       <fct>     <list>           \n#> 1 Afghanistan Asia      <tibble [12 × 4]>\n#> 2 Albania     Europe    <tibble [12 × 4]>\n#> 3 Algeria     Africa    <tibble [12 × 4]>\n#> 4 Angola      Africa    <tibble [12 × 4]>\n#> 5 Argentina   Americas  <tibble [12 × 4]>\n#> 6 Australia   Oceania   <tibble [12 × 4]>\n#> # … with 136 more rows\nby_country$data[[1]]\n#> # A tibble: 12 x 4\n#>    year lifeExp      pop gdpPercap\n#>   <int>   <dbl>    <int>     <dbl>\n#> 1  1952    28.8  8425333      779.\n#> 2  1957    30.3  9240934      821.\n#> 3  1962    32.0 10267083      853.\n#> 4  1967    34.0 11537966      836.\n#> 5  1972    36.1 13079460      740.\n#> 6  1977    38.4 14880372      786.\n#> # … with 6 more rows"},{"path":"many-models.html","id":"list-columns","chapter":"25 Many models","heading":"25.2.2 List-columns","text":"Now nested data frame, ’re good position fit models. model-fitting function:want apply every data frame. data frames list, can use purrr::map() apply country_model element:However, rather leaving list models free-floating object, think ’s better store column by_country data frame. Storing related objects columns key part value data frames, think list-columns good idea. course working countries, going lots lists one element per country. store together one data frame?words, instead creating new object global environment, ’re going create new variable by_country data frame. ’s job dplyr::mutate():big advantage: related objects stored together, don’t need manually keep sync filter arrange. semantics data frame takes care :list data frames list models separate objects, remember whenever re-order subset one vector, need re-order subset others order keep sync. forget, code continue work, give wrong answer!","code":"\ncountry_model <- function(df) {\n  lm(lifeExp ~ year, data = df)\n}\nmodels <- map(by_country$data, country_model)\nby_country <- by_country %>% \n  mutate(model = map(data, country_model))\nby_country\n#> # A tibble: 142 x 4\n#> # Groups:   country, continent [142]\n#>   country     continent data              model \n#>   <fct>       <fct>     <list>            <list>\n#> 1 Afghanistan Asia      <tibble [12 × 4]> <lm>  \n#> 2 Albania     Europe    <tibble [12 × 4]> <lm>  \n#> 3 Algeria     Africa    <tibble [12 × 4]> <lm>  \n#> 4 Angola      Africa    <tibble [12 × 4]> <lm>  \n#> 5 Argentina   Americas  <tibble [12 × 4]> <lm>  \n#> 6 Australia   Oceania   <tibble [12 × 4]> <lm>  \n#> # … with 136 more rows\nby_country %>% \n  filter(continent == \"Europe\")\n#> # A tibble: 30 x 4\n#> # Groups:   country, continent [30]\n#>   country                continent data              model \n#>   <fct>                  <fct>     <list>            <list>\n#> 1 Albania                Europe    <tibble [12 × 4]> <lm>  \n#> 2 Austria                Europe    <tibble [12 × 4]> <lm>  \n#> 3 Belgium                Europe    <tibble [12 × 4]> <lm>  \n#> 4 Bosnia and Herzegovina Europe    <tibble [12 × 4]> <lm>  \n#> 5 Bulgaria               Europe    <tibble [12 × 4]> <lm>  \n#> 6 Croatia                Europe    <tibble [12 × 4]> <lm>  \n#> # … with 24 more rows\nby_country %>% \n  arrange(continent, country)\n#> # A tibble: 142 x 4\n#> # Groups:   country, continent [142]\n#>   country      continent data              model \n#>   <fct>        <fct>     <list>            <list>\n#> 1 Algeria      Africa    <tibble [12 × 4]> <lm>  \n#> 2 Angola       Africa    <tibble [12 × 4]> <lm>  \n#> 3 Benin        Africa    <tibble [12 × 4]> <lm>  \n#> 4 Botswana     Africa    <tibble [12 × 4]> <lm>  \n#> 5 Burkina Faso Africa    <tibble [12 × 4]> <lm>  \n#> 6 Burundi      Africa    <tibble [12 × 4]> <lm>  \n#> # … with 136 more rows"},{"path":"many-models.html","id":"unnesting","chapter":"25 Many models","heading":"25.2.3 Unnesting","text":"Previously computed residuals single model single dataset. Now 142 data frames 142 models. compute residuals, need call add_residuals() model-data pair:can plot list data frames? Instead struggling answer question, let’s turn list data frames back regular data frame. Previously used nest() turn regular data frame nested data frame, now opposite unnest():Note regular column repeated row nested tibble.Now regular data frame, can plot residuals:Facetting continent particularly revealing:looks like ’ve missed mild patterns. ’s also something interesting going Africa: see large residuals suggests model isn’t fitting well . ’ll explore next section, attacking slightly different angle.","code":"\nby_country <- by_country %>% \n  mutate(\n    resids = map2(data, model, add_residuals)\n  )\nby_country\n#> # A tibble: 142 x 5\n#> # Groups:   country, continent [142]\n#>   country     continent data              model  resids           \n#>   <fct>       <fct>     <list>            <list> <list>           \n#> 1 Afghanistan Asia      <tibble [12 × 4]> <lm>   <tibble [12 × 5]>\n#> 2 Albania     Europe    <tibble [12 × 4]> <lm>   <tibble [12 × 5]>\n#> 3 Algeria     Africa    <tibble [12 × 4]> <lm>   <tibble [12 × 5]>\n#> 4 Angola      Africa    <tibble [12 × 4]> <lm>   <tibble [12 × 5]>\n#> 5 Argentina   Americas  <tibble [12 × 4]> <lm>   <tibble [12 × 5]>\n#> 6 Australia   Oceania   <tibble [12 × 4]> <lm>   <tibble [12 × 5]>\n#> # … with 136 more rows\nresids <- unnest(by_country, resids)\nresids\n#> # A tibble: 1,704 x 9\n#> # Groups:   country, continent [142]\n#>   country    continent data        model  year lifeExp     pop gdpPercap   resid\n#>   <fct>      <fct>     <list>      <lis> <int>   <dbl>   <int>     <dbl>   <dbl>\n#> 1 Afghanist… Asia      <tibble [1… <lm>   1952    28.8  8.43e6      779. -1.11  \n#> 2 Afghanist… Asia      <tibble [1… <lm>   1957    30.3  9.24e6      821. -0.952 \n#> 3 Afghanist… Asia      <tibble [1… <lm>   1962    32.0  1.03e7      853. -0.664 \n#> 4 Afghanist… Asia      <tibble [1… <lm>   1967    34.0  1.15e7      836. -0.0172\n#> 5 Afghanist… Asia      <tibble [1… <lm>   1972    36.1  1.31e7      740.  0.674 \n#> 6 Afghanist… Asia      <tibble [1… <lm>   1977    38.4  1.49e7      786.  1.65  \n#> # … with 1,698 more rows\nresids %>% \n  ggplot(aes(year, resid)) +\n    geom_line(aes(group = country), alpha = 1 / 3) + \n    geom_smooth(se = FALSE)\n#> `geom_smooth()` using method = 'gam' and formula 'y ~ s(x, bs = \"cs\")'\nresids %>% \n  ggplot(aes(year, resid, group = country)) +\n    geom_line(alpha = 1 / 3) + \n    facet_wrap(~continent)"},{"path":"many-models.html","id":"model-quality","chapter":"25 Many models","heading":"25.2.4 Model quality","text":"Instead looking residuals model, look general measurements model quality. learned compute specific measures previous chapter. ’ll show different approach using broom package. broom package provides general set functions turn models tidy data. ’ll use broom::glance() extract model quality metrics. apply model, get data frame single row:can use mutate() unnest() create data frame row country:(Pay attention variables aren’t printed: ’s lot useful stuff .)data frame hand, can start look models don’t fit well:worst models appear Africa. Let’s double check plot. relatively small number observations discrete variable, geom_jitter() effective:pull countries particularly bad \\(R^2\\) plot data:see two main effects : tragedies HIV/AIDS epidemic Rwandan genocide.","code":"\nbroom::glance(nz_mod)\n#> # A tibble: 1 x 12\n#>   r.squared adj.r.squared sigma statistic p.value    df logLik   AIC   BIC\n#>       <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl> <dbl> <dbl>\n#> 1     0.954         0.949 0.804      205. 5.41e-8     1  -13.3  32.6  34.1\n#> # … with 3 more variables: deviance <dbl>, df.residual <int>, nobs <int>\nglance <- by_country %>% \n  mutate(glance = map(model, broom::glance)) %>% \n  select(country, continent, glance) %>% \n  unnest(glance)\nglance\n#> # A tibble: 142 x 14\n#> # Groups:   country, continent [142]\n#>   country continent r.squared adj.r.squared sigma statistic  p.value    df\n#>   <fct>   <fct>         <dbl>         <dbl> <dbl>     <dbl>    <dbl> <dbl>\n#> 1 Afghan… Asia          0.948         0.942 1.22      181.  9.84e- 8     1\n#> 2 Albania Europe        0.911         0.902 1.98      102.  1.46e- 6     1\n#> 3 Algeria Africa        0.985         0.984 1.32      662.  1.81e-10     1\n#> 4 Angola  Africa        0.888         0.877 1.41       79.1 4.59e- 6     1\n#> 5 Argent… Americas      0.996         0.995 0.292    2246.  4.22e-13     1\n#> 6 Austra… Oceania       0.980         0.978 0.621     481.  8.67e-10     1\n#> # … with 136 more rows, and 6 more variables: logLik <dbl>, AIC <dbl>,\n#> #   BIC <dbl>, deviance <dbl>, df.residual <int>, nobs <int>\nglance %>% \n  arrange(r.squared)\n#> # A tibble: 142 x 14\n#> # Groups:   country, continent [142]\n#>   country continent r.squared adj.r.squared sigma statistic p.value    df logLik\n#>   <fct>   <fct>         <dbl>         <dbl> <dbl>     <dbl>   <dbl> <dbl>  <dbl>\n#> 1 Rwanda  Africa       0.0172      -0.0811   6.56     0.175   0.685     1  -38.5\n#> 2 Botswa… Africa       0.0340      -0.0626   6.11     0.352   0.566     1  -37.7\n#> 3 Zimbab… Africa       0.0562      -0.0381   7.21     0.596   0.458     1  -39.6\n#> 4 Zambia  Africa       0.0598      -0.0342   4.53     0.636   0.444     1  -34.1\n#> 5 Swazil… Africa       0.0682      -0.0250   6.64     0.732   0.412     1  -38.7\n#> 6 Lesotho Africa       0.0849      -0.00666  5.93     0.927   0.358     1  -37.3\n#> # … with 136 more rows, and 5 more variables: AIC <dbl>, BIC <dbl>,\n#> #   deviance <dbl>, df.residual <int>, nobs <int>\nglance %>% \n  ggplot(aes(continent, r.squared)) + \n    geom_jitter(width = 0.5)\nbad_fit <- filter(glance, r.squared < 0.25)\n\ngapminder %>% \n  semi_join(bad_fit, by = \"country\") %>% \n  ggplot(aes(year, lifeExp, colour = country)) +\n    geom_line()"},{"path":"many-models.html","id":"exercises-68","chapter":"25 Many models","heading":"25.2.5 Exercises","text":"linear trend seems slightly simple overall trend.\nCan better quadratic polynomial? can interpret\ncoefficients quadratic? (Hint might want transform\nyear mean zero.)linear trend seems slightly simple overall trend.\nCan better quadratic polynomial? can interpret\ncoefficients quadratic? (Hint might want transform\nyear mean zero.)Explore methods visualising distribution \\(R^2\\) per\ncontinent. might want try ggbeeswarm package, provides\nsimilar methods avoiding overlaps jitter, uses deterministic\nmethods.Explore methods visualising distribution \\(R^2\\) per\ncontinent. might want try ggbeeswarm package, provides\nsimilar methods avoiding overlaps jitter, uses deterministic\nmethods.create last plot (showing data countries \nworst model fits), needed two steps: created data frame \none row per country semi-joined original dataset.\n’s possible avoid join use unnest() instead \nunnest(.drop = TRUE). ?create last plot (showing data countries \nworst model fits), needed two steps: created data frame \none row per country semi-joined original dataset.\n’s possible avoid join use unnest() instead \nunnest(.drop = TRUE). ?","code":""},{"path":"many-models.html","id":"list-columns-1","chapter":"25 Many models","heading":"25.3 List-columns","text":"Now ’ve seen basic workflow managing many models, let’s dive back details. section, ’ll explore list-column data structure little detail. ’s recently ’ve really appreciated idea list-column. List-columns implicit definition data frame: data frame named list equal length vectors. list vector, ’s always legitimate use list column data frame. However, base R doesn’t make easy create list-columns, data.frame() treats list list columns:.can prevent data.frame() (), result doesn’t print particularly well:Tibble alleviates problem lazier (tibble() doesn’t modify inputs) providing better print method:’s even easier tribble() can automatically work need list:List-columns often useful intermediate data structure. ’re hard work directly, R functions work atomic vectors data frames, advantage keeping related items together data frame worth little hassle.Generally three parts effective list-column pipeline:create list-column using one nest(), summarise() + list(),\nmutate() + map function, described Creating list-columns.create list-column using one nest(), summarise() + list(),\nmutate() + map function, described Creating list-columns.create intermediate list-columns transforming existing\nlist columns map(), map2() pmap(). example,\ncase study , created list-column models transforming\nlist-column data frames.create intermediate list-columns transforming existing\nlist columns map(), map2() pmap(). example,\ncase study , created list-column models transforming\nlist-column data frames.simplify list-column back data frame atomic vector,\ndescribed Simplifying list-columns.simplify list-column back data frame atomic vector,\ndescribed Simplifying list-columns.","code":"\ndata.frame(x = list(1:3, 3:5))\n#>   x.1.3 x.3.5\n#> 1     1     3\n#> 2     2     4\n#> 3     3     5\ndata.frame(\n  x = I(list(1:3, 3:5)), \n  y = c(\"1, 2\", \"3, 4, 5\")\n)\n#>         x       y\n#> 1 1, 2, 3    1, 2\n#> 2 3, 4, 5 3, 4, 5\ntibble(\n  x = list(1:3, 3:5), \n  y = c(\"1, 2\", \"3, 4, 5\")\n)\n#> # A tibble: 2 x 2\n#>   x         y      \n#>   <list>    <chr>  \n#> 1 <int [3]> 1, 2   \n#> 2 <int [3]> 3, 4, 5\ntribble(\n   ~x, ~y,\n  1:3, \"1, 2\",\n  3:5, \"3, 4, 5\"\n)\n#> # A tibble: 2 x 2\n#>   x         y      \n#>   <list>    <chr>  \n#> 1 <int [3]> 1, 2   \n#> 2 <int [3]> 3, 4, 5"},{"path":"many-models.html","id":"creating-list-columns","chapter":"25 Many models","heading":"25.4 Creating list-columns","text":"Typically, won’t create list-columns tibble(). Instead, ’ll create regular columns, using one three methods:tidyr::nest() convert grouped data frame nested data\nframe list-column data frames.tidyr::nest() convert grouped data frame nested data\nframe list-column data frames.mutate() vectorised functions return list.mutate() vectorised functions return list.summarise() summary functions return multiple results.summarise() summary functions return multiple results.Alternatively, might create named list, using tibble::enframe().Generally, creating list-columns, make sure ’re homogeneous: element contain type thing. checks make sure true, use purrr remember ’ve learned type-stable functions, find happens naturally.","code":""},{"path":"many-models.html","id":"with-nesting","chapter":"25 Many models","heading":"25.4.1 With nesting","text":"nest() creates nested data frame, data frame list-column data frames. nested data frame row meta-observation: columns give variables define observation (like country continent ), list-column data frames gives individual observations make meta-observation.two ways use nest(). far ’ve seen use grouped data frame. applied grouped data frame, nest() keeps grouping columns , bundles everything else list-column:can also use ungrouped data frame, specifying columns want nest:","code":"\ngapminder %>% \n  group_by(country, continent) %>% \n  nest()\n#> # A tibble: 142 x 3\n#> # Groups:   country, continent [142]\n#>   country     continent data             \n#>   <fct>       <fct>     <list>           \n#> 1 Afghanistan Asia      <tibble [12 × 4]>\n#> 2 Albania     Europe    <tibble [12 × 4]>\n#> 3 Algeria     Africa    <tibble [12 × 4]>\n#> 4 Angola      Africa    <tibble [12 × 4]>\n#> 5 Argentina   Americas  <tibble [12 × 4]>\n#> 6 Australia   Oceania   <tibble [12 × 4]>\n#> # … with 136 more rows\ngapminder %>% \n  nest(data = c(year:gdpPercap))\n#> # A tibble: 142 x 3\n#>   country     continent data             \n#>   <fct>       <fct>     <list>           \n#> 1 Afghanistan Asia      <tibble [12 × 4]>\n#> 2 Albania     Europe    <tibble [12 × 4]>\n#> 3 Algeria     Africa    <tibble [12 × 4]>\n#> 4 Angola      Africa    <tibble [12 × 4]>\n#> 5 Argentina   Americas  <tibble [12 × 4]>\n#> 6 Australia   Oceania   <tibble [12 × 4]>\n#> # … with 136 more rows"},{"path":"many-models.html","id":"from-vectorised-functions","chapter":"25 Many models","heading":"25.4.2 From vectorised functions","text":"useful functions take atomic vector return list. example, strings learned stringr::str_split() takes character vector returns list character vectors. use inside mutate, ’ll get list-column:unnest() knows handle lists vectors:(find using pattern lot, make sure check tidyr::separate_rows() wrapper around common pattern).Another example pattern using map(), map2(), pmap() purrr. example, take final example Invoking different functions rewrite use mutate():Note technically sim isn’t homogeneous contains double integer vectors. However, unlikely cause many problems since integers doubles numeric vectors.","code":"\ndf <- tribble(\n  ~x1,\n  \"a,b,c\", \n  \"d,e,f,g\"\n) \n\ndf %>% \n  mutate(x2 = stringr::str_split(x1, \",\"))\n#> # A tibble: 2 x 2\n#>   x1      x2       \n#>   <chr>   <list>   \n#> 1 a,b,c   <chr [3]>\n#> 2 d,e,f,g <chr [4]>\ndf %>% \n  mutate(x2 = stringr::str_split(x1, \",\")) %>% \n  unnest(x2)\n#> # A tibble: 7 x 2\n#>   x1      x2   \n#>   <chr>   <chr>\n#> 1 a,b,c   a    \n#> 2 a,b,c   b    \n#> 3 a,b,c   c    \n#> 4 d,e,f,g d    \n#> 5 d,e,f,g e    \n#> 6 d,e,f,g f    \n#> # … with 1 more row\nsim <- tribble(\n  ~f,      ~params,\n  \"runif\", list(min = -1, max = 1),\n  \"rnorm\", list(sd = 5),\n  \"rpois\", list(lambda = 10)\n)\n\nsim %>%\n  mutate(sims = invoke_map(f, params, n = 10))\n#> # A tibble: 3 x 3\n#>   f     params           sims      \n#>   <chr> <list>           <list>    \n#> 1 runif <named list [2]> <dbl [10]>\n#> 2 rnorm <named list [1]> <dbl [10]>\n#> 3 rpois <named list [1]> <int [10]>"},{"path":"many-models.html","id":"from-multivalued-summaries","chapter":"25 Many models","heading":"25.4.3 From multivalued summaries","text":"One restriction summarise() works summary functions return single value. means can’t use functions like quantile() return vector arbitrary length:can however, wrap result list! obeys contract summarise(), summary now list (vector) length 1.make useful results unnest, ’ll also need capture probabilities:","code":"\nmtcars %>% \n  group_by(cyl) %>% \n  summarise(q = quantile(mpg))\n#> `summarise()` has grouped output by 'cyl'. You can override using the `.groups` argument.\n#> # A tibble: 15 x 2\n#> # Groups:   cyl [3]\n#>     cyl     q\n#>   <dbl> <dbl>\n#> 1     4  21.4\n#> 2     4  22.8\n#> 3     4  26  \n#> 4     4  30.4\n#> 5     4  33.9\n#> 6     6  17.8\n#> # … with 9 more rows\nmtcars %>% \n  group_by(cyl) %>% \n  summarise(q = list(quantile(mpg)))\n#> # A tibble: 3 x 2\n#>     cyl q        \n#> * <dbl> <list>   \n#> 1     4 <dbl [5]>\n#> 2     6 <dbl [5]>\n#> 3     8 <dbl [5]>\nprobs <- c(0.01, 0.25, 0.5, 0.75, 0.99)\nmtcars %>% \n  group_by(cyl) %>% \n  summarise(p = list(probs), q = list(quantile(mpg, probs))) %>% \n  unnest(c(p, q))\n#> # A tibble: 15 x 3\n#>     cyl     p     q\n#>   <dbl> <dbl> <dbl>\n#> 1     4  0.01  21.4\n#> 2     4  0.25  22.8\n#> 3     4  0.5   26  \n#> 4     4  0.75  30.4\n#> 5     4  0.99  33.8\n#> 6     6  0.01  17.8\n#> # … with 9 more rows"},{"path":"many-models.html","id":"from-a-named-list","chapter":"25 Many models","heading":"25.4.4 From a named list","text":"Data frames list-columns provide solution common problem: want iterate contents list elements? Instead trying jam everything one object, ’s often easier make data frame: one column can contain elements, one column can contain list. easy way create data frame list tibble::enframe().advantage structure generalises straightforward way - names useful character vector metadata, don’t help types data, multiple vectors.Now want iterate names values parallel, can use map2():","code":"\nx <- list(\n  a = 1:5,\n  b = 3:4, \n  c = 5:6\n) \n\ndf <- enframe(x)\ndf\n#> # A tibble: 3 x 2\n#>   name  value    \n#>   <chr> <list>   \n#> 1 a     <int [5]>\n#> 2 b     <int [2]>\n#> 3 c     <int [2]>\ndf %>% \n  mutate(\n    smry = map2_chr(name, value, ~ stringr::str_c(.x, \": \", .y[1]))\n  )\n#> # A tibble: 3 x 3\n#>   name  value     smry \n#>   <chr> <list>    <chr>\n#> 1 a     <int [5]> a: 1 \n#> 2 b     <int [2]> b: 3 \n#> 3 c     <int [2]> c: 5"},{"path":"many-models.html","id":"exercises-69","chapter":"25 Many models","heading":"25.4.5 Exercises","text":"List functions can think take atomic vector \nreturn list.List functions can think take atomic vector \nreturn list.Brainstorm useful summary functions , like quantile(), return\nmultiple values.Brainstorm useful summary functions , like quantile(), return\nmultiple values.’s missing following data frame? quantile() return\nmissing piece? isn’t helpful ?\n\nmtcars %>% \n  group_by(cyl) %>% \n  summarise(q = list(quantile(mpg))) %>% \n  unnest(q)\n#> # tibble: 15 x 2\n#>     cyl     q\n#>   <dbl> <dbl>\n#> 1     4  21.4\n#> 2     4  22.8\n#> 3     4  26  \n#> 4     4  30.4\n#> 5     4  33.9\n#> 6     6  17.8\n#> # … 9 rowsWhat’s missing following data frame? quantile() return\nmissing piece? isn’t helpful ?code ? might might useful?\n\nmtcars %>% \n  group_by(cyl) %>% \n  summarise_all(list(list))code ? might might useful?","code":"\nmtcars %>% \n  group_by(cyl) %>% \n  summarise(q = list(quantile(mpg))) %>% \n  unnest(q)\n#> # A tibble: 15 x 2\n#>     cyl     q\n#>   <dbl> <dbl>\n#> 1     4  21.4\n#> 2     4  22.8\n#> 3     4  26  \n#> 4     4  30.4\n#> 5     4  33.9\n#> 6     6  17.8\n#> # … with 9 more rows\nmtcars %>% \n  group_by(cyl) %>% \n  summarise_all(list(list))"},{"path":"many-models.html","id":"simplifying-list-columns","chapter":"25 Many models","heading":"25.5 Simplifying list-columns","text":"apply techniques data manipulation visualisation ’ve learned book, ’ll need simplify list-column back regular column (atomic vector), set columns. technique ’ll use collapse back simpler structure depends whether want single value per element, multiple values:want single value, use mutate() map_lgl(),\nmap_int(), map_dbl(), map_chr() create atomic vector.want single value, use mutate() map_lgl(),\nmap_int(), map_dbl(), map_chr() create atomic vector.want many values, use unnest() convert list-columns back\nregular columns, repeating rows many times necessary.want many values, use unnest() convert list-columns back\nregular columns, repeating rows many times necessary.described detail .","code":""},{"path":"many-models.html","id":"list-to-vector","chapter":"25 Many models","heading":"25.5.1 List to vector","text":"can reduce list column atomic vector regular column. example, can always summarise object type length, code work regardless sort list-column :basic information get default tbl print method, now can use filtering. useful technique heterogeneous list, want filter parts aren’t working .Don’t forget map_*() shortcuts - can use map_chr(x, \"apple\") extract string stored apple element x. useful pulling apart nested lists regular columns. Use .null argument provide value use element missing (instead returning NULL):","code":"\ndf <- tribble(\n  ~x,\n  letters[1:5],\n  1:3,\n  runif(5)\n)\n  \ndf %>% mutate(\n  type = map_chr(x, typeof),\n  length = map_int(x, length)\n)\n#> # A tibble: 3 x 3\n#>   x         type      length\n#>   <list>    <chr>      <int>\n#> 1 <chr [5]> character      5\n#> 2 <int [3]> integer        3\n#> 3 <dbl [5]> double         5\ndf <- tribble(\n  ~x,\n  list(a = 1, b = 2),\n  list(a = 2, c = 4)\n)\ndf %>% mutate(\n  a = map_dbl(x, \"a\"),\n  b = map_dbl(x, \"b\", .null = NA_real_)\n)\n#> # A tibble: 2 x 3\n#>   x                    a     b\n#>   <list>           <dbl> <dbl>\n#> 1 <named list [2]>     1     2\n#> 2 <named list [2]>     2    NA"},{"path":"many-models.html","id":"unnesting-1","chapter":"25 Many models","heading":"25.5.2 Unnesting","text":"unnest() works repeating regular columns element list-column. example, following simple example repeat first row 4 times (first element y length four), second row :means can’t simultaneously unnest two columns contain different number elements:principle applies unnesting list-columns data frames. can unnest multiple list-cols long data frames row number rows.","code":"\ntibble(x = 1:2, y = list(1:4, 1)) %>% unnest(y)\n#> # A tibble: 5 x 2\n#>       x     y\n#>   <int> <dbl>\n#> 1     1     1\n#> 2     1     2\n#> 3     1     3\n#> 4     1     4\n#> 5     2     1\n# Ok, because y and z have the same number of elements in\n# every row\ndf1 <- tribble(\n  ~x, ~y,           ~z,\n   1, c(\"a\", \"b\"), 1:2,\n   2, \"c\",           3\n)\ndf1\n#> # A tibble: 2 x 3\n#>       x y         z        \n#>   <dbl> <list>    <list>   \n#> 1     1 <chr [2]> <int [2]>\n#> 2     2 <chr [1]> <dbl [1]>\ndf1 %>% unnest(c(y, z))\n#> # A tibble: 3 x 3\n#>       x y         z\n#>   <dbl> <chr> <dbl>\n#> 1     1 a         1\n#> 2     1 b         2\n#> 3     2 c         3\n\n# Doesn't work because y and z have different number of elements\ndf2 <- tribble(\n  ~x, ~y,           ~z,\n   1, \"a\",         1:2,  \n   2, c(\"b\", \"c\"),   3\n)\ndf2\n#> # A tibble: 2 x 3\n#>       x y         z        \n#>   <dbl> <list>    <list>   \n#> 1     1 <chr [1]> <int [2]>\n#> 2     2 <chr [2]> <dbl [1]>\ndf2 %>% unnest(c(y, z))\n#> # A tibble: 4 x 3\n#>       x y         z\n#>   <dbl> <chr> <dbl>\n#> 1     1 a         1\n#> 2     1 a         2\n#> 3     2 b         3\n#> 4     2 c         3"},{"path":"many-models.html","id":"exercises-70","chapter":"25 Many models","heading":"25.5.3 Exercises","text":"might lengths() function useful creating atomic\nvector columns list-columns?might lengths() function useful creating atomic\nvector columns list-columns?List common types vector found data frame. makes\nlists different?List common types vector found data frame. makes\nlists different?","code":""},{"path":"many-models.html","id":"making-tidy-data-with-broom","chapter":"25 Many models","heading":"25.6 Making tidy data with broom","text":"broom package provides three general tools turning models tidy data frames:broom::glance(model) returns row model. column gives \nmodel summary: either measure model quality, complexity, \ncombination two.broom::glance(model) returns row model. column gives \nmodel summary: either measure model quality, complexity, \ncombination two.broom::tidy(model) returns row coefficient model. \ncolumn gives information estimate variability.broom::tidy(model) returns row coefficient model. \ncolumn gives information estimate variability.broom::augment(model, data) returns row row data, adding\nextra values like residuals, influence statistics.broom::augment(model, data) returns row row data, adding\nextra values like residuals, influence statistics.","code":""},{"path":"communicate-intro.html","id":"communicate-intro","chapter":"26 Introduction","heading":"26 Introduction","text":"far, ’ve learned tools get data R, tidy form convenient analysis, understand data transformation, visualisation modelling. However, doesn’t matter great analysis unless can explain others: need communicate results.Communication theme following four chapters:R Markdown, learn R Markdown, tool integrating\nprose, code, results. can use R Markdown notebook mode \nanalyst--analyst communication, report mode \nanalyst--decision-maker communication. Thanks power R Markdown\nformats, can even use document purposes.R Markdown, learn R Markdown, tool integrating\nprose, code, results. can use R Markdown notebook mode \nanalyst--analyst communication, report mode \nanalyst--decision-maker communication. Thanks power R Markdown\nformats, can even use document purposes.Graphics communication, learn take exploratory\ngraphics turn expository graphics, graphics help \nnewcomer analysis understand ’s going quickly \neasily possible.Graphics communication, learn take exploratory\ngraphics turn expository graphics, graphics help \nnewcomer analysis understand ’s going quickly \neasily possible.R Markdown formats, ’ll learn little many varieties\noutputs can produce using R Markdown, including dashboards, websites,\nbooks.R Markdown formats, ’ll learn little many varieties\noutputs can produce using R Markdown, including dashboards, websites,\nbooks.’ll finish R Markdown workflow, ’ll learn \n“analysis notebook” systematically record successes \nfailures can learn .’ll finish R Markdown workflow, ’ll learn \n“analysis notebook” systematically record successes \nfailures can learn .Unfortunately, chapters focus mostly technical mechanics communication, really hard problems communicating thoughts humans. However, lot great books communication, ’ll point end chapter.","code":""},{"path":"r-markdown.html","id":"r-markdown","chapter":"27 R Markdown","heading":"27 R Markdown","text":"","code":""},{"path":"r-markdown.html","id":"introduction-18","chapter":"27 R Markdown","heading":"27.1 Introduction","text":"R Markdown provides unified authoring framework data science, combining code, results, prose commentary. R Markdown documents fully reproducible support dozens output formats, like PDFs, Word files, slideshows, .R Markdown files designed used three ways:communicating decision makers, want focus conclusions,\ncode behind analysis.communicating decision makers, want focus conclusions,\ncode behind analysis.collaborating data scientists (including future !), \ninterested conclusions, reached (.e.\ncode).collaborating data scientists (including future !), \ninterested conclusions, reached (.e.\ncode).environment data science, modern day lab\nnotebook can capture , also \nthinking.environment data science, modern day lab\nnotebook can capture , also \nthinking.R Markdown integrates number R packages external tools. means help , --large, available ?. Instead, work chapter, use R Markdown future, keep resources close hand:R Markdown Cheat Sheet: Help > Cheatsheets > R Markdown Cheat Sheet,R Markdown Cheat Sheet: Help > Cheatsheets > R Markdown Cheat Sheet,R Markdown Reference Guide: Help > Cheatsheets > R Markdown Reference\nGuide.R Markdown Reference Guide: Help > Cheatsheets > R Markdown Reference\nGuide.cheatsheets also available https://rstudio.com/resources/cheatsheets/.","code":""},{"path":"r-markdown.html","id":"prerequisites-18","chapter":"27 R Markdown","heading":"27.1.1 Prerequisites","text":"need rmarkdown package, don’t need explicitly install load , RStudio automatically needed.","code":""},{"path":"r-markdown.html","id":"r-markdown-basics","chapter":"27 R Markdown","heading":"27.2 R Markdown basics","text":"R Markdown file, plain text file extension .Rmd:contains three important types content:(optional) YAML header surrounded ---s.Chunks R code surrounded ```.Text mixed simple text formatting like # heading _italics_.open .Rmd, get notebook interface code output interleaved. can run code chunk clicking Run icon (looks like play button top chunk), pressing Cmd/Ctrl + Shift + Enter. RStudio executes code displays results inline code:produce complete report containing text, code, results, click “Knit” press Cmd/Ctrl + Shift + K. can also programmatically rmarkdown::render(\"1-example.Rmd\"). display report viewer pane, create self-contained HTML file can share others.knit document, R Markdown sends .Rmd file knitr, http://yihui.name/knitr/, executes code chunks creates new markdown (.md) document includes code output. markdown file generated knitr processed pandoc, http://pandoc.org/, responsible creating finished file. advantage two step workflow can create wide range output formats, ’ll learn R markdown formats.get started .Rmd file, select File > New File > R Markdown… menubar. RStudio launch wizard can use pre-populate file useful content reminds key features R Markdown work.following sections dive three components R Markdown document details: markdown text, code chunks, YAML header.","code":"---\ntitle: \"Diamond sizes\"\ndate: 2016-08-25\noutput: html_document\n---\n\n```{r setup, include = FALSE}\nlibrary(ggplot2)\nlibrary(dplyr)\n\nsmaller <- diamonds %>% \n  filter(carat <= 2.5)\n```\n\nWe have data about `r nrow(diamonds)` diamonds. Only \n`r nrow(diamonds) - nrow(smaller)` are larger than\n2.5 carats. The distribution of the remainder is shown\nbelow:\n\n```{r, echo = FALSE}\nsmaller %>% \n  ggplot(aes(carat)) + \n  geom_freqpoly(binwidth = 0.01)\n```"},{"path":"r-markdown.html","id":"exercises-71","chapter":"27 R Markdown","heading":"27.2.1 Exercises","text":"Create new notebook using File > New File > R Notebook. Read \ninstructions. Practice running chunks. Verify can modify\ncode, re-run , see modified output.Create new notebook using File > New File > R Notebook. Read \ninstructions. Practice running chunks. Verify can modify\ncode, re-run , see modified output.Create new R Markdown document File > New File > R Markdown…\nKnit clicking appropriate button. Knit using \nappropriate keyboard short cut. Verify can modify \ninput see output update.Create new R Markdown document File > New File > R Markdown…\nKnit clicking appropriate button. Knit using \nappropriate keyboard short cut. Verify can modify \ninput see output update.Compare contrast R notebook R markdown files created\n. outputs similar? different? \ninputs similar? different? happens \ncopy YAML header one ?Compare contrast R notebook R markdown files created\n. outputs similar? different? \ninputs similar? different? happens \ncopy YAML header one ?Create one new R Markdown document three built-\nformats: HTML, PDF Word. Knit three documents.\noutput differ? input differ? (may need\ninstall LaTeX order build PDF output — RStudio \nprompt necessary.)Create one new R Markdown document three built-\nformats: HTML, PDF Word. Knit three documents.\noutput differ? input differ? (may need\ninstall LaTeX order build PDF output — RStudio \nprompt necessary.)","code":""},{"path":"r-markdown.html","id":"text-formatting-with-markdown","chapter":"27 R Markdown","heading":"27.3 Text formatting with Markdown","text":"Prose .Rmd files written Markdown, lightweight set conventions formatting plain text files. Markdown designed easy read easy write. also easy learn. guide shows use Pandoc’s Markdown, slightly extended version Markdown R Markdown understands.best way learn simply try . take days, soon become second nature, won’t need think . forget, can get handy reference sheet Help > Markdown Quick Reference.","code":"Text formatting \n------------------------------------------------------------\n\n*italic*  or _italic_\n**bold**   __bold__\n`code`\nsuperscript^2^ and subscript~2~\n\nHeadings\n------------------------------------------------------------\n\n# 1st Level Header\n\n## 2nd Level Header\n\n### 3rd Level Header\n\nLists\n------------------------------------------------------------\n\n*   Bulleted list item 1\n\n*   Item 2\n\n    * Item 2a\n\n    * Item 2b\n\n1.  Numbered list item 1\n\n1.  Item 2. The numbers are incremented automatically in the output.\n\nLinks and images\n------------------------------------------------------------\n\n<http://example.com>\n\n[linked phrase](http://example.com)\n\n![optional caption text](path/to/img.png)\n\nTables \n------------------------------------------------------------\n\nFirst Header  | Second Header\n------------- | -------------\nContent Cell  | Content Cell\nContent Cell  | Content Cell"},{"path":"r-markdown.html","id":"exercises-72","chapter":"27 R Markdown","heading":"27.3.1 Exercises","text":"Practice ’ve learned creating brief CV. title \nname, include headings (least) education \nemployment. sections include bulleted list \njobs/degrees. Highlight year bold.Practice ’ve learned creating brief CV. title \nname, include headings (least) education \nemployment. sections include bulleted list \njobs/degrees. Highlight year bold.Using R Markdown quick reference, figure :\nAdd footnote.\nAdd horizontal rule.\nAdd block quote.\nUsing R Markdown quick reference, figure :Add footnote.Add horizontal rule.Add block quote.Copy paste contents diamond-sizes.Rmd \nhttps://github.com/hadley/r4ds/tree/master/rmarkdown local\nR markdown document. Check can run , add text \nfrequency polygon describes striking features.Copy paste contents diamond-sizes.Rmd \nhttps://github.com/hadley/r4ds/tree/master/rmarkdown local\nR markdown document. Check can run , add text \nfrequency polygon describes striking features.","code":""},{"path":"r-markdown.html","id":"code-chunks","chapter":"27 R Markdown","heading":"27.4 Code chunks","text":"run code inside R Markdown document, need insert chunk. three ways :keyboard shortcut Cmd/Ctrl + Alt + IThe keyboard shortcut Cmd/Ctrl + Alt + IThe “Insert” button icon editor toolbar.“Insert” button icon editor toolbar.manually typing chunk delimiters ```{r} ```.manually typing chunk delimiters ```{r} ```.Obviously, ’d recommend learn keyboard shortcut. save lot time long run!can continue run code using keyboard shortcut now (hope!) know love: Cmd/Ctrl + Enter. However, chunks get new keyboard shortcut: Cmd/Ctrl + Shift + Enter, runs code chunk. Think chunk like function. chunk relatively self-contained, focussed around single task.following sections describe chunk header consists ```{r, followed optional chunk name, followed comma separated options, followed }. Next comes R code chunk end indicated final ```.","code":""},{"path":"r-markdown.html","id":"chunk-name","chapter":"27 R Markdown","heading":"27.4.1 Chunk name","text":"Chunks can given optional name: ```{r -name}. three advantages:can easily navigate specific chunks using drop-\ncode navigator bottom-left script editor:\ncan easily navigate specific chunks using drop-\ncode navigator bottom-left script editor:Graphics produced chunks useful names make\neasier use elsewhere. important options.Graphics produced chunks useful names make\neasier use elsewhere. important options.can set networks cached chunks avoid re-performing expensive\ncomputations every run. .can set networks cached chunks avoid re-performing expensive\ncomputations every run. .one chunk name imbues special behaviour: setup. ’re notebook mode, chunk named setup run automatically , code run.","code":""},{"path":"r-markdown.html","id":"chunk-options","chapter":"27 R Markdown","heading":"27.4.2 Chunk options","text":"Chunk output can customised options, arguments supplied chunk header. Knitr provides almost 60 options can use customize code chunks. ’ll cover important chunk options ’ll use frequently. can see full list http://yihui.name/knitr/options/.important set options controls code block executed results inserted finished report:eval = FALSE prevents code evaluated. (obviously \ncode run, results generated). useful \ndisplaying example code, disabling large block code without\ncommenting line.eval = FALSE prevents code evaluated. (obviously \ncode run, results generated). useful \ndisplaying example code, disabling large block code without\ncommenting line.include = FALSE runs code, doesn’t show code results\nfinal document. Use setup code don’t want\ncluttering report.include = FALSE runs code, doesn’t show code results\nfinal document. Use setup code don’t want\ncluttering report.echo = FALSE prevents code, results appearing \nfinished file. Use writing reports aimed people don’t\nwant see underlying R code.echo = FALSE prevents code, results appearing \nfinished file. Use writing reports aimed people don’t\nwant see underlying R code.message = FALSE warning = FALSE prevents messages warnings\nappearing finished file.message = FALSE warning = FALSE prevents messages warnings\nappearing finished file.results = 'hide' hides printed output; fig.show = 'hide' hides\nplots.results = 'hide' hides printed output; fig.show = 'hide' hides\nplots.error = TRUE causes render continue even code returns error.\nrarely something ’ll want include final version\nreport, can useful need debug exactly\ngoing inside .Rmd. ’s also useful ’re teaching R\nwant deliberately include error. default, error = FALSE causes\nknitting fail single error document.error = TRUE causes render continue even code returns error.\nrarely something ’ll want include final version\nreport, can useful need debug exactly\ngoing inside .Rmd. ’s also useful ’re teaching R\nwant deliberately include error. default, error = FALSE causes\nknitting fail single error document.following table summarises types output option suppresses:","code":""},{"path":"r-markdown.html","id":"table","chapter":"27 R Markdown","heading":"27.4.3 Table","text":"default, R Markdown prints data frames matrices ’d see console:prefer data displayed additional formatting can use knitr::kable function. code generates Table 27.1.Table 27.1: knitr kable.Read documentation ?knitr::kable see ways can customise table. even deeper customisation, consider xtable, stargazer, pander, tables, ascii packages. provides set tools returning formatted tables R code.also rich set options controlling figures embedded. ’ll learn saving plots.","code":"\nmtcars[1:5, ]\n#>                    mpg cyl disp  hp drat    wt  qsec vs am gear carb\n#> Mazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n#> Mazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n#> Datsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n#> Hornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n#> Hornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nknitr::kable(\n  mtcars[1:5, ], \n  caption = \"A knitr kable.\"\n)"},{"path":"r-markdown.html","id":"caching","chapter":"27 R Markdown","heading":"27.4.4 Caching","text":"Normally, knit document starts completely clean slate. great reproducibility, ensures ’ve captured every important computation code. However, can painful computations take long time. solution cache = TRUE. set, save output chunk specially named file disk. subsequent runs, knitr check see code changed, hasn’t, reuse cached results.caching system must used care, default based code , dependencies. example, processed_data chunk depends raw_data chunk:Caching processed_data chunk means get re-run dplyr pipeline changed, won’t get rerun read_csv() call changes. can avoid problem dependson chunk option:dependson contain character vector every chunk cached chunk depends . Knitr update results cached chunk whenever detects one dependencies changed.Note chunks won’t update a_very_large_file.csv changes, knitr caching tracks changes within .Rmd file. want also track changes file can use cache.extra option. arbitrary R expression invalidate cache whenever changes. good function use file.info(): returns bunch information file including last modified. can write:caching strategies get progressively complicated, ’s good idea regularly clear caches knitr::clean_cache().’ve used advice David Robinson name chunks: chunk named primary object creates. makes easier understand dependson specification.","code":"```{r raw_data}\nrawdata <- readr::read_csv(\"a_very_large_file.csv\")\n```\n\n```{r processed_data, cache = TRUE}\nprocessed_data <- rawdata %>% \n  filter(!is.na(import_var)) %>% \n  mutate(new_variable = complicated_transformation(x, y, z))\n``````{r processed_data, cache = TRUE, dependson = \"raw_data\"}\nprocessed_data <- rawdata %>% \n  filter(!is.na(import_var)) %>% \n  mutate(new_variable = complicated_transformation(x, y, z))\n``````{r raw_data, cache.extra = file.info(\"a_very_large_file.csv\")}\nrawdata <- readr::read_csv(\"a_very_large_file.csv\")\n```"},{"path":"r-markdown.html","id":"global-options","chapter":"27 R Markdown","heading":"27.4.5 Global options","text":"work knitr, discover default chunk options don’t fit needs want change . can calling knitr::opts_chunk$set() code chunk. example, writing books tutorials set:uses preferred comment formatting, ensures code output kept closely entwined. hand, preparing report, might set:hide code default, showing chunks deliberately choose show (echo = TRUE). might consider setting message = FALSE warning = FALSE, make harder debug problems wouldn’t see messages final document.","code":"\nknitr::opts_chunk$set(\n  comment = \"#>\",\n  collapse = TRUE\n)\nknitr::opts_chunk$set(\n  echo = FALSE\n)"},{"path":"r-markdown.html","id":"inline-code","chapter":"27 R Markdown","heading":"27.4.6 Inline code","text":"one way embed R code R Markdown document: directly text, : `r `. can useful mention properties data text. example, example document used start chapter :data `r nrow(diamonds)` diamonds.\n`r nrow(diamonds) - nrow(smaller)` larger\n2.5 carats. distribution remainder shown :report knit, results computations inserted text:data 53940 diamonds. 126 larger \n2.5 carats. distribution remainder shown :inserting numbers text, format() friend. allows set number digits don’t print ridiculous degree accuracy, big.mark make numbers easier read. ’ll often combine helper function:","code":"\ncomma <- function(x) format(x, digits = 2, big.mark = \",\")\ncomma(3452345)\n#> [1] \"3,452,345\"\ncomma(.12358124331)\n#> [1] \"0.12\""},{"path":"r-markdown.html","id":"exercises-73","chapter":"27 R Markdown","heading":"27.4.7 Exercises","text":"Add section explores diamond sizes vary cut, colour,\nclarity. Assume ’re writing report someone doesn’t know\nR, instead setting echo = FALSE chunk, set global\noption.Add section explores diamond sizes vary cut, colour,\nclarity. Assume ’re writing report someone doesn’t know\nR, instead setting echo = FALSE chunk, set global\noption.Download diamond-sizes.Rmd \nhttps://github.com/hadley/r4ds/tree/master/rmarkdown. Add section\ndescribes largest 20 diamonds, including table displays\nimportant attributes.Download diamond-sizes.Rmd \nhttps://github.com/hadley/r4ds/tree/master/rmarkdown. Add section\ndescribes largest 20 diamonds, including table displays\nimportant attributes.Modify diamonds-sizes.Rmd use comma() produce nicely\nformatted output. Also include percentage diamonds \nlarger 2.5 carats.Modify diamonds-sizes.Rmd use comma() produce nicely\nformatted output. Also include percentage diamonds \nlarger 2.5 carats.Set network chunks d depends c b, \nb c depend . chunk print lubridate::now(),\nset cache = TRUE, verify understanding caching.Set network chunks d depends c b, \nb c depend . chunk print lubridate::now(),\nset cache = TRUE, verify understanding caching.","code":""},{"path":"r-markdown.html","id":"troubleshooting","chapter":"27 R Markdown","heading":"27.5 Troubleshooting","text":"Troubleshooting R Markdown documents can challenging longer interactive R environment, need learn new tricks. first thing always try recreate problem interactive session. Restart R, “Run chunks” (either Code menu, Run region), keyboard shortcut Ctrl + Alt + R. ’re lucky, recreate problem, can figure ’s going interactively.doesn’t help, must something different interactive environment R markdown environment. ’re going need systematically explore options. common difference working directory: working directory R Markdown directory lives. Check working directory expect including getwd() chunk.Next, brainstorm things might cause bug. ’ll need systematically check ’re R session R markdown session. easiest way set error = TRUE chunk causing problem, use print() str() check settings expect.","code":""},{"path":"r-markdown.html","id":"yaml-header","chapter":"27 R Markdown","heading":"27.6 YAML header","text":"can control many “whole document” settings tweaking parameters YAML header. might wonder YAML stands : ’s “yet another markup language”, designed representing hierarchical data way ’s easy humans read write. R Markdown uses control many details output. ’ll discuss two: document parameters bibliographies.","code":""},{"path":"r-markdown.html","id":"parameters","chapter":"27 R Markdown","heading":"27.6.1 Parameters","text":"R Markdown documents can include one parameters whose values can set render report. Parameters useful want re-render report distinct values various key inputs. example, might producing sales reports per branch, exam results student, demographic summaries country. declare one parameters, use params field.example uses my_class parameter determine class cars display:can see, parameters available within code chunks read-list named params.can write atomic vectors directly YAML header. can also run arbitrary R expressions prefacing parameter value !r. good way specify date/time parameters.RStudio, can click “Knit Parameters” option Knit dropdown menu set parameters, render, preview report single user friendly step. can customise dialog setting options header. See http://rmarkdown.rstudio.com/developer_parameterized_reports.html#parameter_user_interfaces details.Alternatively, need produce many parameterised reports, can call rmarkdown::render() list params:particularly powerful conjunction purrr:pwalk(). following example creates report value class found mpg. First create data frame one row class, giving filename report params:match column names argument names render(), use purrr’s parallel walk call render() row:","code":"---\noutput: html_document\nparams:\n  my_class: \"suv\"\n---\n\n```{r setup, include = FALSE}\nlibrary(ggplot2)\nlibrary(dplyr)\n\nclass <- mpg %>% filter(class == params$my_class)\n```\n\n# Fuel economy for `r params$my_class`s\n\n```{r, message = FALSE}\nggplot(class, aes(displ, hwy)) + \n  geom_point() + \n  geom_smooth(se = FALSE)\n```params:\n  start: !r lubridate::ymd(\"2015-01-01\")\n  snapshot: !r lubridate::ymd_hms(\"2015-01-01 12:30:00\")\nrmarkdown::render(\"fuel-economy.Rmd\", params = list(my_class = \"suv\"))\nreports <- tibble(\n  class = unique(mpg$class),\n  filename = stringr::str_c(\"fuel-economy-\", class, \".html\"),\n  params = purrr::map(class, ~ list(my_class = .))\n)\nreports\n#> # A tibble: 7 x 3\n#>   class   filename                  params          \n#>   <chr>   <chr>                     <list>          \n#> 1 compact fuel-economy-compact.html <named list [1]>\n#> 2 midsize fuel-economy-midsize.html <named list [1]>\n#> 3 suv     fuel-economy-suv.html     <named list [1]>\n#> 4 2seater fuel-economy-2seater.html <named list [1]>\n#> 5 minivan fuel-economy-minivan.html <named list [1]>\n#> 6 pickup  fuel-economy-pickup.html  <named list [1]>\n#> # … with 1 more row\nreports %>% \n  select(output_file = filename, params) %>% \n  purrr::pwalk(rmarkdown::render, input = \"fuel-economy.Rmd\")"},{"path":"r-markdown.html","id":"bibliographies-and-citations","chapter":"27 R Markdown","heading":"27.6.2 Bibliographies and Citations","text":"Pandoc can automatically generate citations bibliography number styles. use feature, specify bibliography file using bibliography field file’s header. field contain path directory contains .Rmd file file contains bibliography file:can use many common bibliography formats including BibLaTeX, BibTeX, endnote, medline.create citation within .Rmd file, use key composed ‘@’ + citation identifier bibliography file. place citation square brackets. examples:R Markdown renders file, build append bibliography end document. bibliography contain cited references bibliography file, contain section heading. result common practice end file section header bibliography, # References # Bibliography.can change style citations bibliography referencing CSL (citation style language) file csl field:bibliography field, csl file contain path file. assume csl file directory .Rmd file. good place find CSL style files common bibliography styles http://github.com/citation-style-language/styles.","code":"bibliography: rmarkdown.bibSeparate multiple citations with a `;`: Blah blah [@smith04; @doe99].\n\nYou can add arbitrary comments inside the square brackets: \nBlah blah [see @doe99, pp. 33-35; also @smith04, ch. 1].\n\nRemove the square brackets to create an in-text citation: @smith04 \nsays blah, or @smith04 [p. 33] says blah.\n\nAdd a `-` before the citation to suppress the author's name: \nSmith says blah [-@smith04].bibliography: rmarkdown.bib\ncsl: apa.csl"},{"path":"r-markdown.html","id":"learning-more-3","chapter":"27 R Markdown","heading":"27.7 Learning more","text":"R Markdown still relatively young, still growing rapidly. best place stay top innovations official R Markdown website: http://rmarkdown.rstudio.com.two important topics haven’t covered : collaboration, details accurately communicating ideas humans. Collaboration vital part modern data science, can make life much easier using version control tools, like Git GitHub. recommend two free resources teach Git:“Happy Git R”: user friendly introduction Git GitHub \nR users, Jenny Bryan. book freely available online:\nhttp://happygitwithr.com“Happy Git R”: user friendly introduction Git GitHub \nR users, Jenny Bryan. book freely available online:\nhttp://happygitwithr.comThe “Git GitHub” chapter R Packages, Hadley. can also\nread free online: http://r-pkgs..co.nz/git.html.“Git GitHub” chapter R Packages, Hadley. can also\nread free online: http://r-pkgs..co.nz/git.html.also touched actually write order clearly communicate results analysis. improve writing, highly recommend reading either Style: Lessons Clarity Grace Joseph M. Williams & Joseph Bizup, Sense Structure: Writing Reader’s Perspective George Gopen. books help understand structure sentences paragraphs, give tools make writing clear. (books rather expensive purchased new, ’re used many English classes plenty cheap second-hand copies). George Gopen also number short articles writing https://www.georgegopen.com/-litigation-articles.html. aimed lawyers, almost everything applies data scientists .","code":""},{"path":"graphics-for-communication.html","id":"graphics-for-communication","chapter":"28 Graphics for communication","heading":"28 Graphics for communication","text":"","code":""},{"path":"graphics-for-communication.html","id":"introduction-19","chapter":"28 Graphics for communication","heading":"28.1 Introduction","text":"exploratory data analysis, learned use plots tools exploration. make exploratory plots, know—even looking—variables plot display. made plot purpose, quickly look , move next plot. course analyses, ’ll produce tens hundreds plots, immediately thrown away.Now understand data, need communicate understanding others. audience likely share background knowledge deeply invested data. help others quickly build good mental model data, need invest considerable effort making plots self-explanatory possible. chapter, ’ll learn tools ggplot2 provides .chapter focuses tools need create good graphics. assume know want, just need know . reason, highly recommend pairing chapter good general visualisation book. particularly like Truthful Art, Albert Cairo. doesn’t teach mechanics creating visualisations, instead focuses need think order create effective graphics.","code":""},{"path":"graphics-for-communication.html","id":"prerequisites-19","chapter":"28 Graphics for communication","heading":"28.1.1 Prerequisites","text":"chapter, ’ll focus ggplot2. ’ll also use little dplyr data manipulation, ggplot2 extension packages, including ggrepel viridis. Rather loading extensions , ’ll refer functions explicitly, using :: notation. help make clear functions built ggplot2, come packages. Don’t forget ’ll need install packages install.packages() don’t already .","code":"\nlibrary(tidyverse)"},{"path":"graphics-for-communication.html","id":"label","chapter":"28 Graphics for communication","heading":"28.2 Label","text":"easiest place start turning exploratory graphic expository graphic good labels. add labels labs() function. example adds plot title:purpose plot title summarise main finding. Avoid titles just describe plot , e.g. “scatterplot engine displacement vs. fuel economy”.need add text, two useful labels can use ggplot2 2.2.0 (available time ’re reading book):subtitle adds additional detail smaller font beneath title.subtitle adds additional detail smaller font beneath title.caption adds text bottom right plot, often used describe\nsource data.caption adds text bottom right plot, often used describe\nsource data.can also use labs() replace axis legend titles. ’s usually good idea replace short variable names detailed descriptions, include units.’s possible use mathematical equations instead text strings. Just switch \"\" quote() read available options ?plotmath:","code":"\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  labs(title = \"Fuel efficiency generally decreases with engine size\")\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  labs(\n    title = \"Fuel efficiency generally decreases with engine size\",\n    subtitle = \"Two seaters (sports cars) are an exception because of their light weight\",\n    caption = \"Data from fueleconomy.gov\"\n  )\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(colour = class)) +\n  geom_smooth(se = FALSE) +\n  labs(\n    x = \"Engine displacement (L)\",\n    y = \"Highway fuel economy (mpg)\",\n    colour = \"Car type\"\n  )\ndf <- tibble(\n  x = runif(10),\n  y = runif(10)\n)\nggplot(df, aes(x, y)) +\n  geom_point() +\n  labs(\n    x = quote(sum(x[i] ^ 2, i == 1, n)),\n    y = quote(alpha + beta + frac(delta, theta))\n  )"},{"path":"graphics-for-communication.html","id":"exercises-74","chapter":"28 Graphics for communication","heading":"28.2.1 Exercises","text":"Create one plot fuel economy data customised title,\nsubtitle, caption, x, y, colour labels.Create one plot fuel economy data customised title,\nsubtitle, caption, x, y, colour labels.geom_smooth() somewhat misleading hwy \nlarge engines skewed upwards due inclusion lightweight\nsports cars big engines. Use modelling tools fit display\nbetter model.geom_smooth() somewhat misleading hwy \nlarge engines skewed upwards due inclusion lightweight\nsports cars big engines. Use modelling tools fit display\nbetter model.Take exploratory graphic ’ve created last month, add\ninformative titles make easier others understand.Take exploratory graphic ’ve created last month, add\ninformative titles make easier others understand.","code":""},{"path":"graphics-for-communication.html","id":"annotations","chapter":"28 Graphics for communication","heading":"28.3 Annotations","text":"addition labelling major components plot, ’s often useful label individual observations groups observations. first tool disposal geom_text(). geom_text() similar geom_point(), additional aesthetic: label. makes possible add textual labels plots.two possible sources labels. First, might tibble provides labels. plot isn’t terribly useful, illustrates useful approach: pull efficient car class dplyr, label plot:hard read labels overlap , points. can make things little better switching geom_label() draws rectangle behind text. also use nudge_y parameter move labels slightly corresponding points:helps bit, look closely top-left hand corner, ’ll notice two labels practically top . happens highway mileage displacement best cars compact subcompact categories exactly . ’s way can fix applying transformation every label. Instead, can use ggrepel package Kamil Slowikowski. useful package automatically adjust labels don’t overlap:Note another handy technique used : added second layer large, hollow points highlight points ’ve labelled.can sometimes use idea replace legend labels placed directly plot. ’s wonderful plot, isn’t bad. (theme(legend.position = \"none\") turns legend — ’ll talk shortly.)Alternatively, might just want add single label plot, ’ll still need create data frame. Often, want label corner plot, ’s convenient create new data frame using summarise() compute maximum values x y.want place text exactly borders plot, can use +Inf -Inf. Since ’re longer computing positions mpg, can use tibble() create data frame:examples, manually broke label lines using \"\\n\". Another approach use stringr::str_wrap() automatically add line breaks, given number characters want per line:Note use hjust vjust control alignment label. Figure 28.1 shows nine possible combinations.\nFigure 28.1: nine combinations hjust vjust.\nRemember, addition geom_text(), many geoms ggplot2 available help annotate plot. ideas:Use geom_hline() geom_vline() add reference lines. often make\nthick (size = 2) white (colour = white), draw \nunderneath primary data layer. makes easy see, without\ndrawing attention away data.Use geom_hline() geom_vline() add reference lines. often make\nthick (size = 2) white (colour = white), draw \nunderneath primary data layer. makes easy see, without\ndrawing attention away data.Use geom_rect() draw rectangle around points interest. \nboundaries rectangle defined aesthetics xmin, xmax,\nymin, ymax.Use geom_rect() draw rectangle around points interest. \nboundaries rectangle defined aesthetics xmin, xmax,\nymin, ymax.Use geom_segment() arrow argument draw attention\npoint arrow. Use aesthetics x y define \nstarting location, xend yend define end location.Use geom_segment() arrow argument draw attention\npoint arrow. Use aesthetics x y define \nstarting location, xend yend define end location.limit imagination (patience positioning annotations aesthetically pleasing)!","code":"\nbest_in_class <- mpg %>%\n  group_by(class) %>%\n  filter(row_number(desc(hwy)) == 1)\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(colour = class)) +\n  geom_text(aes(label = model), data = best_in_class)\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(colour = class)) +\n  geom_label(aes(label = model), data = best_in_class, nudge_y = 2, alpha = 0.5)\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(colour = class)) +\n  geom_point(size = 3, shape = 1, data = best_in_class) +\n  ggrepel::geom_label_repel(aes(label = model), data = best_in_class)\nclass_avg <- mpg %>%\n  group_by(class) %>%\n  summarise(\n    displ = median(displ),\n    hwy = median(hwy)\n  )\n\nggplot(mpg, aes(displ, hwy, colour = class)) +\n  ggrepel::geom_label_repel(aes(label = class),\n    data = class_avg,\n    size = 6,\n    label.size = 0,\n    segment.color = NA\n  ) +\n  geom_point() +\n  theme(legend.position = \"none\")\nlabel <- mpg %>%\n  summarise(\n    displ = max(displ),\n    hwy = max(hwy),\n    label = \"Increasing engine size is \\nrelated to decreasing fuel economy.\"\n  )\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  geom_text(aes(label = label), data = label, vjust = \"top\", hjust = \"right\")\nlabel <- tibble(\n  displ = Inf,\n  hwy = Inf,\n  label = \"Increasing engine size is \\nrelated to decreasing fuel economy.\"\n)\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  geom_text(aes(label = label), data = label, vjust = \"top\", hjust = \"right\")\n\"Increasing engine size is related to decreasing fuel economy.\" %>%\n  stringr::str_wrap(width = 40) %>%\n  writeLines()\n#> Increasing engine size is related to\n#> decreasing fuel economy."},{"path":"graphics-for-communication.html","id":"exercises-75","chapter":"28 Graphics for communication","heading":"28.3.1 Exercises","text":"Use geom_text() infinite positions place text \nfour corners plot.Use geom_text() infinite positions place text \nfour corners plot.Read documentation annotate(). can use add text\nlabel plot without create tibble?Read documentation annotate(). can use add text\nlabel plot without create tibble?labels geom_text() interact faceting? can \nadd label single facet? can put different label \nfacet? (Hint: think underlying data.)labels geom_text() interact faceting? can \nadd label single facet? can put different label \nfacet? (Hint: think underlying data.)arguments geom_label() control appearance background\nbox?arguments geom_label() control appearance background\nbox?four arguments arrow()? work? Create series\nplots demonstrate important options.four arguments arrow()? work? Create series\nplots demonstrate important options.","code":""},{"path":"graphics-for-communication.html","id":"scales","chapter":"28 Graphics for communication","heading":"28.4 Scales","text":"third way can make plot better communication adjust scales. Scales control mapping data values things can perceive. Normally, ggplot2 automatically adds scales . example, type:ggplot2 automatically adds default scales behind scenes:Note naming scheme scales: scale_ followed name aesthetic, _, name scale. default scales named according type variable align : continuous, discrete, datetime, date. lots non-default scales ’ll learn .default scales carefully chosen good job wide range inputs. Nevertheless, might want override defaults two reasons:might want tweak parameters default scale.\nallows things like change breaks axes, \nkey labels legend.might want tweak parameters default scale.\nallows things like change breaks axes, \nkey labels legend.might want replace scale altogether, use completely\ndifferent algorithm. Often can better default \nknow data.might want replace scale altogether, use completely\ndifferent algorithm. Often can better default \nknow data.","code":"\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(colour = class))\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(colour = class)) +\n  scale_x_continuous() +\n  scale_y_continuous() +\n  scale_colour_discrete()"},{"path":"graphics-for-communication.html","id":"axis-ticks-and-legend-keys","chapter":"28 Graphics for communication","heading":"28.4.1 Axis ticks and legend keys","text":"two primary arguments affect appearance ticks axes keys legend: breaks labels. Breaks controls position ticks, values associated keys. Labels controls text label associated tick/key. common use breaks override default choice:can use labels way (character vector length breaks), can also set NULL suppress labels altogether. useful maps, publishing plots can’t share absolute numbers.can also use breaks labels control appearance legends. Collectively axes legends called guides. Axes used x y aesthetics; legends used everything else.Another use breaks relatively data points want highlight exactly observations occur. example, take plot shows US president started ended term.Note specification breaks labels date datetime scales little different:date_labels takes format specification, form \nparse_datetime().date_labels takes format specification, form \nparse_datetime().date_breaks (shown ), takes string like “2 days” “1 month”.date_breaks (shown ), takes string like “2 days” “1 month”.","code":"\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  scale_y_continuous(breaks = seq(15, 40, by = 5))\nggplot(mpg, aes(displ, hwy)) +\n  geom_point() +\n  scale_x_continuous(labels = NULL) +\n  scale_y_continuous(labels = NULL)\npresidential %>%\n  mutate(id = 33 + row_number()) %>%\n  ggplot(aes(start, id)) +\n    geom_point() +\n    geom_segment(aes(xend = end, yend = id)) +\n    scale_x_date(NULL, breaks = presidential$start, date_labels = \"'%y\")"},{"path":"graphics-for-communication.html","id":"legend-layout","chapter":"28 Graphics for communication","heading":"28.4.2 Legend layout","text":"often use breaks labels tweak axes. also work legends, techniques likely use.control overall position legend, need use theme() setting. ’ll come back themes end chapter, brief, control non-data parts plot. theme setting legend.position controls legend drawn:can also use legend.position = \"none\" suppress display legend altogether.control display individual legends, use guides() along guide_legend() guide_colourbar(). following example shows two important settings: controlling number rows legend uses nrow, overriding one aesthetics make points bigger. particularly useful used low alpha display many points plot.","code":"\nbase <- ggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(colour = class))\n\nbase + theme(legend.position = \"left\")\nbase + theme(legend.position = \"top\")\nbase + theme(legend.position = \"bottom\")\nbase + theme(legend.position = \"right\") # the default\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(colour = class)) +\n  geom_smooth(se = FALSE) +\n  theme(legend.position = \"bottom\") +\n  guides(colour = guide_legend(nrow = 1, override.aes = list(size = 4)))\n#> `geom_smooth()` using method = 'loess' and formula 'y ~ x'"},{"path":"graphics-for-communication.html","id":"replacing-a-scale","chapter":"28 Graphics for communication","heading":"28.4.3 Replacing a scale","text":"Instead just tweaking details little, can instead replace scale altogether. two types scales ’re mostly likely want switch : continuous position scales colour scales. Fortunately, principles apply aesthetics, ’ve mastered position colour, ’ll able quickly pick scale replacements.’s useful plot transformations variable. example, ’ve seen diamond prices ’s easier see precise relationship carat price log transform :However, disadvantage transformation axes now labelled transformed values, making hard interpret plot. Instead transformation aesthetic mapping, can instead scale. visually identical, except axes labelled original data scale.Another scale frequently customised colour. default categorical scale picks colours evenly spaced around colour wheel. Useful alternatives ColorBrewer scales hand tuned work better people common types colour blindness. two plots look similar, enough difference shades red green dots right can distinguished even people red-green colour blindness.Don’t forget simpler techniques. just colours, can add redundant shape mapping. also help ensure plot interpretable black white.ColorBrewer scales documented online http://colorbrewer2.org/ made available R via RColorBrewer package, Erich Neuwirth. Figure 28.2 shows complete list palettes. sequential (top) diverging (bottom) palettes particularly useful categorical values ordered, “middle”. often arises ’ve used cut() make continuous variable categorical variable.\nFigure 28.2: ColourBrewer scales.\npredefined mapping values colours, use scale_colour_manual(). example, map presidential party colour, want use standard mapping red Republicans blue Democrats:continuous colour, can use built-scale_colour_gradient() scale_fill_gradient(). diverging scale, can use scale_colour_gradient2(). allows give, example, positive negative values different colours. ’s sometimes also useful want distinguish points mean.Another option scale_colour_viridis() provided viridis package. ’s continuous analog categorical ColorBrewer scales. designers, Nathaniel Smith Stéfan van der Walt, carefully tailored continuous colour scheme good perceptual properties. ’s example viridis vignette.Note colour scales come two variety: scale_colour_x() scale_fill_x() colour fill aesthetics respectively (colour scales available UK US spellings).","code":"\nggplot(diamonds, aes(carat, price)) +\n  geom_bin2d()\n\nggplot(diamonds, aes(log10(carat), log10(price))) +\n  geom_bin2d()\nggplot(diamonds, aes(carat, price)) +\n  geom_bin2d() + \n  scale_x_log10() + \n  scale_y_log10()\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(color = drv))\n\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(color = drv)) +\n  scale_colour_brewer(palette = \"Set1\")\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(color = drv, shape = drv)) +\n  scale_colour_brewer(palette = \"Set1\")\npresidential %>%\n  mutate(id = 33 + row_number()) %>%\n  ggplot(aes(start, id, colour = party)) +\n    geom_point() +\n    geom_segment(aes(xend = end, yend = id)) +\n    scale_colour_manual(values = c(Republican = \"red\", Democratic = \"blue\"))\ndf <- tibble(\n  x = rnorm(10000),\n  y = rnorm(10000)\n)\nggplot(df, aes(x, y)) +\n  geom_hex() +\n  coord_fixed()\n\nggplot(df, aes(x, y)) +\n  geom_hex() +\n  viridis::scale_fill_viridis() +\n  coord_fixed()"},{"path":"graphics-for-communication.html","id":"exercises-76","chapter":"28 Graphics for communication","heading":"28.4.4 Exercises","text":"doesn’t following code override default scale?\n\nggplot(df, aes(x, y)) +\n  geom_hex() +\n  scale_colour_gradient(low = \"white\", high = \"red\") +\n  coord_fixed()doesn’t following code override default scale?first argument every scale? compare labs()?first argument every scale? compare labs()?Change display presidential terms :\nCombining two variants shown .\nImproving display y axis.\nLabelling term name president.\nAdding informative plot labels.\nPlacing breaks every 4 years (trickier seems!).\nChange display presidential terms :Combining two variants shown .Improving display y axis.Labelling term name president.Adding informative plot labels.Placing breaks every 4 years (trickier seems!).Use override.aes make legend following plot easier see.\n\nggplot(diamonds, aes(carat, price)) +\n  geom_point(aes(colour = cut), alpha = 1/20)\nUse override.aes make legend following plot easier see.","code":"\nggplot(df, aes(x, y)) +\n  geom_hex() +\n  scale_colour_gradient(low = \"white\", high = \"red\") +\n  coord_fixed()\nggplot(diamonds, aes(carat, price)) +\n  geom_point(aes(colour = cut), alpha = 1/20)"},{"path":"graphics-for-communication.html","id":"zooming","chapter":"28 Graphics for communication","heading":"28.5 Zooming","text":"three ways control plot limits:Adjusting data plottedSetting limits scaleSetting xlim ylim coord_cartesian()zoom region plot, ’s generally best use coord_cartesian(). Compare following two plots:can also set limits individual scales. Reducing limits basically equivalent subsetting data. generally useful want expand limits, example, match scales across different plots. example, extract two classes cars plot separately, ’s difficult compare plots three scales (x-axis, y-axis, colour aesthetic) different ranges.One way overcome problem share scales across multiple plots, training scales limits full data.particular case, simply used faceting, technique useful generally, instance, want spread plots multiple pages report.","code":"\nggplot(mpg, mapping = aes(displ, hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth() +\n  coord_cartesian(xlim = c(5, 7), ylim = c(10, 30))\n\nmpg %>%\n  filter(displ >= 5, displ <= 7, hwy >= 10, hwy <= 30) %>%\n  ggplot(aes(displ, hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth()\nsuv <- mpg %>% filter(class == \"suv\")\ncompact <- mpg %>% filter(class == \"compact\")\n\nggplot(suv, aes(displ, hwy, colour = drv)) +\n  geom_point()\n\nggplot(compact, aes(displ, hwy, colour = drv)) +\n  geom_point()\nx_scale <- scale_x_continuous(limits = range(mpg$displ))\ny_scale <- scale_y_continuous(limits = range(mpg$hwy))\ncol_scale <- scale_colour_discrete(limits = unique(mpg$drv))\n\nggplot(suv, aes(displ, hwy, colour = drv)) +\n  geom_point() +\n  x_scale +\n  y_scale +\n  col_scale\n\nggplot(compact, aes(displ, hwy, colour = drv)) +\n  geom_point() +\n  x_scale +\n  y_scale +\n  col_scale"},{"path":"graphics-for-communication.html","id":"themes","chapter":"28 Graphics for communication","heading":"28.6 Themes","text":"Finally, can customise non-data elements plot theme:ggplot2 includes eight themes default, shown Figure 28.3. Many included add-packages like ggthemes (https://github.com/jrnold/ggthemes), Jeffrey Arnold.\nFigure 28.3: eight themes built-ggplot2.\nMany people wonder default theme grey background. deliberate choice puts data forward still making grid lines visible. white grid lines visible (important significantly aid position judgements), little visual impact can easily tune . grey background gives plot similar typographic colour text, ensuring graphics fit flow document without jumping bright white background. Finally, grey background creates continuous field colour ensures plot perceived single visual entity.’s also possible control individual components theme, like size colour font used y axis. Unfortunately, level detail outside scope book, ’ll need read ggplot2 book full details. can also create themes, trying match particular corporate journal style.","code":"\nggplot(mpg, aes(displ, hwy)) +\n  geom_point(aes(color = class)) +\n  geom_smooth(se = FALSE) +\n  theme_bw()"},{"path":"graphics-for-communication.html","id":"saving-your-plots","chapter":"28 Graphics for communication","heading":"28.7 Saving your plots","text":"two main ways get plots R final write-: ggsave() knitr. ggsave() save recent plot disk:don’t specify width height taken dimensions current plotting device. reproducible code, ’ll want specify .Generally, however, think assembling final reports using R Markdown, want focus important code chunk options know graphics. can learn ggsave() documentation.","code":"\nggplot(mpg, aes(displ, hwy)) + geom_point()\nggsave(\"my-plot.pdf\")\n#> Saving 7 x 4.32 in image"},{"path":"graphics-for-communication.html","id":"figure-sizing","chapter":"28 Graphics for communication","heading":"28.7.1 Figure sizing","text":"biggest challenge graphics R Markdown getting figures right size shape. five main options control figure sizing: fig.width, fig.height, fig.asp, .width .height. Image sizing challenging two sizes (size figure created R size inserted output document), multiple ways specifying size (.e., height, width, aspect ratio: pick two three).ever use three five options:find aesthetically pleasing plots consistent\nwidth. enforce , set fig.width = 6 (6\") fig.asp = 0.618\n(golden ratio) defaults. individual chunks, \nadjust fig.asp.find aesthetically pleasing plots consistent\nwidth. enforce , set fig.width = 6 (6\") fig.asp = 0.618\n(golden ratio) defaults. individual chunks, \nadjust fig.asp.control output size .width set percentage\nline width. default .width = \"70%\"\nfig.align = \"center\". give plots room breathe, without taking\nmuch space.control output size .width set percentage\nline width. default .width = \"70%\"\nfig.align = \"center\". give plots room breathe, without taking\nmuch space.put multiple plots single row set .width \n50% two plots, 33% 3 plots, 25% 4 plots, set\nfig.align = \"default\". Depending ’m trying illustrate (e.g.\nshow data show plot variations), ’ll also tweak fig.width, \ndiscussed .put multiple plots single row set .width \n50% two plots, 33% 3 plots, 25% 4 plots, set\nfig.align = \"default\". Depending ’m trying illustrate (e.g.\nshow data show plot variations), ’ll also tweak fig.width, \ndiscussed .find ’re squint read text plot, need tweak fig.width. fig.width larger size figure rendered final doc, text small; fig.width smaller, text big. ’ll often need little experimentation figure right ratio fig.width eventual width document. illustrate principle, following three plots fig.width 4, 6, 8 respectively:want make sure font size consistent across figures, whenever set .width, ’ll also need adjust fig.width maintain ratio default .width. example, default fig.width 6 .width 0.7, set .width = \"50%\" ’ll need set fig.width 4.3 (6 * 0.5 / 0.7).","code":""},{"path":"graphics-for-communication.html","id":"other-important-options","chapter":"28 Graphics for communication","heading":"28.7.2 Other important options","text":"mingling code text, like book, recommend setting fig.show = \"hold\" plots shown code. pleasant side effect forcing break large blocks code explanations.add caption plot, use fig.cap. R Markdown change figure inline “floating”.’re producing PDF output, default graphics type PDF. good default PDFs high quality vector graphics. However, can produce large slow plots displaying thousands points. case, set dev = \"png\" force use PNGs. slightly lower quality, much compact.’s good idea name code chunks produce figures, even don’t routinely label chunks. chunk label used generate file name graphic disk, naming chunks makes much easier pick plots reuse circumstances (.e. want quickly drop single plot email tweet).","code":""},{"path":"graphics-for-communication.html","id":"learning-more-4","chapter":"28 Graphics for communication","heading":"28.8 Learning more","text":"absolute best place learn ggplot2 book: ggplot2: Elegant graphics data analysis. goes much depth underlying theory, many examples combine individual pieces solve practical problems. Unfortunately, book available online free, although can find source code https://github.com/hadley/ggplot2-book.Another great resource ggplot2 extensions gallery https://exts.ggplot2.tidyverse.org/gallery/. site lists many packages extend ggplot2 new geoms scales. ’s great place start ’re trying something seems hard ggplot2.","code":""},{"path":"r-markdown-formats.html","id":"r-markdown-formats","chapter":"29 R Markdown formats","heading":"29 R Markdown formats","text":"","code":""},{"path":"r-markdown-formats.html","id":"introduction-20","chapter":"29 R Markdown formats","heading":"29.1 Introduction","text":"far ’ve seen R Markdown used produce HTML documents. chapter gives brief overview many types output can produce R Markdown. two ways set output document:Permanently, modifying YAML header:\ntitle: \"Viridis Demo\"\noutput: html_documentPermanently, modifying YAML header:Transiently, calling rmarkdown::render() hand:\n\nrmarkdown::render(\"diamond-sizes.Rmd\", output_format = \"word_document\")\nuseful want programmatically produce multiple types \noutput.Transiently, calling rmarkdown::render() hand:useful want programmatically produce multiple types \noutput.RStudio’s knit button renders file first format listed output field. can render additional formats clicking dropdown menu beside knit button.","code":"title: \"Viridis Demo\"\noutput: html_document\nrmarkdown::render(\"diamond-sizes.Rmd\", output_format = \"word_document\")"},{"path":"r-markdown-formats.html","id":"output-options","chapter":"29 R Markdown formats","heading":"29.2 Output options","text":"output format associated R function. can either write foo pkg::foo. omit pkg, default assumed rmarkdown. ’s important know name function makes output ’s get help. example, figure parameters can set html_document, look ?rmarkdown::html_document.override default parameter values, need use expanded output field. example, wanted render html_document floating table contents, ’d use:can even render multiple outputs supplying list formats:Note special syntax don’t want override default options.","code":"output:\n  html_document:\n    toc: true\n    toc_float: trueoutput:\n  html_document:\n    toc: true\n    toc_float: true\n  pdf_document: default"},{"path":"r-markdown-formats.html","id":"documents","chapter":"29 R Markdown formats","heading":"29.3 Documents","text":"previous chapter focused default html_document output. number basic variations theme, generating different types documents:pdf_document makes PDF LaTeX (open source document layout\nsystem), ’ll need install. RStudio prompt \ndon’t already .pdf_document makes PDF LaTeX (open source document layout\nsystem), ’ll need install. RStudio prompt \ndon’t already .word_document Microsoft Word documents (.docx).word_document Microsoft Word documents (.docx).odt_document OpenDocument Text documents (.odt).odt_document OpenDocument Text documents (.odt).rtf_document Rich Text Format (.rtf) documents.rtf_document Rich Text Format (.rtf) documents.md_document Markdown document. isn’t typically useful \n, might use , example, corporate CMS \nlab wiki uses markdown.md_document Markdown document. isn’t typically useful \n, might use , example, corporate CMS \nlab wiki uses markdown.github_document: tailored version md_document\ndesigned sharing GitHub.github_document: tailored version md_document\ndesigned sharing GitHub.Remember, generating document share decision makers, can turn default display code setting global options setup chunk:html_documents another option make code chunks hidden default, visible click:","code":"\nknitr::opts_chunk$set(echo = FALSE)output:\n  html_document:\n    code_folding: hide"},{"path":"r-markdown-formats.html","id":"notebooks","chapter":"29 R Markdown formats","heading":"29.4 Notebooks","text":"notebook, html_notebook, variation html_document. rendered outputs similar, purpose different. html_document focused communicating decision makers, notebook focused collaborating data scientists. different purposes lead using HTML output different ways. HTML outputs contain fully rendered output, notebook also contains full source code. means can use .nb.html generated notebook two ways:can view web browser, see rendered output. Unlike\nhtml_document, rendering always includes embedded copy \nsource code generated .can view web browser, see rendered output. Unlike\nhtml_document, rendering always includes embedded copy \nsource code generated .can edit RStudio. open .nb.html file, RStudio \nautomatically recreate .Rmd file generated . future, \nalso able include supporting files (e.g. .csv data files), \nautomatically extracted needed.can edit RStudio. open .nb.html file, RStudio \nautomatically recreate .Rmd file generated . future, \nalso able include supporting files (e.g. .csv data files), \nautomatically extracted needed.Emailing .nb.html files simple way share analyses colleagues. things get painful soon want make changes. starts happen, ’s good time learn Git GitHub. Learning Git GitHub definitely painful first, collaboration payoff huge. mentioned earlier, Git GitHub outside scope book, ’s one tip ’s useful ’re already using : use html_notebook github_document outputs:html_notebook gives local preview, file can share via email. github_document creates minimal md file can check git. can easily see results analysis (just code) change time, GitHub render nicely online.","code":"output:\n  html_notebook: default\n  github_document: default"},{"path":"r-markdown-formats.html","id":"presentations","chapter":"29 R Markdown formats","heading":"29.5 Presentations","text":"can also use R Markdown produce presentations. get less visual control tool like Keynote PowerPoint, automatically inserting results R code presentation can save huge amount time. Presentations work dividing content slides, new slide beginning first (#) second (##) level header. can also insert horizontal rule (***) create new slide without header.R Markdown comes three presentation formats built-:ioslides_presentation - HTML presentation ioslidesioslides_presentation - HTML presentation ioslidesslidy_presentation - HTML presentation W3C Slidyslidy_presentation - HTML presentation W3C Slidybeamer_presentation - PDF presentation LaTeX Beamer.beamer_presentation - PDF presentation LaTeX Beamer.Two popular formats provided packages:revealjs::revealjs_presentation - HTML presentation reveal.js.\nRequires revealjs package.revealjs::revealjs_presentation - HTML presentation reveal.js.\nRequires revealjs package.rmdshower, https://github.com/MangoTheCat/rmdshower, provides \nwrapper around shower, https://github.com/shower/shower,\npresentation enginermdshower, https://github.com/MangoTheCat/rmdshower, provides \nwrapper around shower, https://github.com/shower/shower,\npresentation engine","code":""},{"path":"r-markdown-formats.html","id":"dashboards","chapter":"29 R Markdown formats","heading":"29.6 Dashboards","text":"Dashboards useful way communicate large amounts information visually quickly. Flexdashboard makes particularly easy create dashboards using R Markdown convention headers affect layout:level 1 header (#) begins new page dashboard.level 2 header (##) begins new column.level 3 header (###) begins new row.example, can produce dashboard:Using code:Flexdashboard also provides simple tools creating sidebars, tabsets, value boxes, gauges. learn flexdashboard visit http://rmarkdown.rstudio.com/flexdashboard/.","code":"---\ntitle: \"Diamonds distribution dashboard\"\noutput: flexdashboard::flex_dashboard\n---\n\n```{r setup, include = FALSE}\nlibrary(ggplot2)\nlibrary(dplyr)\nknitr::opts_chunk$set(fig.width = 5, fig.asp = 1/3)\n```\n\n## Column 1\n\n### Carat\n\n```{r}\nggplot(diamonds, aes(carat)) + geom_histogram(binwidth = 0.1)\n```\n\n### Cut\n\n```{r}\nggplot(diamonds, aes(cut)) + geom_bar()\n```\n\n### Colour\n\n```{r}\nggplot(diamonds, aes(color)) + geom_bar()\n```\n\n## Column 2\n\n### The largest diamonds\n\n```{r}\ndiamonds %>% \n  arrange(desc(carat)) %>% \n  head(100) %>% \n  select(carat, cut, color, price) %>% \n  DT::datatable()\n```"},{"path":"r-markdown-formats.html","id":"interactivity","chapter":"29 R Markdown formats","heading":"29.7 Interactivity","text":"HTML format (document, notebook, presentation, dashboard) can contain interactive components.","code":""},{"path":"r-markdown-formats.html","id":"htmlwidgets","chapter":"29 R Markdown formats","heading":"29.7.1 htmlwidgets","text":"HTML interactive format, can take advantage interactivity htmlwidgets, R functions produce interactive HTML visualisations. example, take leaflet map . ’re viewing page web, can drag map around, zoom , etc. obviously can’t book, rmarkdown automatically inserts static screenshot .great thing htmlwidgets don’t need know anything HTML JavaScript use . details wrapped inside package, don’t need worry .many packages provide htmlwidgets, including:dygraphs, http://rstudio.github.io/dygraphs/, interactive time\nseries visualisations.dygraphs, http://rstudio.github.io/dygraphs/, interactive time\nseries visualisations.DT, http://rstudio.github.io/DT/, interactive tables.DT, http://rstudio.github.io/DT/, interactive tables.threejs, https://github.com/bwlewis/rthreejs interactive 3d plots.threejs, https://github.com/bwlewis/rthreejs interactive 3d plots.DiagrammeR, http://rich-iannone.github.io/DiagrammeR/ diagrams\n(like flow charts simple node-link diagrams).DiagrammeR, http://rich-iannone.github.io/DiagrammeR/ diagrams\n(like flow charts simple node-link diagrams).learn htmlwidgets see complete list packages provide visit http://www.htmlwidgets.org/.","code":"\nlibrary(leaflet)\nleaflet() %>%\n  setView(174.764, -36.877, zoom = 16) %>% \n  addTiles() %>%\n  addMarkers(174.764, -36.877, popup = \"Maungawhau\") "},{"path":"r-markdown-formats.html","id":"shiny","chapter":"29 R Markdown formats","heading":"29.7.2 Shiny","text":"htmlwidgets provide client-side interactivity — interactivity happens browser, independently R. one hand, ’s great can distribute HTML file without connection R. However, fundamentally limits can things implemented HTML JavaScript. alternative approach use shiny, package allows create interactivity using R code, JavaScript.call Shiny code R Markdown document, add runtime: shiny header:can use “input” functions add interactive components document:\ncan refer values input$name input$age, code uses automatically re-run whenever change.can’t show live shiny app shiny interactions occur server-side. means can write interactive apps without knowing JavaScript, need server run . introduces logistical issue: Shiny apps need Shiny server run online. run shiny apps computer, shiny automatically sets shiny server , need public facing shiny server want publish sort interactivity online. ’s fundamental trade-shiny: can anything shiny document can R, requires someone running R.Learn Shiny http://shiny.rstudio.com/.","code":"title: \"Shiny Web App\"\noutput: html_document\nruntime: shiny\nlibrary(shiny)\n\ntextInput(\"name\", \"What is your name?\")\nnumericInput(\"age\", \"How old are you?\", NA, min = 0, max = 150)"},{"path":"r-markdown-formats.html","id":"websites","chapter":"29 R Markdown formats","heading":"29.8 Websites","text":"little additional infrastructure can use R Markdown generate complete website:Put .Rmd files single directory. index.Rmd become\nhome page.Put .Rmd files single directory. index.Rmd become\nhome page.Add YAML file named _site.yml provides navigation site.\nexample:\nname: \"-website\"\nnavbar:\n  title: \"Website\"\n  left:\n    - text: \"Home\"\n      href: index.html\n    - text: \"Viridis Colors\"\n      href: 1-example.html\n    - text: \"Terrain Colors\"\n      href: 3-inline.htmlAdd YAML file named _site.yml provides navigation site.\nexample:Execute rmarkdown::render_site() build _site, directory files ready deploy standalone static website, use RStudio Project website directory. RStudio add Build tab IDE can use build preview site.Read http://rmarkdown.rstudio.com/rmarkdown_websites.html.","code":"name: \"my-website\"\nnavbar:\n  title: \"My Website\"\n  left:\n    - text: \"Home\"\n      href: index.html\n    - text: \"Viridis Colors\"\n      href: 1-example.html\n    - text: \"Terrain Colors\"\n      href: 3-inline.html"},{"path":"r-markdown-formats.html","id":"other-formats","chapter":"29 R Markdown formats","heading":"29.9 Other formats","text":"packages provide even output formats:bookdown package, https://github.com/rstudio/bookdown,\nmakes easy write books, like one. learn , read\nAuthoring Books R Markdown,\nYihui Xie, , course, written bookdown. Visit\nhttp://www.bookdown.org see bookdown books written \nwider R community.bookdown package, https://github.com/rstudio/bookdown,\nmakes easy write books, like one. learn , read\nAuthoring Books R Markdown,\nYihui Xie, , course, written bookdown. Visit\nhttp://www.bookdown.org see bookdown books written \nwider R community.prettydoc package, https://github.com/yixuan/prettydoc/,\nprovides lightweight document formats range attractive\nthemes.prettydoc package, https://github.com/yixuan/prettydoc/,\nprovides lightweight document formats range attractive\nthemes.rticles package, https://github.com/rstudio/rticles, compiles \nselection formats tailored specific scientific journals.rticles package, https://github.com/rstudio/rticles, compiles \nselection formats tailored specific scientific journals.See http://rmarkdown.rstudio.com/formats.html list even formats. can also create following instructions http://rmarkdown.rstudio.com/developer_custom_formats.html.","code":""},{"path":"r-markdown-formats.html","id":"learning-more-5","chapter":"29 R Markdown formats","heading":"29.10 Learning more","text":"learn effective communication different formats recommend following resources:improve presentation skills, recommend\nPresentation Patterns, Neal Ford,\nMatthew McCollough, Nathaniel Schutta. provides set effective\npatterns (low- high-level) can apply improve \npresentations.improve presentation skills, recommend\nPresentation Patterns, Neal Ford,\nMatthew McCollough, Nathaniel Schutta. provides set effective\npatterns (low- high-level) can apply improve \npresentations.give academic talks, recommend reading Leek group guide\ngiving talks.give academic talks, recommend reading Leek group guide\ngiving talks.haven’t taken , ’ve heard good things Matt\nMcGarrity’s online course public speaking:\nhttps://www.coursera.org/learn/public-speaking.haven’t taken , ’ve heard good things Matt\nMcGarrity’s online course public speaking:\nhttps://www.coursera.org/learn/public-speaking.creating lot dashboards, make sure read Stephen ’s\nInformation Dashboard Design: Effective Visual Communication\nData. help create dashboards\ntruly useful, just pretty look .creating lot dashboards, make sure read Stephen ’s\nInformation Dashboard Design: Effective Visual Communication\nData. help create dashboards\ntruly useful, just pretty look .Effectively communicating ideas often benefits \nknowledge graphic design. Non-Designer’s Design\nBook great place start.Effectively communicating ideas often benefits \nknowledge graphic design. Non-Designer’s Design\nBook great place start.","code":""},{"path":"r-markdown-workflow.html","id":"r-markdown-workflow","chapter":"30 R Markdown workflow","heading":"30 R Markdown workflow","text":"Earlier, discussed basic workflow capturing R code work interactively console, capture works script editor. R Markdown brings together console script editor, blurring lines interactive exploration long-term code capture. can rapidly iterate within chunk, editing re-executing Cmd/Ctrl + Shift + Enter. ’re happy, move start new chunk.R Markdown also important tightly integrates prose code. makes great analysis notebook lets develop code record thoughts. analysis notebook shares many goals classic lab notebook physical sciences. :Records . Regardless great \nmemory , don’t record , come time \nforgotten important details. Write don’t forget!Records . Regardless great \nmemory , don’t record , come time \nforgotten important details. Write don’t forget!Supports rigorous thinking. likely come strong\nanalysis record thoughts go, continue reflect\n. also saves time eventually write \nanalysis share others.Supports rigorous thinking. likely come strong\nanalysis record thoughts go, continue reflect\n. also saves time eventually write \nanalysis share others.Helps others understand work. rare data analysis \n, ’ll often working part team. lab notebook\nhelps share ’ve done, \ncolleagues lab mates.Helps others understand work. rare data analysis \n, ’ll often working part team. lab notebook\nhelps share ’ve done, \ncolleagues lab mates.Much good advice using lab notebooks effectively can also translated analysis notebooks. ’ve drawn experiences Colin Purrington’s advice lab notebooks (http://colinpurrington.com/tips/lab-notebooks) come following tips:Ensure notebook descriptive title, evocative filename, \nfirst paragraph briefly describes aims analysis.Ensure notebook descriptive title, evocative filename, \nfirst paragraph briefly describes aims analysis.Use YAML header date field record date started working \nnotebook:\ndate: 2016-08-23\nUse ISO8601 YYYY-MM-DD format ’s ambiguity. Use \neven don’t normally write dates way!Use YAML header date field record date started working \nnotebook:Use ISO8601 YYYY-MM-DD format ’s ambiguity. Use \neven don’t normally write dates way!spend lot time analysis idea turns \ndead end, don’t delete ! Write brief note failed \nleave notebook. help avoid going \ndead end come back analysis future.spend lot time analysis idea turns \ndead end, don’t delete ! Write brief note failed \nleave notebook. help avoid going \ndead end come back analysis future.Generally, ’re better data entry outside R. \nneed record small snippet data, clearly lay using\ntibble::tribble().Generally, ’re better data entry outside R. \nneed record small snippet data, clearly lay using\ntibble::tribble().discover error data file, never modify directly, \ninstead write code correct value. Explain made fix.discover error data file, never modify directly, \ninstead write code correct value. Explain made fix.finish day, make sure can knit notebook\n(’re using caching, make sure clear caches). \nlet fix problems code still fresh mind.finish day, make sure can knit notebook\n(’re using caching, make sure clear caches). \nlet fix problems code still fresh mind.want code reproducible long-run (.e. can\ncome back run next month next year), ’ll need track \nversions packages code uses. rigorous approach use\npackrat, http://rstudio.github.io/packrat/, stores packages\nproject directory, checkpoint,\nhttps://github.com/RevolutionAnalytics/checkpoint, reinstall\npackages available specified date. quick dirty hack include\nchunk runs sessionInfo() — won’t let easily recreate\npackages today, least ’ll know .want code reproducible long-run (.e. can\ncome back run next month next year), ’ll need track \nversions packages code uses. rigorous approach use\npackrat, http://rstudio.github.io/packrat/, stores packages\nproject directory, checkpoint,\nhttps://github.com/RevolutionAnalytics/checkpoint, reinstall\npackages available specified date. quick dirty hack include\nchunk runs sessionInfo() — won’t let easily recreate\npackages today, least ’ll know .going create many, many, many analysis notebooks course\ncareer. going organise can find \nfuture? recommend storing individual projects,\ncoming good naming scheme.going create many, many, many analysis notebooks course\ncareer. going organise can find \nfuture? recommend storing individual projects,\ncoming good naming scheme.","code":"date: 2016-08-23"}]
